{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatars.gif","path":"images/avatars.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/zfb.jpg","path":"images/zfb.jpg","modified":0,"renderable":1},{"_id":"themes/next/source/images/mm.jpg","path":"images/mm.jpg","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":0,"renderable":1}],"Cache":[{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1488985946826},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1488985946826},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1488985946826},{"_id":"themes/next/.gitignore","hash":"5f09fca02e030b7676c1d312cd88ce8fbccf381c","modified":1488985946826},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1488985946826},{"_id":"themes/next/.javascript_ignore","hash":"f9ea3c5395f8feb225a24e2c32baa79afda30c16","modified":1488985946826},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1488985946826},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1488985946826},{"_id":"themes/next/.travis.yml","hash":"c42d9608c8c7fe90de7b1581a8dc3886e90c179e","modified":1488985946826},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1488985946827},{"_id":"themes/next/README.en.md","hash":"4ece25ee5f64447cd522e54cb0fffd9a375f0bd4","modified":1488985946827},{"_id":"themes/next/README.md","hash":"500b5606eb6a09c979d16128f8b00f4bf9bc95ac","modified":1488985946827},{"_id":"themes/next/_config.yml","hash":"b4a2628172fc60d12aa8e5d140318dd458f23516","modified":1489279114098},{"_id":"themes/next/bower.json","hash":"5abc236d9cc2512f5457ed57c1fba76669eb7399","modified":1488985946827},{"_id":"themes/next/gulpfile.coffee","hash":"933e6d29eb82522cff0df209d52b935e91b1111c","modified":1488985946827},{"_id":"themes/next/package.json","hash":"7e87b2621104b39a30488654c2a8a0c6a563574b","modified":1488985946833},{"_id":"source/_posts/20170606.md","hash":"d2ce674b6e1dc280c75f5d2ef40a7b60253065b4","modified":1496847933069},{"_id":"source/_posts/booklist.md","hash":"286b9ebb6fefbe4c71788c9c8945f65fd13ec603","modified":1489594126790},{"_id":"source/_posts/cleverboysusegoodnotebook.md","hash":"4449b46373541e80a4bda6aebe584989ca1e6199","modified":1489076880247},{"_id":"source/_posts/dadaozhijian.md","hash":"a2a568f3abc28094c7865ef7547839408057eb72","modified":1489283568986},{"_id":"source/_posts/duominogu.md","hash":"8fb6bf29f76ee2f504f88d3e8ec3179a6f5d68da","modified":1494433921021},{"_id":"source/_posts/duominogu.md~","hash":"9642d6c3bf95273845f8ddcdfd7e06ece460038a","modified":1494433569819},{"_id":"source/_posts/findandawk.md","hash":"bb3f68559a99e834b666e8797258612ffb63fe9f","modified":1488293307303},{"_id":"source/_posts/groupbyandrollup.md","hash":"c31b2b41fc8151cc54b77e4eaaf6a6b985e36cca","modified":1489414018237},{"_id":"source/_posts/hello-world.md","hash":"177a1aa623b8efd4a073b3589fada4d0ccb3282d","modified":1489017035715},{"_id":"source/_posts/hello.md","hash":"18e4c079bf7e02c4fd940824713e77114d52110e","modified":1488294032231},{"_id":"source/_posts/iterator.md","hash":"d2763146f3106549fbe895e59a5b36f5e3e22f78","modified":1488979872349},{"_id":"source/_posts/mybaby01.md","hash":"69f6a4391aac6012dc04c1715e658b2aae7a1cf7","modified":1494433912609},{"_id":"source/_posts/mybaby01.md~","hash":"f84bd824dfc804dc47334e8aba265ae7e83a22d4","modified":1494347211996},{"_id":"source/_posts/scala-none.md","hash":"121acdd8a28b5f72ccbc7d517eb2ef37aa7936b3","modified":1504193906685},{"_id":"source/_posts/scalaa.md","hash":"e0bc5a970d06d8a7cd2ae36c4b3d185d9dc68bd7","modified":1504102485138},{"_id":"source/_posts/scalab-md.md","hash":"4003d438447f0a224d274f8134c710b289f47226","modified":1504136097748},{"_id":"source/_posts/test.md","hash":"781b9fd3afa05876b8d712082be91efeede8de08","modified":1487958966762},{"_id":"source/About/index.md","hash":"4eaa506fd5125e4a2e2a73cde3519f5ca5f94757","modified":1504195033890},{"_id":"source/categories/index.md","hash":"5e66f4cbf6f485910fd0e62912b12f26322fa3b2","modified":1489274880960},{"_id":"source/tags/index.md","hash":"1ba407be25bafaec1019746c2fdf6f76e7fcee9a","modified":1489275652341},{"_id":"themes/next/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1488985946790},{"_id":"themes/next/.git/config","hash":"91b6a53b2a7f929b698734717a38d4ac169f0c1f","modified":1488985946791},{"_id":"themes/next/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1488985724429},{"_id":"themes/next/.git/index","hash":"bf01809e42c4966b00ca0ad8a18ee45980caa76e","modified":1494424821667},{"_id":"themes/next/.git/packed-refs","hash":"547f7c5e2791e36cc09c2444d3a4a65fe5468d12","modified":1488985946680},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"5ab257af816986cd0e53f9527a92d5934ac70ae9","modified":1488985946826},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"c2024ded82143807c28a299c5fe6b927ef3525ff","modified":1488985946826},{"_id":"themes/next/languages/de.yml","hash":"1fdea1f84b7f691f5b4dd4d2b43eeb27b10fa0c8","modified":1488985946827},{"_id":"themes/next/languages/default.yml","hash":"95ec5cdfb563854f231b76162a3494f6ecc5bf61","modified":1488985946827},{"_id":"themes/next/languages/en.yml","hash":"95ec5cdfb563854f231b76162a3494f6ecc5bf61","modified":1488985946827},{"_id":"themes/next/languages/fr-FR.yml","hash":"e98f1558347752a20019b71f0b1f9c8be1b34f42","modified":1488985946827},{"_id":"themes/next/languages/id.yml","hash":"34396bef27c4ab9e9a3c5d3e3aa94b0e3b3a7b0d","modified":1488985946827},{"_id":"themes/next/languages/ja.yml","hash":"49f12149edcc1892b26a6207328cda64da20116d","modified":1488985946827},{"_id":"themes/next/languages/ko.yml","hash":"b6bc5d6b0c000deb44099b42d3aebb8c49dbfca9","modified":1488985946828},{"_id":"themes/next/languages/pt-BR.yml","hash":"7742ba4c0d682cbe1d38305332ebc928abd754b5","modified":1488985946828},{"_id":"themes/next/languages/pt.yml","hash":"6b660b117314cad93f08757601df3adb04c68beb","modified":1488985946828},{"_id":"themes/next/languages/ru.yml","hash":"5022885d8955e1b91d8841048db272bf99c59a76","modified":1488985946828},{"_id":"themes/next/languages/zh-Hans.yml","hash":"40d01dc46d57f71c2ef635c45b295d4355456e90","modified":1488985946828},{"_id":"themes/next/languages/zh-hk.yml","hash":"19c23d21f262e24c06ee6ddfd51d2a6585304f88","modified":1488985946828},{"_id":"themes/next/languages/zh-tw.yml","hash":"68407799271c78ecc07f03d238257dd8c65ad42d","modified":1488985946828},{"_id":"themes/next/languages/zh.yml","hash":"40d01dc46d57f71c2ef635c45b295d4355456e90","modified":1488989930145},{"_id":"themes/next/layout/_layout.swig","hash":"2c0c3547a5b470024326a33ae2779d5ee0252266","modified":1488985946829},{"_id":"themes/next/layout/archive.swig","hash":"b5b59d70fc1563f482fa07afd435752774ad5981","modified":1488985946833},{"_id":"themes/next/layout/category.swig","hash":"6422d196ceaff4220d54b8af770e7e957f3364ad","modified":1488985946833},{"_id":"themes/next/layout/index.swig","hash":"427d0b95b854e311ae363088ab39a393bf8fdc8b","modified":1488985946833},{"_id":"themes/next/layout/page.swig","hash":"3727fab9dadb967e9c2204edca787dc72264674a","modified":1488985946833},{"_id":"themes/next/layout/post.swig","hash":"e2e512142961ddfe77eba29eaa88f4a2ee43ae18","modified":1488985946833},{"_id":"themes/next/layout/schedule.swig","hash":"1f1cdc268f4ef773fd3ae693bbdf7d0b2f45c3a3","modified":1488985946833},{"_id":"themes/next/layout/tag.swig","hash":"07cf49c49c39a14dfbe9ce8e7d7eea3d4d0a4911","modified":1488985946833},{"_id":"themes/next/scripts/merge-configs.js","hash":"13c8b3a2d9fce06c2488820d9248d190c8100e0a","modified":1488985946834},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1488985946834},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1488985946867},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1488985946867},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1488985946867},{"_id":"source/_posts/spark1.md","hash":"e380b13382801206448b4c9b49a5fedc2e04d738","modified":1489275514813},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1488985946842},{"_id":"themes/next/.git/hooks/applypatch-msg.sample","hash":"86b9655a9ebbde13ac8dd5795eb4d5b539edab0f","modified":1488985724461},{"_id":"themes/next/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1488985724490},{"_id":"themes/next/.git/hooks/pre-applypatch.sample","hash":"42fa41564917b44183a50c4d94bb03e1768ddad8","modified":1488985724504},{"_id":"themes/next/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1488985724489},{"_id":"themes/next/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1488985724493},{"_id":"themes/next/.git/hooks/pre-push.sample","hash":"b4ad74c989616b7395dc6c9fce9871bb1e86dfb5","modified":1488985724495},{"_id":"themes/next/.git/hooks/pre-rebase.sample","hash":"5885a56ab4fca8075a05a562d005e922cde9853b","modified":1488985724493},{"_id":"themes/next/.git/hooks/prepare-commit-msg.sample","hash":"2b6275eda365cad50d167fe3a387c9bc9fedd54f","modified":1488985724490},{"_id":"themes/next/.git/hooks/update.sample","hash":"39355a075977d05708ef74e1b66d09a36e486df1","modified":1488985724494},{"_id":"themes/next/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1488985724537},{"_id":"themes/next/.git/logs/HEAD","hash":"1d07f9a2e699c79e45e9ea44dd1c7bc1ff561d29","modified":1488985946790},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1488985946828},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1488985946829},{"_id":"themes/next/layout/_partials/comments.swig","hash":"970aa668680896262b1056bb5787fc9ec8754495","modified":1488985946829},{"_id":"themes/next/layout/_partials/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1488985946829},{"_id":"themes/next/layout/_partials/footer.swig","hash":"4371b920c11f9c6a5b0b2bf90fb6aeae81bbf77e","modified":1489333412150},{"_id":"themes/next/layout/_partials/head.swig","hash":"a0eafe24d1dae30c790ae35612154b3ffbbd5cce","modified":1488985946830},{"_id":"themes/next/layout/_partials/header.swig","hash":"a1ffbb691dfad3eaf2832a11766e58a179003b8b","modified":1489280853616},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1488985946830},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1488985946830},{"_id":"themes/next/layout/_partials/search.swig","hash":"7b61e96508df70152b809ea5354236ab7f0d54f4","modified":1488985946830},{"_id":"themes/next/layout/_scripts/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1488985946831},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1488985946831},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1488985946831},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"4512867d80d9eddfc3a0f5fea3c456f33aa9d522","modified":1488985946833},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"5864f5567ba5efeabcf6ea355013c0b603ee07f2","modified":1488985946829},{"_id":"themes/next/layout/_macro/post.swig","hash":"2c2efe44ea013030f3ce5da7bfdeddb74489eb6e","modified":1488985946829},{"_id":"themes/next/layout/_macro/reward.swig","hash":"37e5b7c42ec17b9b6b786c5512bcc481a21c974e","modified":1488985946829},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"911b99ba0445b2c07373128d87a4ef2eb7de341a","modified":1488985946829},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"14e785adeb0e671ba0ff9a553e6f0d8def6c670c","modified":1488985946829},{"_id":"themes/next/scripts/tags/button.js","hash":"62e6dbeb53d07627a048132c79630b45d9a8f2cc","modified":1488985946834},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1488985946834},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1488985946834},{"_id":"themes/next/scripts/tags/exturl.js","hash":"79378f3a1cd90518b07808ed09156a3ab55ffa31","modified":1488985946834},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1488985946834},{"_id":"themes/next/scripts/tags/note.js","hash":"6752925eedbdb939d8ec4d11bdfb75199f18dd70","modified":1488985946835},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1488985946842},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"90035272fa31a3f65b3c0e2cb8a633876ef457dc","modified":1488985946842},{"_id":"themes/next/source/images/avatars.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1488985946843},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1488985946843},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1488985946843},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1488985946843},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1488985946843},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1488985946843},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1488985946843},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1488985946843},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1488985946844},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1488985946844},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1488985946844},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1488985946844},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1488985946844},{"_id":"themes/next/source/images/zfb.jpg","hash":"a56542f0428978ff7a91bd10591fb8354a827044","modified":1488992366773},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1488985946831},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1488985946831},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1488985946840},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1488985946840},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1488985946840},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1488985946842},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1488985946842},{"_id":"themes/next/.git/refs/heads/master","hash":"8e263f0e3c466c2948b5c2b1e849c404b26898af","modified":1488985946790},{"_id":"themes/next/layout/_components/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1488985946828},{"_id":"themes/next/layout/_components/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1488985946828},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1488985946830},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1488985946830},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"2d1075f4cabcb3956b7b84a8e210f5a66f0a5562","modified":1488985946830},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1488985946830},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1488985946830},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1488985946830},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1488985946830},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1488985946830},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"63315fcf210799f894208c9f512737096df84962","modified":1488985946831},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1488985946831},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1488985946831},{"_id":"themes/next/layout/_scripts/third-party/analytics.swig","hash":"e46900412e28f529c26e25e6bada342006435a32","modified":1488985946831},{"_id":"themes/next/layout/_scripts/third-party/comments.swig","hash":"a279e1881208aff2f669fe235e9661ab825bc540","modified":1488985946832},{"_id":"themes/next/layout/_scripts/third-party/lean-analytics.swig","hash":"92dc60821307fc9769bea9b2d60adaeb798342af","modified":1488985946833},{"_id":"themes/next/layout/_scripts/third-party/localsearch.swig","hash":"f4dbd4c896e6510ded8ebe05394c28f8a86e71bf","modified":1488985946833},{"_id":"themes/next/layout/_scripts/third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1488985946833},{"_id":"themes/next/layout/_scripts/third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1488985946833},{"_id":"themes/next/layout/_scripts/third-party/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1488985946833},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1488985946840},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"715d5b40dc52f319fe4bff0325beb874774d9bd9","modified":1488985946840},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"78a83c38f69a8747bb74e420e6c9eeef1ea76525","modified":1488985946840},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"c8d35a6b9e3bff6d8fdb66de853065af9d37562d","modified":1488985946842},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"06f432f328a5b8a9ef0dbd5301b002aba600b4ce","modified":1488985946842},{"_id":"themes/next/source/css/_variables/base.styl","hash":"74a4f177e56dba5371571baf1962d52325d94440","modified":1488989457249},{"_id":"themes/next/source/images/mm.jpg","hash":"22fc9bbbe8c2413c20c1abe99c11e7c3b40d8406","modified":1488992361666},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1488985946844},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1488985946844},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"aab7be0a6e2724b3faa9338db93c19556c559625","modified":1488985946844},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1488985946844},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1488985946844},{"_id":"themes/next/source/js/src/motion.js","hash":"269414e84df544a4ccb88519f6abae4943db3c67","modified":1488985946844},{"_id":"themes/next/source/js/src/post-details.js","hash":"3b2d64c2e6ae072ba2a9ebf7f09908a1543abd58","modified":1488985946844},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1488985946845},{"_id":"themes/next/source/js/src/utils.js","hash":"e13c9ccf70d593bdf3b8cc1d768f595abd610e6e","modified":1488985946845},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1488985946845},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1488985946848},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1488985946848},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1488985946848},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1488985946851},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1488985946851},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1488985946851},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1488985946851},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1488985946852},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1488985946852},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1488985946852},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1488985946852},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1488985946852},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1488985946863},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1488985946864},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1488985946864},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1488985946864},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1488985946864},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1488985946864},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1488985946864},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1488985946865},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1488985946865},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1488985946866},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1488985946867},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1488985946867},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1488985946863},{"_id":"themes/next/.git/logs/refs/heads/master","hash":"1d07f9a2e699c79e45e9ea44dd1c7bc1ff561d29","modified":1488985946790},{"_id":"themes/next/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1488985946770},{"_id":"themes/next/layout/_scripts/third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1488985946831},{"_id":"themes/next/layout/_scripts/third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1488985946831},{"_id":"themes/next/layout/_scripts/third-party/analytics/busuanzi-counter.swig","hash":"4fcbf57c4918528ab51d3d042cff92cf5aefb599","modified":1488985946831},{"_id":"themes/next/layout/_scripts/third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1488985946832},{"_id":"themes/next/layout/_scripts/third-party/analytics/facebook-sdk.swig","hash":"394d008e5e94575280407ad8a1607a028026cbc3","modified":1488985946832},{"_id":"themes/next/layout/_scripts/third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1488985946832},{"_id":"themes/next/layout/_scripts/third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1488985946832},{"_id":"themes/next/layout/_scripts/third-party/analytics/tencent-mta.swig","hash":"a652f202bd5b30c648c228ab8f0e997eb4928e44","modified":1488985946832},{"_id":"themes/next/layout/_scripts/third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1488985946832},{"_id":"themes/next/layout/_scripts/third-party/comments/disqus.swig","hash":"fb1d04ede838b52ca7541973f86c3810f1ad396e","modified":1488985946832},{"_id":"themes/next/layout/_scripts/third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1488985946832},{"_id":"themes/next/layout/_scripts/third-party/comments/gentie.swig","hash":"03592d1d731592103a41ebb87437fe4b0a4c78ca","modified":1488985946832},{"_id":"themes/next/layout/_scripts/third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1488985946832},{"_id":"themes/next/layout/_scripts/third-party/comments/livere.swig","hash":"7240f2e5ec7115f8abbbc4c9ef73d4bed180fdc7","modified":1488985946832},{"_id":"themes/next/layout/_scripts/third-party/comments/youyan.swig","hash":"af9dd8a4aed7d06cf47b363eebff48850888566c","modified":1488985946832},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1488985946839},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"59ad08bcc6fe9793594869ac2b4c525021453e78","modified":1488985946835},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"ef089a407c90e58eca10c49bc47ec978f96e03ba","modified":1488985946835},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"0dfb4b3ba3180d7285e66f270e1d3fa0f132c3d2","modified":1488985946835},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1488985946835},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1488985946835},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"711c8830886619d4f4a0598b0cde5499dce50c62","modified":1488985946836},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1488985946838},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"7804e31c44717c9a9ddf0f8482b9b9c1a0f74538","modified":1488985946839},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1488985946839},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"013619c472c7e4b08311c464fcbe9fcf5edde603","modified":1488985946840},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1488985946840},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1488985946840},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1488985946840},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1488985946840},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1488985946840},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1488985946840},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"fda14bc35be2e1b332809b55b3d07155a833dbf4","modified":1488985946841},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1488985946841},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1488985946841},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1488985946841},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1488985946841},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"82bbaa6322764779a1ac2e2c8390ce901c7972e2","modified":1488985946841},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1488985946841},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1488985946841},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1488985946841},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"f15537cee1a9ef4fa1e72a1670ebce4097db8115","modified":1488985946841},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1488985946841},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1488985946842},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"e3e23751d4ad24e8714b425d768cf68e37de7ded","modified":1488985946842},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1488985946842},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"79da92119bc246fe05d1626ac98426a83ec90a94","modified":1488985946845},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1488985946849},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1488985946849},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1488985946849},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1488985946849},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1488985946849},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1488985946849},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1488985946850},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1488985946850},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1488985946851},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1488985946851},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1488985946852},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1488985946852},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1488985946853},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1488985946853},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1488985946864},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1488985946864},{"_id":"themes/next/.git/objects/pack/pack-a95573b69ce446566225c894464cba61f7e57293.idx","hash":"3903b3385c6affbb99188c8f10c2f0794fcd7a05","modified":1488985946543},{"_id":"themes/next/source/images/avatar.gif","hash":"3bce2245dd3d2ba011064e17595d68a0e8ded53b","modified":1489276847415},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1488985946862},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1488985946863},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1488985946866},{"_id":"themes/next/.git/logs/refs/remotes/origin/HEAD","hash":"1d07f9a2e699c79e45e9ea44dd1c7bc1ff561d29","modified":1488985946790},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"8994ffcce84deac0471532f270f97c44fea54dc0","modified":1488985946835},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1488985946835},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1488985946835},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1488985946836},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1488985946836},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1488985946836},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1488985946836},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"755b04edbbfbdd981a783edb09c9cc34cb79cea7","modified":1488985946836},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1488985946836},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"7778920dd105fa4de3a7ab206eeba30b1a7bac45","modified":1488985946836},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1488985946836},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1488985946836},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1488985946836},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1488985946836},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"beccb53dcd658136fb91a0c5678dea8f37d6e0b6","modified":1488985946837},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"8fae54591877a73dff0b29b2be2e8935e3c63575","modified":1488985946837},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1488985946837},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"b25132fe6a7ad67059a2c3afc60feabb479bdd75","modified":1488985946837},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1488985946837},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"b9a2e76f019a5941191f1263b54aef7b69c48789","modified":1488985946837},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1488985946837},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"e792c8dc41561c96d128e9b421187f1c3dc978a0","modified":1488985946837},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1488985946837},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"963105a531403d7aad6d9e5e23e3bfabb8ec065a","modified":1488985946837},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1488985946837},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"bfd806d0a9f21446a22df82ac02e37d0075cc3b5","modified":1488985946837},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"2e7ec9aaa3293941106b1bdd09055246aa3c3dc6","modified":1488985946837},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1488985946838},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"5f6ea57aabfa30a437059bf8352f1ad829dbd4ff","modified":1488985946838},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1488985946838},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1488985946838},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"7690b9596ec3a49befbe529a5a2649abec0faf76","modified":1488985946838},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"a2ec22ef4a6817bbb2abe8660fcd99fe4ca0cc5e","modified":1488985946838},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"234facd038f144bd0fe09a31ed1357c5d74c517f","modified":1488985946838},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1488985946838},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1488985946838},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1488985946838},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"b8969e1654eec89a0fd10d88b337fee9cb03cd44","modified":1488985946838},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1488985946838},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"74d0ba86f698165d13402670382a822c8736a556","modified":1488985946838},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"8fe1e55bc290e6aaf07cc644fe27b62107a272a8","modified":1488985946839},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1488985946839},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1488985946839},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1488985946839},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1488985946839},{"_id":"themes/next/source/css/_common/components/third-party/gentie.styl","hash":"586a3ec0f1015e7207cd6a2474362e068c341744","modified":1488985946839},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1488985946839},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"173490e21bece35a34858e8e534cf86e34561350","modified":1488985946839},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"42348219db93a85d2ee23cb06cebd4d8ab121726","modified":1488985946839},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1488985946841},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1488985946841},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1488985946841},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1488985946849},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1488985946849},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1488985946849},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1488985946849},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1488985946850},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1488985946850},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1488985946854},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1488985946856},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1488985946862},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1488985946848},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1488985946860},{"_id":"themes/next/.git/objects/pack/pack-a95573b69ce446566225c894464cba61f7e57293.pack","hash":"3e3ac201472e4a0454af57a6dddf7ee59436b224","modified":1488985946536}],"Category":[{"name":"生活","_id":"cj70mt0or000460tuxolgvinf"},{"name":"spark","_id":"cj70mt0pw000i60tu3c2ps462"},{"name":"技术","_id":"cj70mt0qk000u60tujrwu8jwn"}],"Data":[],"Page":[{"title":"关于我","date":"2017-03-11T23:15:38.000Z","_content":"我是一名软件研发工程师，主要从事大数据方向的研发。\n1. 大数据技能\n    * 熟悉hadoop、spark、flume、hive、hbase、zookeeper等开源组件\n        1. 熟悉使用hadoop的mr计算框架，熟悉hdfs的api，熟练使用和维护hadoop集群\n        1. 熟悉掌握flume的架构设计原理，可以定制话完成实际业务需求\n        111. 熟练使用sql及hive相关操作原理\n        111. 熟悉hbase存储原理，熟练掌握相关api\n        111. 熟悉zk的工作原理。。。\n\n1. 编程技能\n    * 熟练使用java、scala、python、R、shell等语言进行开发\n        11. 熟练使用java基础编程\n        111. 熟悉常用框架如spring，mybatis用此写过相关后台数据接口\n        111. 熟悉scala技能\n        111. 熟悉python的numpy pandas matplotlib \n    * 精通linux操作系统熟练使用shell、sed、vi、emacs、等相关工具\n    * 熟悉机器学习相关算法并有实际项目经验\n        111. 熟悉svm算法原理，有使用svm作分类回归的实际项目经验\n        111. 熟悉MLP算法的算法原理，使用h2o的mlp完成分类问题\n        111. 熟悉其他常用机器学习算法。\n1. 邮箱地址\n    - 980548079@qq.com\n    ![wo](/home/fangqing/Downloads/xiong.JPG)\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"http://music.163.com/outchain/player?type=2&id=430685732&auto=1&height=66\"></iframe>\n","source":"About/index.md","raw":"---\ntitle: 关于我\ndate: 2017-03-12 07:15:38\n---\n我是一名软件研发工程师，主要从事大数据方向的研发。\n1. 大数据技能\n    * 熟悉hadoop、spark、flume、hive、hbase、zookeeper等开源组件\n        1. 熟悉使用hadoop的mr计算框架，熟悉hdfs的api，熟练使用和维护hadoop集群\n        1. 熟悉掌握flume的架构设计原理，可以定制话完成实际业务需求\n        111. 熟练使用sql及hive相关操作原理\n        111. 熟悉hbase存储原理，熟练掌握相关api\n        111. 熟悉zk的工作原理。。。\n\n1. 编程技能\n    * 熟练使用java、scala、python、R、shell等语言进行开发\n        11. 熟练使用java基础编程\n        111. 熟悉常用框架如spring，mybatis用此写过相关后台数据接口\n        111. 熟悉scala技能\n        111. 熟悉python的numpy pandas matplotlib \n    * 精通linux操作系统熟练使用shell、sed、vi、emacs、等相关工具\n    * 熟悉机器学习相关算法并有实际项目经验\n        111. 熟悉svm算法原理，有使用svm作分类回归的实际项目经验\n        111. 熟悉MLP算法的算法原理，使用h2o的mlp完成分类问题\n        111. 熟悉其他常用机器学习算法。\n1. 邮箱地址\n    - 980548079@qq.com\n    ![wo](/home/fangqing/Downloads/xiong.JPG)\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"http://music.163.com/outchain/player?type=2&id=430685732&auto=1&height=66\"></iframe>\n","updated":"2017-08-31T15:57:13.890Z","path":"About/index.html","_id":"cj70mt0oh000160tus9f21p48","comments":1,"layout":"page","content":"<p>我是一名软件研发工程师，主要从事大数据方向的研发。</p>\n<ol>\n<li><p>大数据技能</p>\n<ul>\n<li>熟悉hadoop、spark、flume、hive、hbase、zookeeper等开源组件<ol>\n<li>熟悉使用hadoop的mr计算框架，熟悉hdfs的api，熟练使用和维护hadoop集群</li>\n<li>熟悉掌握flume的架构设计原理，可以定制话完成实际业务需求</li>\n<li>熟练使用sql及hive相关操作原理</li>\n<li>熟悉hbase存储原理，熟练掌握相关api</li>\n<li>熟悉zk的工作原理。。。</li>\n</ol>\n</li>\n</ul>\n</li>\n<li><p>编程技能</p>\n<ul>\n<li>熟练使用java、scala、python、R、shell等语言进行开发<ol>\n<li>熟练使用java基础编程</li>\n<li>熟悉常用框架如spring，mybatis用此写过相关后台数据接口</li>\n<li>熟悉scala技能</li>\n<li>熟悉python的numpy pandas matplotlib </li>\n</ol>\n</li>\n<li>精通linux操作系统熟练使用shell、sed、vi、emacs、等相关工具</li>\n<li>熟悉机器学习相关算法并有实际项目经验<ol>\n<li>熟悉svm算法原理，有使用svm作分类回归的实际项目经验</li>\n<li>熟悉MLP算法的算法原理，使用h2o的mlp完成分类问题</li>\n<li>熟悉其他常用机器学习算法。</li>\n</ol>\n</li>\n</ul>\n</li>\n<li>邮箱地址<ul>\n<li>980548079@qq.com<br><img src=\"/home/fangqing/Downloads/xiong.JPG\" alt=\"wo\"><iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=\"330\" height=\"86\" src=\"http://music.163.com/outchain/player?type=2&id=430685732&auto=1&height=66\"></iframe>\n</li>\n</ul>\n</li>\n</ol>\n","excerpt":"","more":"<p>我是一名软件研发工程师，主要从事大数据方向的研发。</p>\n<ol>\n<li><p>大数据技能</p>\n<ul>\n<li>熟悉hadoop、spark、flume、hive、hbase、zookeeper等开源组件<ol>\n<li>熟悉使用hadoop的mr计算框架，熟悉hdfs的api，熟练使用和维护hadoop集群</li>\n<li>熟悉掌握flume的架构设计原理，可以定制话完成实际业务需求</li>\n<li>熟练使用sql及hive相关操作原理</li>\n<li>熟悉hbase存储原理，熟练掌握相关api</li>\n<li>熟悉zk的工作原理。。。</li>\n</ol>\n</li>\n</ul>\n</li>\n<li><p>编程技能</p>\n<ul>\n<li>熟练使用java、scala、python、R、shell等语言进行开发<ol>\n<li>熟练使用java基础编程</li>\n<li>熟悉常用框架如spring，mybatis用此写过相关后台数据接口</li>\n<li>熟悉scala技能</li>\n<li>熟悉python的numpy pandas matplotlib </li>\n</ol>\n</li>\n<li>精通linux操作系统熟练使用shell、sed、vi、emacs、等相关工具</li>\n<li>熟悉机器学习相关算法并有实际项目经验<ol>\n<li>熟悉svm算法原理，有使用svm作分类回归的实际项目经验</li>\n<li>熟悉MLP算法的算法原理，使用h2o的mlp完成分类问题</li>\n<li>熟悉其他常用机器学习算法。</li>\n</ol>\n</li>\n</ul>\n</li>\n<li>邮箱地址<ul>\n<li>980548079@qq.com<br><img src=\"/home/fangqing/Downloads/xiong.JPG\" alt=\"wo\"><iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"http://music.163.com/outchain/player?type=2&id=430685732&auto=1&height=66\"></iframe>\n</li>\n</ul>\n</li>\n</ol>\n"},{"title":"分类","date":"2017-03-11T23:17:33.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: 分类\ndate: 2017-03-12 07:17:33\ntype: \"categories\"\n---\n","updated":"2017-03-11T23:28:00.960Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cj70mt0oo000360tuq8yk789w","content":"","excerpt":"","more":""},{"title":"标签","date":"2017-03-11T15:40:13.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: 标签\ndate: 2017-03-11 23:40:13\ntype: \"tags\"\n---\n","updated":"2017-03-11T23:40:52.341Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cj70mt0p4000760tu92aan66a","content":"","excerpt":"","more":""}],"Post":[{"title":"吾家有女（三）","date":"2017-06-07T14:03:26.000Z","_content":"成长仿佛只是一瞬间。\n\n每个人每天都在不断的成长，不同的时间段表现不一样，孩子的时候成长表现在身体上，家长会对孩子说你长高了长大了；成人之后成长体现在言行举止上,周围的人说懂事了有礼貌了；再大一点之后成长体现在思想境界上，傍边经常交流的人会说看问题比较深入等等这都是成长,我们一生都在不断的成长,在不同的环境中我们也有不同的收获，有收获就能成长。\n<!-- more -->\n我的孩子马上就要三周岁了，她在不经意间让我觉得她长大了，有些事情让我感到吃惊，我想我应该写下了记录我孩子的成长。\n按照惯例女儿每天睡觉都要妈妈抱着才能睡，有一天晚上妻子不舒服，我让妻子先去休息,过不大一会儿孩子跑进来说妈妈起来报我睡，我过去跟她说宝贝妈妈今天不舒服我们今天不要妈妈抱抱好不好，小家伙一点面子不给果断说不好我要妈妈抱抱睡。我耐心的给她说今天不行，爸爸抱你好不好，她依然固执的说不好。她开始哭了，刚开始只是撒娇是的小小哭闹，一会儿就开始大哭了，简直是私心裂肺的哭了，我提醒妻子不能放弃在坚持一下。\n她不仅仅是哭了，她爬上床一边撕心裂肺的哭一边抱着妻子的头哭着说:“妈妈不要这样好吗，妈妈不要这样好吗，妈妈你起来抱抱我”，我们在旁边听了也很心疼，我还是跟她说宝贝妈妈不舒服了不能抱你了，你乖一点爸爸抱你睡觉好不好，她还是坚持要妻子抱，我有点生气了但是我忍着自己心里的怒气还是耐心的对他说爸爸抱，一边也用老一八来吓唬她，她由于害怕跑到我的怀里，稍微安静了一会儿。\n女儿两岁之前百分之九十的时间都是和妻子一起，所以她非常粘着妻子，关键时候我这个老爸也靠边站，这是第一次我跟妻子说你要让她跟别人稍微亲近一点，要不然妻子一个人太累了，但是通过这件事我发现这是在帮助她成长，她对妈妈太过依赖不利于她的成长尽管她还只是两岁多的孩子，我还是希望能和她慢慢沟通，同时我发现她在语言表达情感沟通上的成长超乎我的想像，她居然能对妈妈说别这样好吗。\n那一刻我觉得我女儿长大了，但是在那之前我一点也不觉得她长大了，感觉她突然一下子就长大了，我也反思自己太忽略了孩子的成长，没有留意到孩子成长的细节，小孩的脑袋非常灵敏，自那之后我不断的找机会讨好她，和她一起玩耍，陪她一起成长。\n现在她也愿意我抱她睡觉了，也愿意听我们讲道理给他听了，但是她的耐心还是不够，就像刚才非要自己刷牙，我们要帮他刷他就着急直接把牙杯扔到地上了，我们需要更大的耐心去引导她克服她这种急躁的缺点。\n当我们发现成长似乎只在一瞬间时，是我们忽略了生活，我们不要一瞬间的成长，我陪你一起长大。\n\n\n\n","source":"_posts/20170606.md","raw":"---\ntitle: 吾家有女（三）\ndate: 2017-06-07 22:03:26\ncategory: 生活\ntags: \n  -生活\n---\n成长仿佛只是一瞬间。\n\n每个人每天都在不断的成长，不同的时间段表现不一样，孩子的时候成长表现在身体上，家长会对孩子说你长高了长大了；成人之后成长体现在言行举止上,周围的人说懂事了有礼貌了；再大一点之后成长体现在思想境界上，傍边经常交流的人会说看问题比较深入等等这都是成长,我们一生都在不断的成长,在不同的环境中我们也有不同的收获，有收获就能成长。\n<!-- more -->\n我的孩子马上就要三周岁了，她在不经意间让我觉得她长大了，有些事情让我感到吃惊，我想我应该写下了记录我孩子的成长。\n按照惯例女儿每天睡觉都要妈妈抱着才能睡，有一天晚上妻子不舒服，我让妻子先去休息,过不大一会儿孩子跑进来说妈妈起来报我睡，我过去跟她说宝贝妈妈今天不舒服我们今天不要妈妈抱抱好不好，小家伙一点面子不给果断说不好我要妈妈抱抱睡。我耐心的给她说今天不行，爸爸抱你好不好，她依然固执的说不好。她开始哭了，刚开始只是撒娇是的小小哭闹，一会儿就开始大哭了，简直是私心裂肺的哭了，我提醒妻子不能放弃在坚持一下。\n她不仅仅是哭了，她爬上床一边撕心裂肺的哭一边抱着妻子的头哭着说:“妈妈不要这样好吗，妈妈不要这样好吗，妈妈你起来抱抱我”，我们在旁边听了也很心疼，我还是跟她说宝贝妈妈不舒服了不能抱你了，你乖一点爸爸抱你睡觉好不好，她还是坚持要妻子抱，我有点生气了但是我忍着自己心里的怒气还是耐心的对他说爸爸抱，一边也用老一八来吓唬她，她由于害怕跑到我的怀里，稍微安静了一会儿。\n女儿两岁之前百分之九十的时间都是和妻子一起，所以她非常粘着妻子，关键时候我这个老爸也靠边站，这是第一次我跟妻子说你要让她跟别人稍微亲近一点，要不然妻子一个人太累了，但是通过这件事我发现这是在帮助她成长，她对妈妈太过依赖不利于她的成长尽管她还只是两岁多的孩子，我还是希望能和她慢慢沟通，同时我发现她在语言表达情感沟通上的成长超乎我的想像，她居然能对妈妈说别这样好吗。\n那一刻我觉得我女儿长大了，但是在那之前我一点也不觉得她长大了，感觉她突然一下子就长大了，我也反思自己太忽略了孩子的成长，没有留意到孩子成长的细节，小孩的脑袋非常灵敏，自那之后我不断的找机会讨好她，和她一起玩耍，陪她一起成长。\n现在她也愿意我抱她睡觉了，也愿意听我们讲道理给他听了，但是她的耐心还是不够，就像刚才非要自己刷牙，我们要帮他刷他就着急直接把牙杯扔到地上了，我们需要更大的耐心去引导她克服她这种急躁的缺点。\n当我们发现成长似乎只在一瞬间时，是我们忽略了生活，我们不要一瞬间的成长，我陪你一起长大。\n\n\n\n","slug":"20170606","published":1,"updated":"2017-06-07T15:05:33.069Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj70mt0o9000060tus60zt94y","content":"<p>成长仿佛只是一瞬间。</p>\n<p>每个人每天都在不断的成长，不同的时间段表现不一样，孩子的时候成长表现在身体上，家长会对孩子说你长高了长大了；成人之后成长体现在言行举止上,周围的人说懂事了有礼貌了；再大一点之后成长体现在思想境界上，傍边经常交流的人会说看问题比较深入等等这都是成长,我们一生都在不断的成长,在不同的环境中我们也有不同的收获，有收获就能成长。<br><a id=\"more\"></a><br>我的孩子马上就要三周岁了，她在不经意间让我觉得她长大了，有些事情让我感到吃惊，我想我应该写下了记录我孩子的成长。<br>按照惯例女儿每天睡觉都要妈妈抱着才能睡，有一天晚上妻子不舒服，我让妻子先去休息,过不大一会儿孩子跑进来说妈妈起来报我睡，我过去跟她说宝贝妈妈今天不舒服我们今天不要妈妈抱抱好不好，小家伙一点面子不给果断说不好我要妈妈抱抱睡。我耐心的给她说今天不行，爸爸抱你好不好，她依然固执的说不好。她开始哭了，刚开始只是撒娇是的小小哭闹，一会儿就开始大哭了，简直是私心裂肺的哭了，我提醒妻子不能放弃在坚持一下。<br>她不仅仅是哭了，她爬上床一边撕心裂肺的哭一边抱着妻子的头哭着说:“妈妈不要这样好吗，妈妈不要这样好吗，妈妈你起来抱抱我”，我们在旁边听了也很心疼，我还是跟她说宝贝妈妈不舒服了不能抱你了，你乖一点爸爸抱你睡觉好不好，她还是坚持要妻子抱，我有点生气了但是我忍着自己心里的怒气还是耐心的对他说爸爸抱，一边也用老一八来吓唬她，她由于害怕跑到我的怀里，稍微安静了一会儿。<br>女儿两岁之前百分之九十的时间都是和妻子一起，所以她非常粘着妻子，关键时候我这个老爸也靠边站，这是第一次我跟妻子说你要让她跟别人稍微亲近一点，要不然妻子一个人太累了，但是通过这件事我发现这是在帮助她成长，她对妈妈太过依赖不利于她的成长尽管她还只是两岁多的孩子，我还是希望能和她慢慢沟通，同时我发现她在语言表达情感沟通上的成长超乎我的想像，她居然能对妈妈说别这样好吗。<br>那一刻我觉得我女儿长大了，但是在那之前我一点也不觉得她长大了，感觉她突然一下子就长大了，我也反思自己太忽略了孩子的成长，没有留意到孩子成长的细节，小孩的脑袋非常灵敏，自那之后我不断的找机会讨好她，和她一起玩耍，陪她一起成长。<br>现在她也愿意我抱她睡觉了，也愿意听我们讲道理给他听了，但是她的耐心还是不够，就像刚才非要自己刷牙，我们要帮他刷他就着急直接把牙杯扔到地上了，我们需要更大的耐心去引导她克服她这种急躁的缺点。<br>当我们发现成长似乎只在一瞬间时，是我们忽略了生活，我们不要一瞬间的成长，我陪你一起长大。</p>\n","excerpt":"<p>成长仿佛只是一瞬间。</p>\n<p>每个人每天都在不断的成长，不同的时间段表现不一样，孩子的时候成长表现在身体上，家长会对孩子说你长高了长大了；成人之后成长体现在言行举止上,周围的人说懂事了有礼貌了；再大一点之后成长体现在思想境界上，傍边经常交流的人会说看问题比较深入等等这都是成长,我们一生都在不断的成长,在不同的环境中我们也有不同的收获，有收获就能成长。<br>","more":"<br>我的孩子马上就要三周岁了，她在不经意间让我觉得她长大了，有些事情让我感到吃惊，我想我应该写下了记录我孩子的成长。<br>按照惯例女儿每天睡觉都要妈妈抱着才能睡，有一天晚上妻子不舒服，我让妻子先去休息,过不大一会儿孩子跑进来说妈妈起来报我睡，我过去跟她说宝贝妈妈今天不舒服我们今天不要妈妈抱抱好不好，小家伙一点面子不给果断说不好我要妈妈抱抱睡。我耐心的给她说今天不行，爸爸抱你好不好，她依然固执的说不好。她开始哭了，刚开始只是撒娇是的小小哭闹，一会儿就开始大哭了，简直是私心裂肺的哭了，我提醒妻子不能放弃在坚持一下。<br>她不仅仅是哭了，她爬上床一边撕心裂肺的哭一边抱着妻子的头哭着说:“妈妈不要这样好吗，妈妈不要这样好吗，妈妈你起来抱抱我”，我们在旁边听了也很心疼，我还是跟她说宝贝妈妈不舒服了不能抱你了，你乖一点爸爸抱你睡觉好不好，她还是坚持要妻子抱，我有点生气了但是我忍着自己心里的怒气还是耐心的对他说爸爸抱，一边也用老一八来吓唬她，她由于害怕跑到我的怀里，稍微安静了一会儿。<br>女儿两岁之前百分之九十的时间都是和妻子一起，所以她非常粘着妻子，关键时候我这个老爸也靠边站，这是第一次我跟妻子说你要让她跟别人稍微亲近一点，要不然妻子一个人太累了，但是通过这件事我发现这是在帮助她成长，她对妈妈太过依赖不利于她的成长尽管她还只是两岁多的孩子，我还是希望能和她慢慢沟通，同时我发现她在语言表达情感沟通上的成长超乎我的想像，她居然能对妈妈说别这样好吗。<br>那一刻我觉得我女儿长大了，但是在那之前我一点也不觉得她长大了，感觉她突然一下子就长大了，我也反思自己太忽略了孩子的成长，没有留意到孩子成长的细节，小孩的脑袋非常灵敏，自那之后我不断的找机会讨好她，和她一起玩耍，陪她一起成长。<br>现在她也愿意我抱她睡觉了，也愿意听我们讲道理给他听了，但是她的耐心还是不够，就像刚才非要自己刷牙，我们要帮他刷他就着急直接把牙杯扔到地上了，我们需要更大的耐心去引导她克服她这种急躁的缺点。<br>当我们发现成长似乎只在一瞬间时，是我们忽略了生活，我们不要一瞬间的成长，我陪你一起长大。</p>"},{"title":"我的文艺情节","date":"2017-03-15T15:47:30.000Z","_content":"我读过一些文艺作品，随着时间的流逝和工作的忙碌，我渐渐的少了很多阅读的机会，但是很怀念那些经典作品，现在将我读过的作品记录在这里，有些作品值得细细品味，反复阅读。\n<!-- more -->\n## 已读\n### 路遥\n* 平凡的世界\n* 在困难的日子里\n* 我和五叔的六次相遇\n* 黄叶在秋风中飘落\n* 惊心动魄的一幕\n* 人生\n* 早晨从中午开始\n\n下面这些没读过\n>月夜静悄悄\n一生中最高兴的一天\n夏\n姐姐\n风雪腊梅\n青松与小红花\n匆匆过客\n痛苦\n\n### 史铁生\n* 午餐半小時\n* 我的遥远的清平湾\n* 命若琴弦\n* 原罪。宿命\n* 我与地坛\n\n### 柳青\n* 创业史\n\n### 莫言\n* 蛙\n","source":"_posts/booklist.md","raw":"---\ntitle: 我的文艺情节\ndate: 2017-03-15 23:47:30\ntags:\n---\n我读过一些文艺作品，随着时间的流逝和工作的忙碌，我渐渐的少了很多阅读的机会，但是很怀念那些经典作品，现在将我读过的作品记录在这里，有些作品值得细细品味，反复阅读。\n<!-- more -->\n## 已读\n### 路遥\n* 平凡的世界\n* 在困难的日子里\n* 我和五叔的六次相遇\n* 黄叶在秋风中飘落\n* 惊心动魄的一幕\n* 人生\n* 早晨从中午开始\n\n下面这些没读过\n>月夜静悄悄\n一生中最高兴的一天\n夏\n姐姐\n风雪腊梅\n青松与小红花\n匆匆过客\n痛苦\n\n### 史铁生\n* 午餐半小時\n* 我的遥远的清平湾\n* 命若琴弦\n* 原罪。宿命\n* 我与地坛\n\n### 柳青\n* 创业史\n\n### 莫言\n* 蛙\n","slug":"booklist","published":1,"updated":"2017-03-15T16:08:46.790Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj70mt0ok000260tuqi1n4fdg","content":"<p>我读过一些文艺作品，随着时间的流逝和工作的忙碌，我渐渐的少了很多阅读的机会，但是很怀念那些经典作品，现在将我读过的作品记录在这里，有些作品值得细细品味，反复阅读。<br><a id=\"more\"></a></p>\n<h2 id=\"已读\"><a href=\"#已读\" class=\"headerlink\" title=\"已读\"></a>已读</h2><h3 id=\"路遥\"><a href=\"#路遥\" class=\"headerlink\" title=\"路遥\"></a>路遥</h3><ul>\n<li>平凡的世界</li>\n<li>在困难的日子里</li>\n<li>我和五叔的六次相遇</li>\n<li>黄叶在秋风中飘落</li>\n<li>惊心动魄的一幕</li>\n<li>人生</li>\n<li>早晨从中午开始</li>\n</ul>\n<p>下面这些没读过</p>\n<blockquote>\n<p>月夜静悄悄<br>一生中最高兴的一天<br>夏<br>姐姐<br>风雪腊梅<br>青松与小红花<br>匆匆过客<br>痛苦</p>\n</blockquote>\n<h3 id=\"史铁生\"><a href=\"#史铁生\" class=\"headerlink\" title=\"史铁生\"></a>史铁生</h3><ul>\n<li>午餐半小時</li>\n<li>我的遥远的清平湾</li>\n<li>命若琴弦</li>\n<li>原罪。宿命</li>\n<li>我与地坛</li>\n</ul>\n<h3 id=\"柳青\"><a href=\"#柳青\" class=\"headerlink\" title=\"柳青\"></a>柳青</h3><ul>\n<li>创业史</li>\n</ul>\n<h3 id=\"莫言\"><a href=\"#莫言\" class=\"headerlink\" title=\"莫言\"></a>莫言</h3><ul>\n<li>蛙</li>\n</ul>\n","excerpt":"<p>我读过一些文艺作品，随着时间的流逝和工作的忙碌，我渐渐的少了很多阅读的机会，但是很怀念那些经典作品，现在将我读过的作品记录在这里，有些作品值得细细品味，反复阅读。<br>","more":"</p>\n<h2 id=\"已读\"><a href=\"#已读\" class=\"headerlink\" title=\"已读\"></a>已读</h2><h3 id=\"路遥\"><a href=\"#路遥\" class=\"headerlink\" title=\"路遥\"></a>路遥</h3><ul>\n<li>平凡的世界</li>\n<li>在困难的日子里</li>\n<li>我和五叔的六次相遇</li>\n<li>黄叶在秋风中飘落</li>\n<li>惊心动魄的一幕</li>\n<li>人生</li>\n<li>早晨从中午开始</li>\n</ul>\n<p>下面这些没读过</p>\n<blockquote>\n<p>月夜静悄悄<br>一生中最高兴的一天<br>夏<br>姐姐<br>风雪腊梅<br>青松与小红花<br>匆匆过客<br>痛苦</p>\n</blockquote>\n<h3 id=\"史铁生\"><a href=\"#史铁生\" class=\"headerlink\" title=\"史铁生\"></a>史铁生</h3><ul>\n<li>午餐半小時</li>\n<li>我的遥远的清平湾</li>\n<li>命若琴弦</li>\n<li>原罪。宿命</li>\n<li>我与地坛</li>\n</ul>\n<h3 id=\"柳青\"><a href=\"#柳青\" class=\"headerlink\" title=\"柳青\"></a>柳青</h3><ul>\n<li>创业史</li>\n</ul>\n<h3 id=\"莫言\"><a href=\"#莫言\" class=\"headerlink\" title=\"莫言\"></a>莫言</h3><ul>\n<li>蛙</li>\n</ul>"},{"title":"聪明人都使用方格子笔记本","date":"2017-03-09T15:57:54.000Z","_content":"很久没有静下心来读一本书了，上次读书已经是在三个月前的事了，今天看到论坛上看到网友推荐经典书籍，仔细研究了一番发现确实有许多经典的书目，真是爱不释手，想到自己这么久没读书了觉得惭愧又内疚，人每天总是需要学习进步的，我堕落了太久幸好今天看到了这个帖子，决定从今天开始有计划的阅读，厚积薄发慢慢积累，今天主要读了聪明人都使用方格子笔记本，读完之后发现自己堕落了太多，基本上退化为低等生命了，因为我们基本上已经失去了思考的能力，与当今社会的精英阶层差距深远。\n<!-- more -->\n这本书根据读者处于不同的年龄和不同的社会角色进行了分析，方格子笔记本确实有很多的方便地方，但是关键还在于我们懂不懂得去做笔记和学习，我觉得我自己这方面是没有做位的，所以我需要加强这方面的能力培养，这是一个好的习惯，我们笔记不但要记录东西还要思考分析，更要总结行动，不然所谓笔记只是浪费时间浪费资源没有任何益处。\n\n在此之前我的思维有些被固化了，我的笔记单调无聊，毫无乐趣可言，基本都是文字，格式完全是不拘一格，没有黄金三分之说，当自己偶尔翻出笔记时也能回想起来很多，但是效率相对方格子笔记本还是会低许多。\n我要打开思路，不断的学习，慢慢提升自己。也许记个好的笔记就是个好的开始。\n","source":"_posts/cleverboysusegoodnotebook.md","raw":"---\ntitle: 聪明人都使用方格子笔记本\ndate: 2017-03-09 23:57:54\ntag: 阅读\n---\n很久没有静下心来读一本书了，上次读书已经是在三个月前的事了，今天看到论坛上看到网友推荐经典书籍，仔细研究了一番发现确实有许多经典的书目，真是爱不释手，想到自己这么久没读书了觉得惭愧又内疚，人每天总是需要学习进步的，我堕落了太久幸好今天看到了这个帖子，决定从今天开始有计划的阅读，厚积薄发慢慢积累，今天主要读了聪明人都使用方格子笔记本，读完之后发现自己堕落了太多，基本上退化为低等生命了，因为我们基本上已经失去了思考的能力，与当今社会的精英阶层差距深远。\n<!-- more -->\n这本书根据读者处于不同的年龄和不同的社会角色进行了分析，方格子笔记本确实有很多的方便地方，但是关键还在于我们懂不懂得去做笔记和学习，我觉得我自己这方面是没有做位的，所以我需要加强这方面的能力培养，这是一个好的习惯，我们笔记不但要记录东西还要思考分析，更要总结行动，不然所谓笔记只是浪费时间浪费资源没有任何益处。\n\n在此之前我的思维有些被固化了，我的笔记单调无聊，毫无乐趣可言，基本都是文字，格式完全是不拘一格，没有黄金三分之说，当自己偶尔翻出笔记时也能回想起来很多，但是效率相对方格子笔记本还是会低许多。\n我要打开思路，不断的学习，慢慢提升自己。也许记个好的笔记就是个好的开始。\n","slug":"cleverboysusegoodnotebook","published":1,"updated":"2017-03-09T16:28:00.247Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj70mt0p0000660tucsr32ywl","content":"<p>很久没有静下心来读一本书了，上次读书已经是在三个月前的事了，今天看到论坛上看到网友推荐经典书籍，仔细研究了一番发现确实有许多经典的书目，真是爱不释手，想到自己这么久没读书了觉得惭愧又内疚，人每天总是需要学习进步的，我堕落了太久幸好今天看到了这个帖子，决定从今天开始有计划的阅读，厚积薄发慢慢积累，今天主要读了聪明人都使用方格子笔记本，读完之后发现自己堕落了太多，基本上退化为低等生命了，因为我们基本上已经失去了思考的能力，与当今社会的精英阶层差距深远。<br><a id=\"more\"></a><br>这本书根据读者处于不同的年龄和不同的社会角色进行了分析，方格子笔记本确实有很多的方便地方，但是关键还在于我们懂不懂得去做笔记和学习，我觉得我自己这方面是没有做位的，所以我需要加强这方面的能力培养，这是一个好的习惯，我们笔记不但要记录东西还要思考分析，更要总结行动，不然所谓笔记只是浪费时间浪费资源没有任何益处。</p>\n<p>在此之前我的思维有些被固化了，我的笔记单调无聊，毫无乐趣可言，基本都是文字，格式完全是不拘一格，没有黄金三分之说，当自己偶尔翻出笔记时也能回想起来很多，但是效率相对方格子笔记本还是会低许多。<br>我要打开思路，不断的学习，慢慢提升自己。也许记个好的笔记就是个好的开始。</p>\n","excerpt":"<p>很久没有静下心来读一本书了，上次读书已经是在三个月前的事了，今天看到论坛上看到网友推荐经典书籍，仔细研究了一番发现确实有许多经典的书目，真是爱不释手，想到自己这么久没读书了觉得惭愧又内疚，人每天总是需要学习进步的，我堕落了太久幸好今天看到了这个帖子，决定从今天开始有计划的阅读，厚积薄发慢慢积累，今天主要读了聪明人都使用方格子笔记本，读完之后发现自己堕落了太多，基本上退化为低等生命了，因为我们基本上已经失去了思考的能力，与当今社会的精英阶层差距深远。<br>","more":"<br>这本书根据读者处于不同的年龄和不同的社会角色进行了分析，方格子笔记本确实有很多的方便地方，但是关键还在于我们懂不懂得去做笔记和学习，我觉得我自己这方面是没有做位的，所以我需要加强这方面的能力培养，这是一个好的习惯，我们笔记不但要记录东西还要思考分析，更要总结行动，不然所谓笔记只是浪费时间浪费资源没有任何益处。</p>\n<p>在此之前我的思维有些被固化了，我的笔记单调无聊，毫无乐趣可言，基本都是文字，格式完全是不拘一格，没有黄金三分之说，当自己偶尔翻出笔记时也能回想起来很多，但是效率相对方格子笔记本还是会低许多。<br>我要打开思路，不断的学习，慢慢提升自己。也许记个好的笔记就是个好的开始。</p>"},{"title":"大道至简之绘画","date":"2017-03-09T16:00:00.000Z","_content":"\n简单是最难的复杂\n<!-- more -->\n人类对简单的追求永无止境，从钻木取火到电子打火，一切都是由繁到简，而简单是最难的复杂，人们为了享受简单便捷必须要克服最难的复杂，一旦克服则简单只剩简单了。\n绘画在我的心里一直属于艺术范畴，而我一直认为自己是一个没有任何艺术细胞的人，直到我看到了 30天学会绘画，我改变了我的观点，只要方法的当复杂的事情也可以变得异常简单，比如立体画我曾经以为只有艺术大师才能驾驭的东西，今天似乎只要花点时间就都可以高定了。\n这本书如同一股清凉的泉水，我喝到了全身清爽。\n简单是最难的复杂，但天下事有难易乎，为之则难之亦易矣，不为则易者亦难矣。\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"http://music.163.com/outchain/player?type=2&id=430685732&auto=1&height=66\"></iframe>\n\n","source":"_posts/dadaozhijian.md","raw":"---\ntitle: 大道至简之绘画\ndate: 2017-03-10\ntags: 阅读\n---\n\n简单是最难的复杂\n<!-- more -->\n人类对简单的追求永无止境，从钻木取火到电子打火，一切都是由繁到简，而简单是最难的复杂，人们为了享受简单便捷必须要克服最难的复杂，一旦克服则简单只剩简单了。\n绘画在我的心里一直属于艺术范畴，而我一直认为自己是一个没有任何艺术细胞的人，直到我看到了 30天学会绘画，我改变了我的观点，只要方法的当复杂的事情也可以变得异常简单，比如立体画我曾经以为只有艺术大师才能驾驭的东西，今天似乎只要花点时间就都可以高定了。\n这本书如同一股清凉的泉水，我喝到了全身清爽。\n简单是最难的复杂，但天下事有难易乎，为之则难之亦易矣，不为则易者亦难矣。\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"http://music.163.com/outchain/player?type=2&id=430685732&auto=1&height=66\"></iframe>\n\n","slug":"dadaozhijian","published":1,"updated":"2017-03-12T01:52:48.986Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj70mt0p8000860tugthcweur","content":"<p>简单是最难的复杂<br><a id=\"more\"></a><br>人类对简单的追求永无止境，从钻木取火到电子打火，一切都是由繁到简，而简单是最难的复杂，人们为了享受简单便捷必须要克服最难的复杂，一旦克服则简单只剩简单了。<br>绘画在我的心里一直属于艺术范畴，而我一直认为自己是一个没有任何艺术细胞的人，直到我看到了 30天学会绘画，我改变了我的观点，只要方法的当复杂的事情也可以变得异常简单，比如立体画我曾经以为只有艺术大师才能驾驭的东西，今天似乎只要花点时间就都可以高定了。<br>这本书如同一股清凉的泉水，我喝到了全身清爽。<br>简单是最难的复杂，但天下事有难易乎，为之则难之亦易矣，不为则易者亦难矣。</p>\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=\"330\" height=\"86\" src=\"http://music.163.com/outchain/player?type=2&id=430685732&auto=1&height=66\"></iframe>\n\n","excerpt":"<p>简单是最难的复杂<br>","more":"<br>人类对简单的追求永无止境，从钻木取火到电子打火，一切都是由繁到简，而简单是最难的复杂，人们为了享受简单便捷必须要克服最难的复杂，一旦克服则简单只剩简单了。<br>绘画在我的心里一直属于艺术范畴，而我一直认为自己是一个没有任何艺术细胞的人，直到我看到了 30天学会绘画，我改变了我的观点，只要方法的当复杂的事情也可以变得异常简单，比如立体画我曾经以为只有艺术大师才能驾驭的东西，今天似乎只要花点时间就都可以高定了。<br>这本书如同一股清凉的泉水，我喝到了全身清爽。<br>简单是最难的复杂，但天下事有难易乎，为之则难之亦易矣，不为则易者亦难矣。</p>\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"http://music.163.com/outchain/player?type=2&id=430685732&auto=1&height=66\"></iframe>"},{"title":"吾家有女（二）","date":"2017-05-09T16:00:00.000Z","_content":"多米诺骨牌  --程序员的玩家\n<!-- more -->\n今天在京东商城买的多米诺骨牌到了，陪小宝玩了几下，感觉多米诺骨牌是大人的玩具更多一点，小孩子暂时还是不能体会这种乐趣。\n不过我倒是发现这多米诺骨牌和写程序是一个样子，搭牌的过程就像是在编程序，想各种机关各种花样模式，最后一下启动就像程序完成编译运行一样，如果我们的牌按照我们设定的机关顺利走完感觉上是一种享受^_^,但是如果没有那就是程序出了bug需要重新调整。\n小宝只是在我们完成的时候高兴的叫起来说真好玩，但是他是不懂摆弄的，就像她看到程序展示出来的东西感觉很漂亮，但她并不懂得这为什么这么好看漂亮。\n多米诺骨牌是程序员的玩具。\n小宝每天都会给我惊喜，今天晚上我说我要去楼下买东西，她说要去妻子和岳母叫她玩玩具她说不玩了，我要出门了，她已经非常自如的把事情翻译清楚了，她心里明白买东西和出门下楼是一回事，她甚至可以熟练的将妻子的家乡话翻译成普通话，成长是一点一滴的但也足以让我感到惊讶和高兴。\n小孩的可爱很难说得清楚，她的微笑，他的哭泣，她生气的样子，她吃饭的样子，她认真做游戏的样子每一样都觉得非常可爱，可爱到不忍心让她在长大，希望他的童年永在。\n最近看着孩子一天一天的长大我在不断的思考教育的问题，小孩子的教育现在是我们家的头等大事了。关于教育真是太多的议论了，当今中国的教育实在是一个怪，我无法想象一个幼儿园都要托关系走后门才能进的去，至于小学中学那就更难，我每每想到自己的孩子即将要面对这残酷的现实我就痛恨这国家为什么不能把基础教育搞好，为什么不能把那些在教育问题上搞怪的无赖全部处理呢。\n真希望我的孩子未来不要面对眼下的这种教育现状，真是可怜。","source":"_posts/duominogu.md","raw":"---\ntitle: 吾家有女（二）\ndate: 2017-05-10\ntags: 生活\n---\n多米诺骨牌  --程序员的玩家\n<!-- more -->\n今天在京东商城买的多米诺骨牌到了，陪小宝玩了几下，感觉多米诺骨牌是大人的玩具更多一点，小孩子暂时还是不能体会这种乐趣。\n不过我倒是发现这多米诺骨牌和写程序是一个样子，搭牌的过程就像是在编程序，想各种机关各种花样模式，最后一下启动就像程序完成编译运行一样，如果我们的牌按照我们设定的机关顺利走完感觉上是一种享受^_^,但是如果没有那就是程序出了bug需要重新调整。\n小宝只是在我们完成的时候高兴的叫起来说真好玩，但是他是不懂摆弄的，就像她看到程序展示出来的东西感觉很漂亮，但她并不懂得这为什么这么好看漂亮。\n多米诺骨牌是程序员的玩具。\n小宝每天都会给我惊喜，今天晚上我说我要去楼下买东西，她说要去妻子和岳母叫她玩玩具她说不玩了，我要出门了，她已经非常自如的把事情翻译清楚了，她心里明白买东西和出门下楼是一回事，她甚至可以熟练的将妻子的家乡话翻译成普通话，成长是一点一滴的但也足以让我感到惊讶和高兴。\n小孩的可爱很难说得清楚，她的微笑，他的哭泣，她生气的样子，她吃饭的样子，她认真做游戏的样子每一样都觉得非常可爱，可爱到不忍心让她在长大，希望他的童年永在。\n最近看着孩子一天一天的长大我在不断的思考教育的问题，小孩子的教育现在是我们家的头等大事了。关于教育真是太多的议论了，当今中国的教育实在是一个怪，我无法想象一个幼儿园都要托关系走后门才能进的去，至于小学中学那就更难，我每每想到自己的孩子即将要面对这残酷的现实我就痛恨这国家为什么不能把基础教育搞好，为什么不能把那些在教育问题上搞怪的无赖全部处理呢。\n真希望我的孩子未来不要面对眼下的这种教育现状，真是可怜。","slug":"duominogu","published":1,"updated":"2017-05-10T16:32:01.021Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj70mt0pg000960tu6xsgjttb","content":"<p>多米诺骨牌  –程序员的玩家<br><a id=\"more\"></a><br>今天在京东商城买的多米诺骨牌到了，陪小宝玩了几下，感觉多米诺骨牌是大人的玩具更多一点，小孩子暂时还是不能体会这种乐趣。<br>不过我倒是发现这多米诺骨牌和写程序是一个样子，搭牌的过程就像是在编程序，想各种机关各种花样模式，最后一下启动就像程序完成编译运行一样，如果我们的牌按照我们设定的机关顺利走完感觉上是一种享受^_^,但是如果没有那就是程序出了bug需要重新调整。<br>小宝只是在我们完成的时候高兴的叫起来说真好玩，但是他是不懂摆弄的，就像她看到程序展示出来的东西感觉很漂亮，但她并不懂得这为什么这么好看漂亮。<br>多米诺骨牌是程序员的玩具。<br>小宝每天都会给我惊喜，今天晚上我说我要去楼下买东西，她说要去妻子和岳母叫她玩玩具她说不玩了，我要出门了，她已经非常自如的把事情翻译清楚了，她心里明白买东西和出门下楼是一回事，她甚至可以熟练的将妻子的家乡话翻译成普通话，成长是一点一滴的但也足以让我感到惊讶和高兴。<br>小孩的可爱很难说得清楚，她的微笑，他的哭泣，她生气的样子，她吃饭的样子，她认真做游戏的样子每一样都觉得非常可爱，可爱到不忍心让她在长大，希望他的童年永在。<br>最近看着孩子一天一天的长大我在不断的思考教育的问题，小孩子的教育现在是我们家的头等大事了。关于教育真是太多的议论了，当今中国的教育实在是一个怪，我无法想象一个幼儿园都要托关系走后门才能进的去，至于小学中学那就更难，我每每想到自己的孩子即将要面对这残酷的现实我就痛恨这国家为什么不能把基础教育搞好，为什么不能把那些在教育问题上搞怪的无赖全部处理呢。<br>真希望我的孩子未来不要面对眼下的这种教育现状，真是可怜。</p>\n","excerpt":"<p>多米诺骨牌  –程序员的玩家<br>","more":"<br>今天在京东商城买的多米诺骨牌到了，陪小宝玩了几下，感觉多米诺骨牌是大人的玩具更多一点，小孩子暂时还是不能体会这种乐趣。<br>不过我倒是发现这多米诺骨牌和写程序是一个样子，搭牌的过程就像是在编程序，想各种机关各种花样模式，最后一下启动就像程序完成编译运行一样，如果我们的牌按照我们设定的机关顺利走完感觉上是一种享受^_^,但是如果没有那就是程序出了bug需要重新调整。<br>小宝只是在我们完成的时候高兴的叫起来说真好玩，但是他是不懂摆弄的，就像她看到程序展示出来的东西感觉很漂亮，但她并不懂得这为什么这么好看漂亮。<br>多米诺骨牌是程序员的玩具。<br>小宝每天都会给我惊喜，今天晚上我说我要去楼下买东西，她说要去妻子和岳母叫她玩玩具她说不玩了，我要出门了，她已经非常自如的把事情翻译清楚了，她心里明白买东西和出门下楼是一回事，她甚至可以熟练的将妻子的家乡话翻译成普通话，成长是一点一滴的但也足以让我感到惊讶和高兴。<br>小孩的可爱很难说得清楚，她的微笑，他的哭泣，她生气的样子，她吃饭的样子，她认真做游戏的样子每一样都觉得非常可爱，可爱到不忍心让她在长大，希望他的童年永在。<br>最近看着孩子一天一天的长大我在不断的思考教育的问题，小孩子的教育现在是我们家的头等大事了。关于教育真是太多的议论了，当今中国的教育实在是一个怪，我无法想象一个幼儿园都要托关系走后门才能进的去，至于小学中学那就更难，我每每想到自己的孩子即将要面对这残酷的现实我就痛恨这国家为什么不能把基础教育搞好，为什么不能把那些在教育问题上搞怪的无赖全部处理呢。<br>真希望我的孩子未来不要面对眼下的这种教育现状，真是可怜。</p>"},{"title":"find awk 和 grep 使用总结","date":"2017-02-23T15:57:54.000Z","_content":"find awk 和 grep 使用总结\n使用linux工作经常会用到find这个工具去查找文件，找到的文件可以通过awk进行操作，也可以配合grep\n一起操作，这三个工具一起使用基本上可以高效的完成linux下百分之九十的文本处理工作，今天主要记录一下这三个\n工具的使用方法。\n<!--more-->\n## find\n如果我什么参数也不给，find会打印出当前目录下所有的文件和目录名\n``` bash\nfind\n.\n./_posts\n./_posts/hello-world.md\n./_posts/findandawk.md\n./_posts/test.md\n```\n这里有几个默认的参数\n1. 默认查找的目录为当前目录\n1. 默认查找匹配所有记录\n1. 默认打印输出匹配到的记录-print\n上面的这个find与下面的find输出一致\n\n``` bash\nfind  ./ -print -name \"*\"\n./\n./-name\n./_posts\n./_posts/hello-world.md\n./_posts/findandawk.md\n./_posts/test.md\n```\n-print0可以去掉换行\n``` bash\nfind  ./ -print0 -name \"*\"\n././-name./_posts./_posts/hello-world.md./_posts/findandawk.md./_posts/test.md\n```\n-printf 可以格式化输出\n```bash\nfind . -type f -printf \" %Tc %p\\n\"\n Mon 27 Feb 2017 08:54:40 PM CST ./-name\n Mon 27 Feb 2017 12:31:29 AM CST ./_posts/hello-world.md\n Mon 27 Feb 2017 09:01:12 PM CST ./_posts/findandawk.md\n Sat 25 Feb 2017 01:56:06 AM CST ./_posts/test.md\n```\n格式化输出的参数有很多这里不一一列举具体可以参考手册 man find 看更多printf的相关内容\n当然输出不是find的主要功能，find 主要还是查找功能。\nfind 可以按照文件名称，文件类型，权限权限，访问时间等等多种维度去查找,但是我们一般使用最多的是按照文件名称来查找的\n```bash\n按文件名查找\nfind ./ -name \"*.md\"\n./_posts/hello-world.md\n./_posts/findandawk.md\n./_posts/test.md\n按文件类型查找\nfind ./ -type f -name \"*.md\"\n./_posts/hello-world.md\n./_posts/findandawk.md\n./_posts/test.md\n\n一天以前的文件\nfind ./ -mtime +1\n```\nfind还有一个exec的参数可以对找到的文件执行相应的操作，但是这个有一定的危险如果你执行的动作是\n删除的话请慎重使用，但是rm在-exec里出现的使用频率往往很高\n```bash\n用ls列出找到的文件\n./ -mtime +1 -exec ls -l {} \\;\n-rw-rw-r--. 1 fangqing fangqing 5456 Feb 25 01:56 ./_posts/test.md\n强制删除当前目录下所有日志，这个动作很危险，要注意检查防止误操作\nfind ./ -name \"*.log\"  -exec rm -f {} \\;\n```\nfind的用法和参数实在是太多，有些高级技巧在需要的时候我就查一下手册，上面find的这些查找参数可以满足日常百分之九十的查找工作。\n# awk\nawk是一门脚本语言，awk是三位作者名字的首字母，关于awk起源的内容不写了。\nawk是linux下文本处理的瑞士军刀，你可以通过awk把字符串按照你想要的格式进行切分、合并、拼接、转换\n甚至算术运算。\n首先需要记录一下awk的几个内置变量\n```bash\nawk\n    ARGC        参数数量\n    ARGIND      当前处理文件在ARGV中的索引\n    ARGV        命令行参数数组\n    FILENAME    当前处理文件的文件名\n    FS          输入字段的分割符\n    NF          记录字段数量.\n    NR          当前输入的记录数量.\n    OFMT        数字输出格式, 默认为\"%.6g\".\n    OFS         输出字段分隔符.\n    ORS         输出记录分割符.\n    RS          输入记录分割符号.\n```\n这几个变量对awk处理文本有很大影响基本常用就是这几个变量下面记录几个案例\n```bash\n./ -print -name \"*\" -exec ls -l {} \\; |awk '{print $NF}'\n./\n-name\n_posts\n./-name\n./-name\n./_posts\nfindandawk.md\nhello-world.md\ntest.md\n./_posts/hello-world.md\n./_posts/hello-world.md\n./_posts/findandawk.md\n./_posts/findandawk.md\n./_posts/test.md\n./_posts/test.md\n````\n-F 可以指定字段分隔符，不指定时默认为空白字符，$NF输出最后一个字段.\n\n## grep\nLinux系统中grep命令是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹 配的行打印出来。grep全称是Global Regular Expression Print，表示全局正则表达式版本，它的使用权限是所有用户。\n1. 格式\ngrep [options]\n\n1. 主要参数\n```\n[options]主要参数：\n－c：只输出匹配行的计数。\n－I：不区分大小写(只适用于单字符)。\n－h：查询多文件时不显示文件名。\n－l：查询多文件时只输出包含匹配字符的文件名。\n－n：显示匹配行及行号。\n－s：不显示不存在或无匹配文本的错误信息。\n－v：显示不包含匹配文本的所有行。\npattern正则表达式主要参数：\n\\： 忽略正则表达式中特殊字符的原有含义。\n^：匹配正则表达式的开始行。\n$: 匹配正则表达式的结束行。\n\\<：从匹配正则表达式单词边界。\n\\>：到匹配正则表达式单词边界。\n[ ]：单个字符，如[A]即A符合要求 。\n[ - ]：范围，如[A-Z]，即A、B、C一直到Z都符合要求 。\n.：所有的单个字符。\n* ：有字符，长度可以为0。\n```\n1. grep命令使用简单实例\n$ grep ‘ERROR’ d*\n匹配当前目录所有以d开头的文件中包含ERROR的行。\n$ grep ‘ERROR’ a.log b.log c.log\n显示在a.log b.log c.log 文件中匹配ERROR的行。\n$ grep ‘[a-z]\\{5\\}’ a.log\n显示所有包含每个字符串至少有5个连续小写字符的字符串的行。\n$ grep ‘w\\(es\\)t.*\\1′ a.log\n如果west被匹配，则es就被存储到内存中，并标记为1，然后搜索任意个字符(.*)，这些字符后面紧跟着 另外一个es(\\1)，找到就显示该行。如果用egrep或grep -E，就不用”\\”号进行转义，直接写成’w(es)t.*\\1′就可以了。\n\n1. grep高级使用技巧\n假设您正在’/usr/local/spark/’目录下搜索带字符 串’spark’的文件：\n$ grep “spark” /usr/local/spark/*\n默认情况下，’grep’只搜索当前目录。遇到子目录，’grep’报出：\ngrep: spark: Is a directory\n-r选项可以第归搜索：grep -r\n也可以忽略子目录：grep -d skip\n\n1. grep其他参数：\ngrep -i pattern files ：不区分大小写地搜索。默认情况区分大小写，\ngrep -l pattern files ：只列出匹配的文件名，\ngrep -L pattern files ：列出不匹配的文件名，\ngrep -w pattern files ：只匹配整个单词，而不是字符串的一部分(如匹配’food’，而不是’foo’)，\ngrep -C number pattern files ：匹配的上下文分别显示[number]行，\n如 grep -C 3 \"hexo\" source/_posts/* 查找hexo输出hexo前后各三行\ngrep pattern1 | pattern2 files ：显示匹配 pattern1 或 pattern2 的行，\ngrep pattern1 files | grep pattern2 ：显示既匹配 pattern1 又匹配 pattern2 的行。\ngrep -n pattern files  即可显示行号信息\ngrep -c pattern files  即可查找总行数\n\n1. grep 正则表达式：\n\\< 和 \\> 分别标注单词的开始与结尾。\n例如：\ngrep good * 会匹配 ‘goodddd’、’ggood’、’good’等，\ngrep ‘\\<good’ * 匹配’goodddd’和’good’，但不是’ggood’，\ngrep ‘\\<good\\>’ 只匹配’good’。\n‘^’：指匹配的字符串在行首，\n‘$’：指匹配的字符串在行 尾，\n11. grep正则表达式可以使用类名\n可以使用国际模式匹配的类名：\n[[:upper:]]   [A-Z]\n[[:lower:]]   [a-z]\n[[:digit:]]   [0-9]\n[[:alnum:]]   [0-9a-zA-Z]\n[[:space:]]   空格或tab\n[[:alpha:]]   [a-zA-Z]\ngrep '#[[:upper:]][[:upper:]]' data.doc     #查询以#开头紧跟着两个大写字\ngrep 在linux下使用很多，功能很全面，尤其是正则表达式使用得当可以很好的提升工作效率。\n","source":"_posts/findandawk.md","raw":"---\ntitle: find awk 和 grep 使用总结\ndate: 2017-02-23 23:57:54\ntags: linux\n---\nfind awk 和 grep 使用总结\n使用linux工作经常会用到find这个工具去查找文件，找到的文件可以通过awk进行操作，也可以配合grep\n一起操作，这三个工具一起使用基本上可以高效的完成linux下百分之九十的文本处理工作，今天主要记录一下这三个\n工具的使用方法。\n<!--more-->\n## find\n如果我什么参数也不给，find会打印出当前目录下所有的文件和目录名\n``` bash\nfind\n.\n./_posts\n./_posts/hello-world.md\n./_posts/findandawk.md\n./_posts/test.md\n```\n这里有几个默认的参数\n1. 默认查找的目录为当前目录\n1. 默认查找匹配所有记录\n1. 默认打印输出匹配到的记录-print\n上面的这个find与下面的find输出一致\n\n``` bash\nfind  ./ -print -name \"*\"\n./\n./-name\n./_posts\n./_posts/hello-world.md\n./_posts/findandawk.md\n./_posts/test.md\n```\n-print0可以去掉换行\n``` bash\nfind  ./ -print0 -name \"*\"\n././-name./_posts./_posts/hello-world.md./_posts/findandawk.md./_posts/test.md\n```\n-printf 可以格式化输出\n```bash\nfind . -type f -printf \" %Tc %p\\n\"\n Mon 27 Feb 2017 08:54:40 PM CST ./-name\n Mon 27 Feb 2017 12:31:29 AM CST ./_posts/hello-world.md\n Mon 27 Feb 2017 09:01:12 PM CST ./_posts/findandawk.md\n Sat 25 Feb 2017 01:56:06 AM CST ./_posts/test.md\n```\n格式化输出的参数有很多这里不一一列举具体可以参考手册 man find 看更多printf的相关内容\n当然输出不是find的主要功能，find 主要还是查找功能。\nfind 可以按照文件名称，文件类型，权限权限，访问时间等等多种维度去查找,但是我们一般使用最多的是按照文件名称来查找的\n```bash\n按文件名查找\nfind ./ -name \"*.md\"\n./_posts/hello-world.md\n./_posts/findandawk.md\n./_posts/test.md\n按文件类型查找\nfind ./ -type f -name \"*.md\"\n./_posts/hello-world.md\n./_posts/findandawk.md\n./_posts/test.md\n\n一天以前的文件\nfind ./ -mtime +1\n```\nfind还有一个exec的参数可以对找到的文件执行相应的操作，但是这个有一定的危险如果你执行的动作是\n删除的话请慎重使用，但是rm在-exec里出现的使用频率往往很高\n```bash\n用ls列出找到的文件\n./ -mtime +1 -exec ls -l {} \\;\n-rw-rw-r--. 1 fangqing fangqing 5456 Feb 25 01:56 ./_posts/test.md\n强制删除当前目录下所有日志，这个动作很危险，要注意检查防止误操作\nfind ./ -name \"*.log\"  -exec rm -f {} \\;\n```\nfind的用法和参数实在是太多，有些高级技巧在需要的时候我就查一下手册，上面find的这些查找参数可以满足日常百分之九十的查找工作。\n# awk\nawk是一门脚本语言，awk是三位作者名字的首字母，关于awk起源的内容不写了。\nawk是linux下文本处理的瑞士军刀，你可以通过awk把字符串按照你想要的格式进行切分、合并、拼接、转换\n甚至算术运算。\n首先需要记录一下awk的几个内置变量\n```bash\nawk\n    ARGC        参数数量\n    ARGIND      当前处理文件在ARGV中的索引\n    ARGV        命令行参数数组\n    FILENAME    当前处理文件的文件名\n    FS          输入字段的分割符\n    NF          记录字段数量.\n    NR          当前输入的记录数量.\n    OFMT        数字输出格式, 默认为\"%.6g\".\n    OFS         输出字段分隔符.\n    ORS         输出记录分割符.\n    RS          输入记录分割符号.\n```\n这几个变量对awk处理文本有很大影响基本常用就是这几个变量下面记录几个案例\n```bash\n./ -print -name \"*\" -exec ls -l {} \\; |awk '{print $NF}'\n./\n-name\n_posts\n./-name\n./-name\n./_posts\nfindandawk.md\nhello-world.md\ntest.md\n./_posts/hello-world.md\n./_posts/hello-world.md\n./_posts/findandawk.md\n./_posts/findandawk.md\n./_posts/test.md\n./_posts/test.md\n````\n-F 可以指定字段分隔符，不指定时默认为空白字符，$NF输出最后一个字段.\n\n## grep\nLinux系统中grep命令是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹 配的行打印出来。grep全称是Global Regular Expression Print，表示全局正则表达式版本，它的使用权限是所有用户。\n1. 格式\ngrep [options]\n\n1. 主要参数\n```\n[options]主要参数：\n－c：只输出匹配行的计数。\n－I：不区分大小写(只适用于单字符)。\n－h：查询多文件时不显示文件名。\n－l：查询多文件时只输出包含匹配字符的文件名。\n－n：显示匹配行及行号。\n－s：不显示不存在或无匹配文本的错误信息。\n－v：显示不包含匹配文本的所有行。\npattern正则表达式主要参数：\n\\： 忽略正则表达式中特殊字符的原有含义。\n^：匹配正则表达式的开始行。\n$: 匹配正则表达式的结束行。\n\\<：从匹配正则表达式单词边界。\n\\>：到匹配正则表达式单词边界。\n[ ]：单个字符，如[A]即A符合要求 。\n[ - ]：范围，如[A-Z]，即A、B、C一直到Z都符合要求 。\n.：所有的单个字符。\n* ：有字符，长度可以为0。\n```\n1. grep命令使用简单实例\n$ grep ‘ERROR’ d*\n匹配当前目录所有以d开头的文件中包含ERROR的行。\n$ grep ‘ERROR’ a.log b.log c.log\n显示在a.log b.log c.log 文件中匹配ERROR的行。\n$ grep ‘[a-z]\\{5\\}’ a.log\n显示所有包含每个字符串至少有5个连续小写字符的字符串的行。\n$ grep ‘w\\(es\\)t.*\\1′ a.log\n如果west被匹配，则es就被存储到内存中，并标记为1，然后搜索任意个字符(.*)，这些字符后面紧跟着 另外一个es(\\1)，找到就显示该行。如果用egrep或grep -E，就不用”\\”号进行转义，直接写成’w(es)t.*\\1′就可以了。\n\n1. grep高级使用技巧\n假设您正在’/usr/local/spark/’目录下搜索带字符 串’spark’的文件：\n$ grep “spark” /usr/local/spark/*\n默认情况下，’grep’只搜索当前目录。遇到子目录，’grep’报出：\ngrep: spark: Is a directory\n-r选项可以第归搜索：grep -r\n也可以忽略子目录：grep -d skip\n\n1. grep其他参数：\ngrep -i pattern files ：不区分大小写地搜索。默认情况区分大小写，\ngrep -l pattern files ：只列出匹配的文件名，\ngrep -L pattern files ：列出不匹配的文件名，\ngrep -w pattern files ：只匹配整个单词，而不是字符串的一部分(如匹配’food’，而不是’foo’)，\ngrep -C number pattern files ：匹配的上下文分别显示[number]行，\n如 grep -C 3 \"hexo\" source/_posts/* 查找hexo输出hexo前后各三行\ngrep pattern1 | pattern2 files ：显示匹配 pattern1 或 pattern2 的行，\ngrep pattern1 files | grep pattern2 ：显示既匹配 pattern1 又匹配 pattern2 的行。\ngrep -n pattern files  即可显示行号信息\ngrep -c pattern files  即可查找总行数\n\n1. grep 正则表达式：\n\\< 和 \\> 分别标注单词的开始与结尾。\n例如：\ngrep good * 会匹配 ‘goodddd’、’ggood’、’good’等，\ngrep ‘\\<good’ * 匹配’goodddd’和’good’，但不是’ggood’，\ngrep ‘\\<good\\>’ 只匹配’good’。\n‘^’：指匹配的字符串在行首，\n‘$’：指匹配的字符串在行 尾，\n11. grep正则表达式可以使用类名\n可以使用国际模式匹配的类名：\n[[:upper:]]   [A-Z]\n[[:lower:]]   [a-z]\n[[:digit:]]   [0-9]\n[[:alnum:]]   [0-9a-zA-Z]\n[[:space:]]   空格或tab\n[[:alpha:]]   [a-zA-Z]\ngrep '#[[:upper:]][[:upper:]]' data.doc     #查询以#开头紧跟着两个大写字\ngrep 在linux下使用很多，功能很全面，尤其是正则表达式使用得当可以很好的提升工作效率。\n","slug":"findandawk","published":1,"updated":"2017-02-28T14:48:27.303Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj70mt0pk000c60tucebvddu5","content":"<p>find awk 和 grep 使用总结<br>使用linux工作经常会用到find这个工具去查找文件，找到的文件可以通过awk进行操作，也可以配合grep<br>一起操作，这三个工具一起使用基本上可以高效的完成linux下百分之九十的文本处理工作，今天主要记录一下这三个<br>工具的使用方法。<br><a id=\"more\"></a></p>\n<h2 id=\"find\"><a href=\"#find\" class=\"headerlink\" title=\"find\"></a>find</h2><p>如果我什么参数也不给，find会打印出当前目录下所有的文件和目录名<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">find</div><div class=\"line\">.</div><div class=\"line\">./_posts</div><div class=\"line\">./_posts/hello-world.md</div><div class=\"line\">./_posts/findandawk.md</div><div class=\"line\">./_posts/test.md</div></pre></td></tr></table></figure></p>\n<p>这里有几个默认的参数</p>\n<ol>\n<li>默认查找的目录为当前目录</li>\n<li>默认查找匹配所有记录</li>\n<li>默认打印输出匹配到的记录-print<br>上面的这个find与下面的find输出一致</li>\n</ol>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">find  ./ -print -name <span class=\"string\">\"*\"</span></div><div class=\"line\">./</div><div class=\"line\">./-name</div><div class=\"line\">./_posts</div><div class=\"line\">./_posts/hello-world.md</div><div class=\"line\">./_posts/findandawk.md</div><div class=\"line\">./_posts/test.md</div></pre></td></tr></table></figure>\n<p>-print0可以去掉换行<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">find  ./ -print0 -name <span class=\"string\">\"*\"</span></div><div class=\"line\">././-name./_posts./_posts/hello-world.md./_posts/findandawk.md./_posts/test.md</div></pre></td></tr></table></figure></p>\n<p>-printf 可以格式化输出<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">find . -type f -printf <span class=\"string\">\" %Tc %p\\n\"</span></div><div class=\"line\"> Mon 27 Feb 2017 08:54:40 PM CST ./-name</div><div class=\"line\"> Mon 27 Feb 2017 12:31:29 AM CST ./_posts/hello-world.md</div><div class=\"line\"> Mon 27 Feb 2017 09:01:12 PM CST ./_posts/findandawk.md</div><div class=\"line\"> Sat 25 Feb 2017 01:56:06 AM CST ./_posts/test.md</div></pre></td></tr></table></figure></p>\n<p>格式化输出的参数有很多这里不一一列举具体可以参考手册 man find 看更多printf的相关内容<br>当然输出不是find的主要功能，find 主要还是查找功能。<br>find 可以按照文件名称，文件类型，权限权限，访问时间等等多种维度去查找,但是我们一般使用最多的是按照文件名称来查找的<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">按文件名查找</div><div class=\"line\">find ./ -name <span class=\"string\">\"*.md\"</span></div><div class=\"line\">./_posts/hello-world.md</div><div class=\"line\">./_posts/findandawk.md</div><div class=\"line\">./_posts/test.md</div><div class=\"line\">按文件类型查找</div><div class=\"line\">find ./ -type f -name <span class=\"string\">\"*.md\"</span></div><div class=\"line\">./_posts/hello-world.md</div><div class=\"line\">./_posts/findandawk.md</div><div class=\"line\">./_posts/test.md</div><div class=\"line\"></div><div class=\"line\">一天以前的文件</div><div class=\"line\">find ./ -mtime +1</div></pre></td></tr></table></figure></p>\n<p>find还有一个exec的参数可以对找到的文件执行相应的操作，但是这个有一定的危险如果你执行的动作是<br>删除的话请慎重使用，但是rm在-exec里出现的使用频率往往很高<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">用ls列出找到的文件</div><div class=\"line\">./ -mtime +1 -exec ls <span class=\"_\">-l</span> &#123;&#125; \\;</div><div class=\"line\">-rw-rw-r--. 1 fangqing fangqing 5456 Feb 25 01:56 ./_posts/test.md</div><div class=\"line\">强制删除当前目录下所有日志，这个动作很危险，要注意检查防止误操作</div><div class=\"line\">find ./ -name <span class=\"string\">\"*.log\"</span>  -exec rm <span class=\"_\">-f</span> &#123;&#125; \\;</div></pre></td></tr></table></figure></p>\n<p>find的用法和参数实在是太多，有些高级技巧在需要的时候我就查一下手册，上面find的这些查找参数可以满足日常百分之九十的查找工作。</p>\n<h1 id=\"awk\"><a href=\"#awk\" class=\"headerlink\" title=\"awk\"></a>awk</h1><p>awk是一门脚本语言，awk是三位作者名字的首字母，关于awk起源的内容不写了。<br>awk是linux下文本处理的瑞士军刀，你可以通过awk把字符串按照你想要的格式进行切分、合并、拼接、转换<br>甚至算术运算。<br>首先需要记录一下awk的几个内置变量<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">awk</div><div class=\"line\">    ARGC        参数数量</div><div class=\"line\">    ARGIND      当前处理文件在ARGV中的索引</div><div class=\"line\">    ARGV        命令行参数数组</div><div class=\"line\">    FILENAME    当前处理文件的文件名</div><div class=\"line\">    FS          输入字段的分割符</div><div class=\"line\">    NF          记录字段数量.</div><div class=\"line\">    NR          当前输入的记录数量.</div><div class=\"line\">    OFMT        数字输出格式, 默认为<span class=\"string\">\"%.6g\"</span>.</div><div class=\"line\">    OFS         输出字段分隔符.</div><div class=\"line\">    ORS         输出记录分割符.</div><div class=\"line\">    RS          输入记录分割符号.</div></pre></td></tr></table></figure></p>\n<p>这几个变量对awk处理文本有很大影响基本常用就是这几个变量下面记录几个案例<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">./ -print -name <span class=\"string\">\"*\"</span> -exec ls <span class=\"_\">-l</span> &#123;&#125; \\; |awk <span class=\"string\">'&#123;print $NF&#125;'</span></div><div class=\"line\">./</div><div class=\"line\">-name</div><div class=\"line\">_posts</div><div class=\"line\">./-name</div><div class=\"line\">./-name</div><div class=\"line\">./_posts</div><div class=\"line\">findandawk.md</div><div class=\"line\">hello-world.md</div><div class=\"line\">test.md</div><div class=\"line\">./_posts/hello-world.md</div><div class=\"line\">./_posts/hello-world.md</div><div class=\"line\">./_posts/findandawk.md</div><div class=\"line\">./_posts/findandawk.md</div><div class=\"line\">./_posts/test.md</div><div class=\"line\">./_posts/test.md</div><div class=\"line\">`</div></pre></td></tr></table></figure></p>\n<p>-F 可以指定字段分隔符，不指定时默认为空白字符，$NF输出最后一个字段.</p>\n<h2 id=\"grep\"><a href=\"#grep\" class=\"headerlink\" title=\"grep\"></a>grep</h2><p>Linux系统中grep命令是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹 配的行打印出来。grep全称是Global Regular Expression Print，表示全局正则表达式版本，它的使用权限是所有用户。</p>\n<ol>\n<li><p>格式<br>grep [options]</p>\n</li>\n<li><p>主要参数</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\">[options]主要参数：</div><div class=\"line\">－c：只输出匹配行的计数。</div><div class=\"line\">－I：不区分大小写(只适用于单字符)。</div><div class=\"line\">－h：查询多文件时不显示文件名。</div><div class=\"line\">－l：查询多文件时只输出包含匹配字符的文件名。</div><div class=\"line\">－n：显示匹配行及行号。</div><div class=\"line\">－s：不显示不存在或无匹配文本的错误信息。</div><div class=\"line\">－v：显示不包含匹配文本的所有行。</div><div class=\"line\">pattern正则表达式主要参数：</div><div class=\"line\">\\： 忽略正则表达式中特殊字符的原有含义。</div><div class=\"line\">^：匹配正则表达式的开始行。</div><div class=\"line\">$: 匹配正则表达式的结束行。</div><div class=\"line\">\\&lt;：从匹配正则表达式单词边界。</div><div class=\"line\">\\&gt;：到匹配正则表达式单词边界。</div><div class=\"line\">[ ]：单个字符，如[A]即A符合要求 。</div><div class=\"line\">[ - ]：范围，如[A-Z]，即A、B、C一直到Z都符合要求 。</div><div class=\"line\">.：所有的单个字符。</div><div class=\"line\">* ：有字符，长度可以为0。</div></pre></td></tr></table></figure>\n</li>\n<li><p>grep命令使用简单实例<br>$ grep ‘ERROR’ d<em><br>匹配当前目录所有以d开头的文件中包含ERROR的行。<br>$ grep ‘ERROR’ a.log b.log c.log<br>显示在a.log b.log c.log 文件中匹配ERROR的行。<br>$ grep ‘[a-z]{5}’ a.log<br>显示所有包含每个字符串至少有5个连续小写字符的字符串的行。<br>$ grep ‘w(es)t.</em>\\1′ a.log<br>如果west被匹配，则es就被存储到内存中，并标记为1，然后搜索任意个字符(.<em>)，这些字符后面紧跟着 另外一个es(\\1)，找到就显示该行。如果用egrep或grep -E，就不用”\\”号进行转义，直接写成’w(es)t.</em>\\1′就可以了。</p>\n</li>\n<li><p>grep高级使用技巧<br>假设您正在’/usr/local/spark/’目录下搜索带字符 串’spark’的文件：<br>$ grep “spark” /usr/local/spark/*<br>默认情况下，’grep’只搜索当前目录。遇到子目录，’grep’报出：<br>grep: spark: Is a directory<br>-r选项可以第归搜索：grep -r<br>也可以忽略子目录：grep -d skip</p>\n</li>\n<li><p>grep其他参数：<br>grep -i pattern files ：不区分大小写地搜索。默认情况区分大小写，<br>grep -l pattern files ：只列出匹配的文件名，<br>grep -L pattern files ：列出不匹配的文件名，<br>grep -w pattern files ：只匹配整个单词，而不是字符串的一部分(如匹配’food’，而不是’foo’)，<br>grep -C number pattern files ：匹配的上下文分别显示[number]行，<br>如 grep -C 3 “hexo” source/_posts/* 查找hexo输出hexo前后各三行<br>grep pattern1 | pattern2 files ：显示匹配 pattern1 或 pattern2 的行，<br>grep pattern1 files | grep pattern2 ：显示既匹配 pattern1 又匹配 pattern2 的行。<br>grep -n pattern files  即可显示行号信息<br>grep -c pattern files  即可查找总行数</p>\n</li>\n<li><p>grep 正则表达式：<br>\\&lt; 和 > 分别标注单词的开始与结尾。<br>例如：<br>grep good <em> 会匹配 ‘goodddd’、’ggood’、’good’等，<br>grep ‘\\&lt;good’ </em> 匹配’goodddd’和’good’，但不是’ggood’，<br>grep ‘\\<good\\>’ 只匹配’good’。<br>‘^’：指匹配的字符串在行首，<br>‘$’：指匹配的字符串在行 尾，</good\\></p>\n</li>\n<li>grep正则表达式可以使用类名<br>可以使用国际模式匹配的类名：<br>[[:upper:]]   [A-Z]<br>[[:lower:]]   [a-z]<br>[[:digit:]]   [0-9]<br>[[:alnum:]]   [0-9a-zA-Z]<br>[[:space:]]   空格或tab<br>[[:alpha:]]   [a-zA-Z]<br>grep ‘#[[:upper:]][[:upper:]]’ data.doc     #查询以#开头紧跟着两个大写字<br>grep 在linux下使用很多，功能很全面，尤其是正则表达式使用得当可以很好的提升工作效率。</li>\n</ol>\n","excerpt":"<p>find awk 和 grep 使用总结<br>使用linux工作经常会用到find这个工具去查找文件，找到的文件可以通过awk进行操作，也可以配合grep<br>一起操作，这三个工具一起使用基本上可以高效的完成linux下百分之九十的文本处理工作，今天主要记录一下这三个<br>工具的使用方法。<br>","more":"</p>\n<h2 id=\"find\"><a href=\"#find\" class=\"headerlink\" title=\"find\"></a>find</h2><p>如果我什么参数也不给，find会打印出当前目录下所有的文件和目录名<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">find</div><div class=\"line\">.</div><div class=\"line\">./_posts</div><div class=\"line\">./_posts/hello-world.md</div><div class=\"line\">./_posts/findandawk.md</div><div class=\"line\">./_posts/test.md</div></pre></td></tr></table></figure></p>\n<p>这里有几个默认的参数</p>\n<ol>\n<li>默认查找的目录为当前目录</li>\n<li>默认查找匹配所有记录</li>\n<li>默认打印输出匹配到的记录-print<br>上面的这个find与下面的find输出一致</li>\n</ol>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">find  ./ -print -name <span class=\"string\">\"*\"</span></div><div class=\"line\">./</div><div class=\"line\">./-name</div><div class=\"line\">./_posts</div><div class=\"line\">./_posts/hello-world.md</div><div class=\"line\">./_posts/findandawk.md</div><div class=\"line\">./_posts/test.md</div></pre></td></tr></table></figure>\n<p>-print0可以去掉换行<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">find  ./ -print0 -name <span class=\"string\">\"*\"</span></div><div class=\"line\">././-name./_posts./_posts/hello-world.md./_posts/findandawk.md./_posts/test.md</div></pre></td></tr></table></figure></p>\n<p>-printf 可以格式化输出<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">find . -type f -printf <span class=\"string\">\" %Tc %p\\n\"</span></div><div class=\"line\"> Mon 27 Feb 2017 08:54:40 PM CST ./-name</div><div class=\"line\"> Mon 27 Feb 2017 12:31:29 AM CST ./_posts/hello-world.md</div><div class=\"line\"> Mon 27 Feb 2017 09:01:12 PM CST ./_posts/findandawk.md</div><div class=\"line\"> Sat 25 Feb 2017 01:56:06 AM CST ./_posts/test.md</div></pre></td></tr></table></figure></p>\n<p>格式化输出的参数有很多这里不一一列举具体可以参考手册 man find 看更多printf的相关内容<br>当然输出不是find的主要功能，find 主要还是查找功能。<br>find 可以按照文件名称，文件类型，权限权限，访问时间等等多种维度去查找,但是我们一般使用最多的是按照文件名称来查找的<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">按文件名查找</div><div class=\"line\">find ./ -name <span class=\"string\">\"*.md\"</span></div><div class=\"line\">./_posts/hello-world.md</div><div class=\"line\">./_posts/findandawk.md</div><div class=\"line\">./_posts/test.md</div><div class=\"line\">按文件类型查找</div><div class=\"line\">find ./ -type f -name <span class=\"string\">\"*.md\"</span></div><div class=\"line\">./_posts/hello-world.md</div><div class=\"line\">./_posts/findandawk.md</div><div class=\"line\">./_posts/test.md</div><div class=\"line\"></div><div class=\"line\">一天以前的文件</div><div class=\"line\">find ./ -mtime +1</div></pre></td></tr></table></figure></p>\n<p>find还有一个exec的参数可以对找到的文件执行相应的操作，但是这个有一定的危险如果你执行的动作是<br>删除的话请慎重使用，但是rm在-exec里出现的使用频率往往很高<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">用ls列出找到的文件</div><div class=\"line\">./ -mtime +1 -exec ls <span class=\"_\">-l</span> &#123;&#125; \\;</div><div class=\"line\">-rw-rw-r--. 1 fangqing fangqing 5456 Feb 25 01:56 ./_posts/test.md</div><div class=\"line\">强制删除当前目录下所有日志，这个动作很危险，要注意检查防止误操作</div><div class=\"line\">find ./ -name <span class=\"string\">\"*.log\"</span>  -exec rm <span class=\"_\">-f</span> &#123;&#125; \\;</div></pre></td></tr></table></figure></p>\n<p>find的用法和参数实在是太多，有些高级技巧在需要的时候我就查一下手册，上面find的这些查找参数可以满足日常百分之九十的查找工作。</p>\n<h1 id=\"awk\"><a href=\"#awk\" class=\"headerlink\" title=\"awk\"></a>awk</h1><p>awk是一门脚本语言，awk是三位作者名字的首字母，关于awk起源的内容不写了。<br>awk是linux下文本处理的瑞士军刀，你可以通过awk把字符串按照你想要的格式进行切分、合并、拼接、转换<br>甚至算术运算。<br>首先需要记录一下awk的几个内置变量<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">awk</div><div class=\"line\">    ARGC        参数数量</div><div class=\"line\">    ARGIND      当前处理文件在ARGV中的索引</div><div class=\"line\">    ARGV        命令行参数数组</div><div class=\"line\">    FILENAME    当前处理文件的文件名</div><div class=\"line\">    FS          输入字段的分割符</div><div class=\"line\">    NF          记录字段数量.</div><div class=\"line\">    NR          当前输入的记录数量.</div><div class=\"line\">    OFMT        数字输出格式, 默认为<span class=\"string\">\"%.6g\"</span>.</div><div class=\"line\">    OFS         输出字段分隔符.</div><div class=\"line\">    ORS         输出记录分割符.</div><div class=\"line\">    RS          输入记录分割符号.</div></pre></td></tr></table></figure></p>\n<p>这几个变量对awk处理文本有很大影响基本常用就是这几个变量下面记录几个案例<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">./ -print -name <span class=\"string\">\"*\"</span> -exec ls <span class=\"_\">-l</span> &#123;&#125; \\; |awk <span class=\"string\">'&#123;print $NF&#125;'</span></div><div class=\"line\">./</div><div class=\"line\">-name</div><div class=\"line\">_posts</div><div class=\"line\">./-name</div><div class=\"line\">./-name</div><div class=\"line\">./_posts</div><div class=\"line\">findandawk.md</div><div class=\"line\">hello-world.md</div><div class=\"line\">test.md</div><div class=\"line\">./_posts/hello-world.md</div><div class=\"line\">./_posts/hello-world.md</div><div class=\"line\">./_posts/findandawk.md</div><div class=\"line\">./_posts/findandawk.md</div><div class=\"line\">./_posts/test.md</div><div class=\"line\">./_posts/test.md</div><div class=\"line\">`</div></pre></td></tr></table></figure></p>\n<p>-F 可以指定字段分隔符，不指定时默认为空白字符，$NF输出最后一个字段.</p>\n<h2 id=\"grep\"><a href=\"#grep\" class=\"headerlink\" title=\"grep\"></a>grep</h2><p>Linux系统中grep命令是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹 配的行打印出来。grep全称是Global Regular Expression Print，表示全局正则表达式版本，它的使用权限是所有用户。</p>\n<ol>\n<li><p>格式<br>grep [options]</p>\n</li>\n<li><p>主要参数</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\">[options]主要参数：</div><div class=\"line\">－c：只输出匹配行的计数。</div><div class=\"line\">－I：不区分大小写(只适用于单字符)。</div><div class=\"line\">－h：查询多文件时不显示文件名。</div><div class=\"line\">－l：查询多文件时只输出包含匹配字符的文件名。</div><div class=\"line\">－n：显示匹配行及行号。</div><div class=\"line\">－s：不显示不存在或无匹配文本的错误信息。</div><div class=\"line\">－v：显示不包含匹配文本的所有行。</div><div class=\"line\">pattern正则表达式主要参数：</div><div class=\"line\">\\： 忽略正则表达式中特殊字符的原有含义。</div><div class=\"line\">^：匹配正则表达式的开始行。</div><div class=\"line\">$: 匹配正则表达式的结束行。</div><div class=\"line\">\\&lt;：从匹配正则表达式单词边界。</div><div class=\"line\">\\&gt;：到匹配正则表达式单词边界。</div><div class=\"line\">[ ]：单个字符，如[A]即A符合要求 。</div><div class=\"line\">[ - ]：范围，如[A-Z]，即A、B、C一直到Z都符合要求 。</div><div class=\"line\">.：所有的单个字符。</div><div class=\"line\">* ：有字符，长度可以为0。</div></pre></td></tr></table></figure>\n</li>\n<li><p>grep命令使用简单实例<br>$ grep ‘ERROR’ d<em><br>匹配当前目录所有以d开头的文件中包含ERROR的行。<br>$ grep ‘ERROR’ a.log b.log c.log<br>显示在a.log b.log c.log 文件中匹配ERROR的行。<br>$ grep ‘[a-z]{5}’ a.log<br>显示所有包含每个字符串至少有5个连续小写字符的字符串的行。<br>$ grep ‘w(es)t.</em>\\1′ a.log<br>如果west被匹配，则es就被存储到内存中，并标记为1，然后搜索任意个字符(.<em>)，这些字符后面紧跟着 另外一个es(\\1)，找到就显示该行。如果用egrep或grep -E，就不用”\\”号进行转义，直接写成’w(es)t.</em>\\1′就可以了。</p>\n</li>\n<li><p>grep高级使用技巧<br>假设您正在’/usr/local/spark/’目录下搜索带字符 串’spark’的文件：<br>$ grep “spark” /usr/local/spark/*<br>默认情况下，’grep’只搜索当前目录。遇到子目录，’grep’报出：<br>grep: spark: Is a directory<br>-r选项可以第归搜索：grep -r<br>也可以忽略子目录：grep -d skip</p>\n</li>\n<li><p>grep其他参数：<br>grep -i pattern files ：不区分大小写地搜索。默认情况区分大小写，<br>grep -l pattern files ：只列出匹配的文件名，<br>grep -L pattern files ：列出不匹配的文件名，<br>grep -w pattern files ：只匹配整个单词，而不是字符串的一部分(如匹配’food’，而不是’foo’)，<br>grep -C number pattern files ：匹配的上下文分别显示[number]行，<br>如 grep -C 3 “hexo” source/_posts/* 查找hexo输出hexo前后各三行<br>grep pattern1 | pattern2 files ：显示匹配 pattern1 或 pattern2 的行，<br>grep pattern1 files | grep pattern2 ：显示既匹配 pattern1 又匹配 pattern2 的行。<br>grep -n pattern files  即可显示行号信息<br>grep -c pattern files  即可查找总行数</p>\n</li>\n<li><p>grep 正则表达式：<br>\\&lt; 和 > 分别标注单词的开始与结尾。<br>例如：<br>grep good <em> 会匹配 ‘goodddd’、’ggood’、’good’等，<br>grep ‘\\&lt;good’ </em> 匹配’goodddd’和’good’，但不是’ggood’，<br>grep ‘\\<good\\>’ 只匹配’good’。<br>‘^’：指匹配的字符串在行首，<br>‘$’：指匹配的字符串在行 尾，</p>\n</li>\n<li>grep正则表达式可以使用类名<br>可以使用国际模式匹配的类名：<br>[[:upper:]]   [A-Z]<br>[[:lower:]]   [a-z]<br>[[:digit:]]   [0-9]<br>[[:alnum:]]   [0-9a-zA-Z]<br>[[:space:]]   空格或tab<br>[[:alpha:]]   [a-zA-Z]<br>grep ‘#[[:upper:]][[:upper:]]’ data.doc     #查询以#开头紧跟着两个大写字<br>grep 在linux下使用很多，功能很全面，尤其是正则表达式使用得当可以很好的提升工作效率。</li>\n</ol>"},{"title":"group by rollup 和 cube的原理分析","date":"2017-03-12T15:53:14.000Z","_content":"\n\n在数据分析中经常会用到分组统计，sparksql的dataframe也支持分组统计，这里记录一下所谓分组统计是什么怎么用,假设有如下的一组临时数据tmp\n```\ncolume1　colume2　　value\nA 　　   X            2\nA 　　   X            1\nA 　 　　Y            2\nA 　 　　Y            1\nB 　 　　X            3\nB 　 　　Y            2\nB 　 　　Y            2\n```\n<!-- more -->\n## group by\n现在通过group by 使用SUM()函数对第三个列值总计：\n```\nSELECT colume1,colume2,SUM(value) \n  FROM tmp\n  GROUP BY colume1,colume2\n```\n分别按照colume1和colume2 进行分组返回结果如下：\n```\ncolume1　colume2　　sum(value)\n\n  A 　　　X 　　　　3\n  B 　　　X 　　　　3\n  A 　　　Y 　　　　3\n  B 　　　Y 　　　　4\n```\nCube和Rollup从分组的查询中取得结果，对第一列的值或者每个出现在Group By列列表中的所有列值的组合应用相同的聚合函数。\n\n##  Rollup\n rollup 是对Group By列列表的第一列进行小计和总计计算的最简单的方法。在我们的例子中，除计算每个唯一的列值的总和以外，还需计算colume1列中A和B行的总和。\n```\n  SELECT colume1,colume2,SUM(value)\n  FROM tmp\n  GROUP BY colume1,colume2\n      WITH ROLLUP\n```\n我们得到的结果是colume1，和colme2的组合统计结果,但是这个组合是以colume1为主体进行的，这并不是一个完全组合，具体结果如下：\n```\ncolume1　colume2　　sum(value)\n\n  A 　　　X 　　　　3\n  A 　　　Y 　　　　3\n  A 　　NULL 　　　6\n  B 　　　X 　　　　3\n  B 　　　Y 　　　　4\n  B 　　NULL 　　　7\n  NULL NULL 　　　13\n```\n\n上述结果中的空值表示在计算聚合值时忽略相关的列。\n\n## Cube\n\n而Cube运算符是对Rollup运算符的扩展，我们称之为数据钻取。Cube同样会对colume1和colume2进行分组统计，但这时候的分组是完全组合，所有可能的组合都会出现在结果集中。\n```\n  SELECT colume1,colume2,SUM(value)\n  FROM tmp\n  GROUP BY colume1,colume2\n      WITH CUBE\n```\n得到的具体结果如下：\n\n```\ncolume1　　colume2　　sum(value)\n  A 　　　X 　　　　3\n  B 　　　X 　　　　3\n  NULL 　X 　　　　6\n  A 　　　Y 　　　　3\n  B 　　　Y 　　　　4\n  NULL 　Y 　　　　7\n  NULL NULL 　　　13\n  A 　　NULL 　　　6\n  B 　　NULL 　　　7\n```\n第1列中的空值表示该列值是第2列的值的积累。这些行包含colume2等于X或者Y的行的小计。其中两个分组列的值都为空，表明这两个列是一个总计，即所有行的和。\n","source":"_posts/groupbyandrollup.md","raw":"---\ntitle: group by rollup 和 cube的原理分析\ndate: 2017-03-12 23:53:14\ntags: \n  - spark\n  - sql\ncategory: spark\n---\n\n\n在数据分析中经常会用到分组统计，sparksql的dataframe也支持分组统计，这里记录一下所谓分组统计是什么怎么用,假设有如下的一组临时数据tmp\n```\ncolume1　colume2　　value\nA 　　   X            2\nA 　　   X            1\nA 　 　　Y            2\nA 　 　　Y            1\nB 　 　　X            3\nB 　 　　Y            2\nB 　 　　Y            2\n```\n<!-- more -->\n## group by\n现在通过group by 使用SUM()函数对第三个列值总计：\n```\nSELECT colume1,colume2,SUM(value) \n  FROM tmp\n  GROUP BY colume1,colume2\n```\n分别按照colume1和colume2 进行分组返回结果如下：\n```\ncolume1　colume2　　sum(value)\n\n  A 　　　X 　　　　3\n  B 　　　X 　　　　3\n  A 　　　Y 　　　　3\n  B 　　　Y 　　　　4\n```\nCube和Rollup从分组的查询中取得结果，对第一列的值或者每个出现在Group By列列表中的所有列值的组合应用相同的聚合函数。\n\n##  Rollup\n rollup 是对Group By列列表的第一列进行小计和总计计算的最简单的方法。在我们的例子中，除计算每个唯一的列值的总和以外，还需计算colume1列中A和B行的总和。\n```\n  SELECT colume1,colume2,SUM(value)\n  FROM tmp\n  GROUP BY colume1,colume2\n      WITH ROLLUP\n```\n我们得到的结果是colume1，和colme2的组合统计结果,但是这个组合是以colume1为主体进行的，这并不是一个完全组合，具体结果如下：\n```\ncolume1　colume2　　sum(value)\n\n  A 　　　X 　　　　3\n  A 　　　Y 　　　　3\n  A 　　NULL 　　　6\n  B 　　　X 　　　　3\n  B 　　　Y 　　　　4\n  B 　　NULL 　　　7\n  NULL NULL 　　　13\n```\n\n上述结果中的空值表示在计算聚合值时忽略相关的列。\n\n## Cube\n\n而Cube运算符是对Rollup运算符的扩展，我们称之为数据钻取。Cube同样会对colume1和colume2进行分组统计，但这时候的分组是完全组合，所有可能的组合都会出现在结果集中。\n```\n  SELECT colume1,colume2,SUM(value)\n  FROM tmp\n  GROUP BY colume1,colume2\n      WITH CUBE\n```\n得到的具体结果如下：\n\n```\ncolume1　　colume2　　sum(value)\n  A 　　　X 　　　　3\n  B 　　　X 　　　　3\n  NULL 　X 　　　　6\n  A 　　　Y 　　　　3\n  B 　　　Y 　　　　4\n  NULL 　Y 　　　　7\n  NULL NULL 　　　13\n  A 　　NULL 　　　6\n  B 　　NULL 　　　7\n```\n第1列中的空值表示该列值是第2列的值的积累。这些行包含colume2等于X或者Y的行的小计。其中两个分组列的值都为空，表明这两个列是一个总计，即所有行的和。\n","slug":"groupbyandrollup","published":1,"updated":"2017-03-13T14:06:58.237Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj70mt0pn000e60tu14g5drzk","content":"<p>在数据分析中经常会用到分组统计，sparksql的dataframe也支持分组统计，这里记录一下所谓分组统计是什么怎么用,假设有如下的一组临时数据tmp<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">colume1　colume2　　value</div><div class=\"line\">A 　　   X            2</div><div class=\"line\">A 　　   X            1</div><div class=\"line\">A 　 　　Y            2</div><div class=\"line\">A 　 　　Y            1</div><div class=\"line\">B 　 　　X            3</div><div class=\"line\">B 　 　　Y            2</div><div class=\"line\">B 　 　　Y            2</div></pre></td></tr></table></figure></p>\n<a id=\"more\"></a>\n<h2 id=\"group-by\"><a href=\"#group-by\" class=\"headerlink\" title=\"group by\"></a>group by</h2><p>现在通过group by 使用SUM()函数对第三个列值总计：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">SELECT colume1,colume2,SUM(value) </div><div class=\"line\">  FROM tmp</div><div class=\"line\">  GROUP BY colume1,colume2</div></pre></td></tr></table></figure></p>\n<p>分别按照colume1和colume2 进行分组返回结果如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">colume1　colume2　　sum(value)</div><div class=\"line\"></div><div class=\"line\">  A 　　　X 　　　　3</div><div class=\"line\">  B 　　　X 　　　　3</div><div class=\"line\">  A 　　　Y 　　　　3</div><div class=\"line\">  B 　　　Y 　　　　4</div></pre></td></tr></table></figure></p>\n<p>Cube和Rollup从分组的查询中取得结果，对第一列的值或者每个出现在Group By列列表中的所有列值的组合应用相同的聚合函数。</p>\n<h2 id=\"Rollup\"><a href=\"#Rollup\" class=\"headerlink\" title=\"Rollup\"></a>Rollup</h2><p> rollup 是对Group By列列表的第一列进行小计和总计计算的最简单的方法。在我们的例子中，除计算每个唯一的列值的总和以外，还需计算colume1列中A和B行的总和。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">SELECT colume1,colume2,SUM(value)</div><div class=\"line\">FROM tmp</div><div class=\"line\">GROUP BY colume1,colume2</div><div class=\"line\">    WITH ROLLUP</div></pre></td></tr></table></figure></p>\n<p>我们得到的结果是colume1，和colme2的组合统计结果,但是这个组合是以colume1为主体进行的，这并不是一个完全组合，具体结果如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">colume1　colume2　　sum(value)</div><div class=\"line\"></div><div class=\"line\">  A 　　　X 　　　　3</div><div class=\"line\">  A 　　　Y 　　　　3</div><div class=\"line\">  A 　　NULL 　　　6</div><div class=\"line\">  B 　　　X 　　　　3</div><div class=\"line\">  B 　　　Y 　　　　4</div><div class=\"line\">  B 　　NULL 　　　7</div><div class=\"line\">  NULL NULL 　　　13</div></pre></td></tr></table></figure></p>\n<p>上述结果中的空值表示在计算聚合值时忽略相关的列。</p>\n<h2 id=\"Cube\"><a href=\"#Cube\" class=\"headerlink\" title=\"Cube\"></a>Cube</h2><p>而Cube运算符是对Rollup运算符的扩展，我们称之为数据钻取。Cube同样会对colume1和colume2进行分组统计，但这时候的分组是完全组合，所有可能的组合都会出现在结果集中。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">SELECT colume1,colume2,SUM(value)</div><div class=\"line\">FROM tmp</div><div class=\"line\">GROUP BY colume1,colume2</div><div class=\"line\">    WITH CUBE</div></pre></td></tr></table></figure></p>\n<p>得到的具体结果如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">colume1　　colume2　　sum(value)</div><div class=\"line\">  A 　　　X 　　　　3</div><div class=\"line\">  B 　　　X 　　　　3</div><div class=\"line\">  NULL 　X 　　　　6</div><div class=\"line\">  A 　　　Y 　　　　3</div><div class=\"line\">  B 　　　Y 　　　　4</div><div class=\"line\">  NULL 　Y 　　　　7</div><div class=\"line\">  NULL NULL 　　　13</div><div class=\"line\">  A 　　NULL 　　　6</div><div class=\"line\">  B 　　NULL 　　　7</div></pre></td></tr></table></figure>\n<p>第1列中的空值表示该列值是第2列的值的积累。这些行包含colume2等于X或者Y的行的小计。其中两个分组列的值都为空，表明这两个列是一个总计，即所有行的和。</p>\n","excerpt":"<p>在数据分析中经常会用到分组统计，sparksql的dataframe也支持分组统计，这里记录一下所谓分组统计是什么怎么用,假设有如下的一组临时数据tmp<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">colume1　colume2　　value</div><div class=\"line\">A 　　   X            2</div><div class=\"line\">A 　　   X            1</div><div class=\"line\">A 　 　　Y            2</div><div class=\"line\">A 　 　　Y            1</div><div class=\"line\">B 　 　　X            3</div><div class=\"line\">B 　 　　Y            2</div><div class=\"line\">B 　 　　Y            2</div></pre></td></tr></table></figure></p>","more":"<h2 id=\"group-by\"><a href=\"#group-by\" class=\"headerlink\" title=\"group by\"></a>group by</h2><p>现在通过group by 使用SUM()函数对第三个列值总计：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">SELECT colume1,colume2,SUM(value) </div><div class=\"line\">  FROM tmp</div><div class=\"line\">  GROUP BY colume1,colume2</div></pre></td></tr></table></figure></p>\n<p>分别按照colume1和colume2 进行分组返回结果如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">colume1　colume2　　sum(value)</div><div class=\"line\"></div><div class=\"line\">  A 　　　X 　　　　3</div><div class=\"line\">  B 　　　X 　　　　3</div><div class=\"line\">  A 　　　Y 　　　　3</div><div class=\"line\">  B 　　　Y 　　　　4</div></pre></td></tr></table></figure></p>\n<p>Cube和Rollup从分组的查询中取得结果，对第一列的值或者每个出现在Group By列列表中的所有列值的组合应用相同的聚合函数。</p>\n<h2 id=\"Rollup\"><a href=\"#Rollup\" class=\"headerlink\" title=\"Rollup\"></a>Rollup</h2><p> rollup 是对Group By列列表的第一列进行小计和总计计算的最简单的方法。在我们的例子中，除计算每个唯一的列值的总和以外，还需计算colume1列中A和B行的总和。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">SELECT colume1,colume2,SUM(value)</div><div class=\"line\">FROM tmp</div><div class=\"line\">GROUP BY colume1,colume2</div><div class=\"line\">    WITH ROLLUP</div></pre></td></tr></table></figure></p>\n<p>我们得到的结果是colume1，和colme2的组合统计结果,但是这个组合是以colume1为主体进行的，这并不是一个完全组合，具体结果如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">colume1　colume2　　sum(value)</div><div class=\"line\"></div><div class=\"line\">  A 　　　X 　　　　3</div><div class=\"line\">  A 　　　Y 　　　　3</div><div class=\"line\">  A 　　NULL 　　　6</div><div class=\"line\">  B 　　　X 　　　　3</div><div class=\"line\">  B 　　　Y 　　　　4</div><div class=\"line\">  B 　　NULL 　　　7</div><div class=\"line\">  NULL NULL 　　　13</div></pre></td></tr></table></figure></p>\n<p>上述结果中的空值表示在计算聚合值时忽略相关的列。</p>\n<h2 id=\"Cube\"><a href=\"#Cube\" class=\"headerlink\" title=\"Cube\"></a>Cube</h2><p>而Cube运算符是对Rollup运算符的扩展，我们称之为数据钻取。Cube同样会对colume1和colume2进行分组统计，但这时候的分组是完全组合，所有可能的组合都会出现在结果集中。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">SELECT colume1,colume2,SUM(value)</div><div class=\"line\">FROM tmp</div><div class=\"line\">GROUP BY colume1,colume2</div><div class=\"line\">    WITH CUBE</div></pre></td></tr></table></figure></p>\n<p>得到的具体结果如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">colume1　　colume2　　sum(value)</div><div class=\"line\">  A 　　　X 　　　　3</div><div class=\"line\">  B 　　　X 　　　　3</div><div class=\"line\">  NULL 　X 　　　　6</div><div class=\"line\">  A 　　　Y 　　　　3</div><div class=\"line\">  B 　　　Y 　　　　4</div><div class=\"line\">  NULL 　Y 　　　　7</div><div class=\"line\">  NULL NULL 　　　13</div><div class=\"line\">  A 　　NULL 　　　6</div><div class=\"line\">  B 　　NULL 　　　7</div></pre></td></tr></table></figure>\n<p>第1列中的空值表示该列值是第2列的值的积累。这些行包含colume2等于X或者Y的行的小计。其中两个分组列的值都为空，表明这两个列是一个总计，即所有行的和。</p>"},{"title":"hexo小试牛刀","date":"2017-02-21T15:57:54.000Z","_content":"欢迎大家关注我的空间，这是我用hexo写的第一篇博客，主要介绍下hexo的基本用法,后续我将在这里记录我学习和生活中值得记录下来的一些东西，仅仅给自己一些记录，当然如果有人看到了觉得有用那也挺好。\n## 入门\n<!--more-->\n###  创建一篇文章\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\n详情请查阅: [Writing](https://hexo.io/docs/writing.html)\n\n### 创建完成就可以通过server来查看结果\n\n``` bash\n$ hexo server\n```\n\n详情请查阅: [Server](https://hexo.io/docs/server.html)\n\n### 生成静态文件\n\n``` bash\n$ hexo generate\n```\n\n详情请查阅: [Generating](https://hexo.io/docs/generating.html)\n\n### 发布到远程仓库\n\n``` bash\n$ hexo deploy\n```\n\n想请参考: [Deployment](https://hexo.io/docs/deployment.html)\n关于hexo的更多信息请登陆: [hexo](https://hexo.io/docs)\n\n## 使用next\n```\n$ cd your-hexo-site\n$ git clone https://github.com/iissnan/hexo-theme-next themes/next\n```\n从Next的Gihub仓库中获取最新版本。\n启用\n需要修改/root/_config.yml配置项theme：\n```\n# Extensions\n## Plugins: http://hexo.io/plugins/\n## Themes: http://hexo.io/themes/\ntheme: next\n```\n验证是否启用\n```\n$ hexo s --debug\n```\nENOSPC Error (Linux)\n```\n  Sometimes when running the command $ hexo server it returns an error:\n\n  Error: watch ENOSPC ...\n\n  It can be fixed by running $ npm dedupe or, if that doesn’t help, try the following in the Linux console:\n\n  $ echo fs.inotify.max_user_watches=524288 | sudo tee -a /etc/sysctl.conf && sudo sysctl -p\n```\n  This will increase the limit for the number of files you can watch.\n","source":"_posts/hello-world.md","raw":"---\ntitle: hexo小试牛刀\ndate: 2017-02-21 23:57:54\ntag: hexo\n---\n欢迎大家关注我的空间，这是我用hexo写的第一篇博客，主要介绍下hexo的基本用法,后续我将在这里记录我学习和生活中值得记录下来的一些东西，仅仅给自己一些记录，当然如果有人看到了觉得有用那也挺好。\n## 入门\n<!--more-->\n###  创建一篇文章\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\n详情请查阅: [Writing](https://hexo.io/docs/writing.html)\n\n### 创建完成就可以通过server来查看结果\n\n``` bash\n$ hexo server\n```\n\n详情请查阅: [Server](https://hexo.io/docs/server.html)\n\n### 生成静态文件\n\n``` bash\n$ hexo generate\n```\n\n详情请查阅: [Generating](https://hexo.io/docs/generating.html)\n\n### 发布到远程仓库\n\n``` bash\n$ hexo deploy\n```\n\n想请参考: [Deployment](https://hexo.io/docs/deployment.html)\n关于hexo的更多信息请登陆: [hexo](https://hexo.io/docs)\n\n## 使用next\n```\n$ cd your-hexo-site\n$ git clone https://github.com/iissnan/hexo-theme-next themes/next\n```\n从Next的Gihub仓库中获取最新版本。\n启用\n需要修改/root/_config.yml配置项theme：\n```\n# Extensions\n## Plugins: http://hexo.io/plugins/\n## Themes: http://hexo.io/themes/\ntheme: next\n```\n验证是否启用\n```\n$ hexo s --debug\n```\nENOSPC Error (Linux)\n```\n  Sometimes when running the command $ hexo server it returns an error:\n\n  Error: watch ENOSPC ...\n\n  It can be fixed by running $ npm dedupe or, if that doesn’t help, try the following in the Linux console:\n\n  $ echo fs.inotify.max_user_watches=524288 | sudo tee -a /etc/sysctl.conf && sudo sysctl -p\n```\n  This will increase the limit for the number of files you can watch.\n","slug":"hello-world","published":1,"updated":"2017-03-08T23:50:35.715Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj70mt0ps000g60tupfaojebl","content":"<p>欢迎大家关注我的空间，这是我用hexo写的第一篇博客，主要介绍下hexo的基本用法,后续我将在这里记录我学习和生活中值得记录下来的一些东西，仅仅给自己一些记录，当然如果有人看到了觉得有用那也挺好。</p>\n<h2 id=\"入门\"><a href=\"#入门\" class=\"headerlink\" title=\"入门\"></a>入门</h2><a id=\"more\"></a>\n<h3 id=\"创建一篇文章\"><a href=\"#创建一篇文章\" class=\"headerlink\" title=\"创建一篇文章\"></a>创建一篇文章</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></div></pre></td></tr></table></figure>\n<p>详情请查阅: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"external\">Writing</a></p>\n<h3 id=\"创建完成就可以通过server来查看结果\"><a href=\"#创建完成就可以通过server来查看结果\" class=\"headerlink\" title=\"创建完成就可以通过server来查看结果\"></a>创建完成就可以通过server来查看结果</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo server</div></pre></td></tr></table></figure>\n<p>详情请查阅: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"external\">Server</a></p>\n<h3 id=\"生成静态文件\"><a href=\"#生成静态文件\" class=\"headerlink\" title=\"生成静态文件\"></a>生成静态文件</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo generate</div></pre></td></tr></table></figure>\n<p>详情请查阅: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"external\">Generating</a></p>\n<h3 id=\"发布到远程仓库\"><a href=\"#发布到远程仓库\" class=\"headerlink\" title=\"发布到远程仓库\"></a>发布到远程仓库</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo deploy</div></pre></td></tr></table></figure>\n<p>想请参考: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"external\">Deployment</a><br>关于hexo的更多信息请登陆: <a href=\"https://hexo.io/docs\" target=\"_blank\" rel=\"external\">hexo</a></p>\n<h2 id=\"使用next\"><a href=\"#使用next\" class=\"headerlink\" title=\"使用next\"></a>使用next</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ cd your-hexo-site</div><div class=\"line\">$ git clone https://github.com/iissnan/hexo-theme-next themes/next</div></pre></td></tr></table></figure>\n<p>从Next的Gihub仓库中获取最新版本。<br>启用<br>需要修改/root/_config.yml配置项theme：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"># Extensions</div><div class=\"line\">## Plugins: http://hexo.io/plugins/</div><div class=\"line\">## Themes: http://hexo.io/themes/</div><div class=\"line\">theme: next</div></pre></td></tr></table></figure></p>\n<p>验证是否启用<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo s --debug</div></pre></td></tr></table></figure></p>\n<p>ENOSPC Error (Linux)<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">Sometimes when running the command $ hexo server it returns an error:</div><div class=\"line\"></div><div class=\"line\">Error: watch ENOSPC ...</div><div class=\"line\"></div><div class=\"line\">It can be fixed by running $ npm dedupe or, if that doesn’t help, try the following in the Linux console:</div><div class=\"line\"></div><div class=\"line\">$ echo fs.inotify.max_user_watches=524288 | sudo tee -a /etc/sysctl.conf &amp;&amp; sudo sysctl -p</div></pre></td></tr></table></figure></p>\n<p>  This will increase the limit for the number of files you can watch.</p>\n","excerpt":"<p>欢迎大家关注我的空间，这是我用hexo写的第一篇博客，主要介绍下hexo的基本用法,后续我将在这里记录我学习和生活中值得记录下来的一些东西，仅仅给自己一些记录，当然如果有人看到了觉得有用那也挺好。</p>\n<h2 id=\"入门\"><a href=\"#入门\" class=\"headerlink\" title=\"入门\"></a>入门</h2>","more":"<h3 id=\"创建一篇文章\"><a href=\"#创建一篇文章\" class=\"headerlink\" title=\"创建一篇文章\"></a>创建一篇文章</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></div></pre></td></tr></table></figure>\n<p>详情请查阅: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"创建完成就可以通过server来查看结果\"><a href=\"#创建完成就可以通过server来查看结果\" class=\"headerlink\" title=\"创建完成就可以通过server来查看结果\"></a>创建完成就可以通过server来查看结果</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo server</div></pre></td></tr></table></figure>\n<p>详情请查阅: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"生成静态文件\"><a href=\"#生成静态文件\" class=\"headerlink\" title=\"生成静态文件\"></a>生成静态文件</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo generate</div></pre></td></tr></table></figure>\n<p>详情请查阅: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"发布到远程仓库\"><a href=\"#发布到远程仓库\" class=\"headerlink\" title=\"发布到远程仓库\"></a>发布到远程仓库</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo deploy</div></pre></td></tr></table></figure>\n<p>想请参考: <a href=\"https://hexo.io/docs/deployment.html\">Deployment</a><br>关于hexo的更多信息请登陆: <a href=\"https://hexo.io/docs\">hexo</a></p>\n<h2 id=\"使用next\"><a href=\"#使用next\" class=\"headerlink\" title=\"使用next\"></a>使用next</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ cd your-hexo-site</div><div class=\"line\">$ git clone https://github.com/iissnan/hexo-theme-next themes/next</div></pre></td></tr></table></figure>\n<p>从Next的Gihub仓库中获取最新版本。<br>启用<br>需要修改/root/_config.yml配置项theme：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"># Extensions</div><div class=\"line\">## Plugins: http://hexo.io/plugins/</div><div class=\"line\">## Themes: http://hexo.io/themes/</div><div class=\"line\">theme: next</div></pre></td></tr></table></figure></p>\n<p>验证是否启用<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo s --debug</div></pre></td></tr></table></figure></p>\n<p>ENOSPC Error (Linux)<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">Sometimes when running the command $ hexo server it returns an error:</div><div class=\"line\"></div><div class=\"line\">Error: watch ENOSPC ...</div><div class=\"line\"></div><div class=\"line\">It can be fixed by running $ npm dedupe or, if that doesn’t help, try the following in the Linux console:</div><div class=\"line\"></div><div class=\"line\">$ echo fs.inotify.max_user_watches=524288 | sudo tee -a /etc/sysctl.conf &amp;&amp; sudo sysctl -p</div></pre></td></tr></table></figure></p>\n<p>  This will increase the limit for the number of files you can watch.</p>"},{"title":"spark设置shufferpattition数量","date":"2017-02-28T15:57:54.000Z","_content":"1. spark 2.0.1 加载csv文件\n```\nval option=Map(\"header\"->\"true\",\"seq\"->\":\")\nval tmpdf=spark.sqlContext.read.option(op).format(csv).load(\"/test.csv\");\ntmpdf.OrderBy(\"age\").write.csv(\"/testrs.csv\");\n```\n以上代码在spark-shell中执行默认有spark对象\n输出后在/testrs.csv目录下发现有200个小文件，非常小每个文件大约只有一辆行记录，这是因为sparksql默认的spark.sql.shuffle.partitions值为200，将这个参数在spark的配置文件spark-default中修改成我们想要的就可以了，也可以在代码中动态设置这个值，我自己设置为20。\n","source":"_posts/hello.md","raw":"---\ntitle: spark设置shufferpattition数量\ndate: 2017-02-28 23:57:54\ntags: spark\n---\n1. spark 2.0.1 加载csv文件\n```\nval option=Map(\"header\"->\"true\",\"seq\"->\":\")\nval tmpdf=spark.sqlContext.read.option(op).format(csv).load(\"/test.csv\");\ntmpdf.OrderBy(\"age\").write.csv(\"/testrs.csv\");\n```\n以上代码在spark-shell中执行默认有spark对象\n输出后在/testrs.csv目录下发现有200个小文件，非常小每个文件大约只有一辆行记录，这是因为sparksql默认的spark.sql.shuffle.partitions值为200，将这个参数在spark的配置文件spark-default中修改成我们想要的就可以了，也可以在代码中动态设置这个值，我自己设置为20。\n","slug":"hello","published":1,"updated":"2017-02-28T15:00:32.231Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj70mt0py000j60tupzkv0omm","content":"<ol>\n<li>spark 2.0.1 加载csv文件<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">val option=Map(&quot;header&quot;-&gt;&quot;true&quot;,&quot;seq&quot;-&gt;&quot;:&quot;)</div><div class=\"line\">val tmpdf=spark.sqlContext.read.option(op).format(csv).load(&quot;/test.csv&quot;);</div><div class=\"line\">tmpdf.OrderBy(&quot;age&quot;).write.csv(&quot;/testrs.csv&quot;);</div></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>以上代码在spark-shell中执行默认有spark对象<br>输出后在/testrs.csv目录下发现有200个小文件，非常小每个文件大约只有一辆行记录，这是因为sparksql默认的spark.sql.shuffle.partitions值为200，将这个参数在spark的配置文件spark-default中修改成我们想要的就可以了，也可以在代码中动态设置这个值，我自己设置为20。</p>\n","excerpt":"","more":"<ol>\n<li>spark 2.0.1 加载csv文件<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">val option=Map(&quot;header&quot;-&gt;&quot;true&quot;,&quot;seq&quot;-&gt;&quot;:&quot;)</div><div class=\"line\">val tmpdf=spark.sqlContext.read.option(op).format(csv).load(&quot;/test.csv&quot;);</div><div class=\"line\">tmpdf.OrderBy(&quot;age&quot;).write.csv(&quot;/testrs.csv&quot;);</div></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>以上代码在spark-shell中执行默认有spark对象<br>输出后在/testrs.csv目录下发现有200个小文件，非常小每个文件大约只有一辆行记录，这是因为sparksql默认的spark.sql.shuffle.partitions值为200，将这个参数在spark的配置文件spark-default中修改成我们想要的就可以了，也可以在代码中动态设置这个值，我自己设置为20。</p>\n"},{"title":"java 迭代器","date":"2017-03-08T12:57:54.000Z","_content":"\n\n\n这是一道java基础题目，要求输出一个数组\n```bash\n1 3 6 10 15 21\n2 5 9 14 20\n4 8 13 19\n7 12 18\n11 17\n16\n```\n<!-- more -->\n基本上就是数组的操作，代码如下：\n```java\nint xx[][] = new int[6][6];\nint num = 1;\n\n     for (int i = 0; i <= 5; i++) {\n\n         for (int j = 0; j <= 5; j++) {\n\n             xx[i][j] = -1;\n\n         }\n\n     }\n     for (int i = 0; i <= 5; i++) {\n         int m = i;\n         for (int j = 0; j <= i; j++) {\n             xx[j][m--] = num;\n             num++;\n         }\n         System.out.println();\n     }\n     for (int i = 0; i <= 5; i++) {\n         for (int j = 0; j <= 5; j++) {\n             if (xx[i][j] != -1)\n                 System.out.print(xx[j][i] + \" \");\n         }\n         System.out.println();\n     }\n```\n由这道题目引申出了数组的操作，上述题目我们自己定义了xx数组，自己定义的数组增、删、边界都要自己维护，但是一般项目开发中我们 都会选择array数组，array数组本身是一个动态数组，数组中定义了元素的增删操作也可以通过迭代器来判断边界。\n具体举例如下：\n\n```java\npublic static void main(String[] args) {\n       ArrayList<Integer> arrlist = new ArrayList<Integer>(5);//ArrayList 类型参数不能为基本类型\n       for (int i = 0; i < 5; i++) {\n           arrlist.add(i);\n       }\n       arrlist.add(0);\n       Iterator<Integer> it;\n       arrprint(arrlist.iterator());\n       for (it = arrlist.iterator(); it.hasNext(); ) {\n           if (it.next() == 0) {\n               it.remove();//如果是arrlist删除的话就会报错\n//                arrlist.remove((Integer)0); //Exception in thread \"main\" java.util.ConcurrentModificationException\n           }\n       }\n       System.out.println();\n       arrprint(arrlist.iterator());\n   }\n\n   private static void arrprint(Iterator<Integer> it) {\n       for (; it.hasNext(); ) {\n           System.out.print(it.next() + \" \");\n       }\n   }\n```\n结果如下\n\n```bash\n0 1 2 3 4 0\n1 2 3 4\n```\n上面报错的原因是当我们找到要删除的元素时直接去arraylist里面删除了元素没有告诉 迭代器，导致迭代器失效了。而直接使用迭代器删除就不会有这个问题，迭代器维护了数组的 下表索引。 在Java中，有很多的数据容器上面是以arrarylist为例，对于这些容器的操作有很多的共性。Java采用了迭代器来为各种容器提供了公共的操作接口。这样使得对容器的遍历操作与其具体的底层实现相隔离，达到解耦的效果，同时也更安全更便捷。\n迭代器是一种设计模式，它是一个对象，它可以遍历并选择序列中的对象，而开发人员不需要了解该序列的底层结构。迭代器通常被称为“轻量级”对象，因为创建它的代价小。\n\n1. Java中的Iterator有以下几个特点：\n11. 使用方法iterator()要求容器返回一个Iterator。第一次调用Iterator的next()方法时，它返回序列的第一个元素。注意：iterator()方法是java.lang.Iterable接口,被Collection继承。\n11. 使用next()获得序列中的下一个元素。\n11. 使用hasNext()检查序列中是否还有元素。\n11. 使用remove()将迭代器新返回的元素删除。\n","source":"_posts/iterator.md","raw":"---\ntitle: java 迭代器\ndate: 2017-03-08 20:57:54\ntags: java\n---\n\n\n\n这是一道java基础题目，要求输出一个数组\n```bash\n1 3 6 10 15 21\n2 5 9 14 20\n4 8 13 19\n7 12 18\n11 17\n16\n```\n<!-- more -->\n基本上就是数组的操作，代码如下：\n```java\nint xx[][] = new int[6][6];\nint num = 1;\n\n     for (int i = 0; i <= 5; i++) {\n\n         for (int j = 0; j <= 5; j++) {\n\n             xx[i][j] = -1;\n\n         }\n\n     }\n     for (int i = 0; i <= 5; i++) {\n         int m = i;\n         for (int j = 0; j <= i; j++) {\n             xx[j][m--] = num;\n             num++;\n         }\n         System.out.println();\n     }\n     for (int i = 0; i <= 5; i++) {\n         for (int j = 0; j <= 5; j++) {\n             if (xx[i][j] != -1)\n                 System.out.print(xx[j][i] + \" \");\n         }\n         System.out.println();\n     }\n```\n由这道题目引申出了数组的操作，上述题目我们自己定义了xx数组，自己定义的数组增、删、边界都要自己维护，但是一般项目开发中我们 都会选择array数组，array数组本身是一个动态数组，数组中定义了元素的增删操作也可以通过迭代器来判断边界。\n具体举例如下：\n\n```java\npublic static void main(String[] args) {\n       ArrayList<Integer> arrlist = new ArrayList<Integer>(5);//ArrayList 类型参数不能为基本类型\n       for (int i = 0; i < 5; i++) {\n           arrlist.add(i);\n       }\n       arrlist.add(0);\n       Iterator<Integer> it;\n       arrprint(arrlist.iterator());\n       for (it = arrlist.iterator(); it.hasNext(); ) {\n           if (it.next() == 0) {\n               it.remove();//如果是arrlist删除的话就会报错\n//                arrlist.remove((Integer)0); //Exception in thread \"main\" java.util.ConcurrentModificationException\n           }\n       }\n       System.out.println();\n       arrprint(arrlist.iterator());\n   }\n\n   private static void arrprint(Iterator<Integer> it) {\n       for (; it.hasNext(); ) {\n           System.out.print(it.next() + \" \");\n       }\n   }\n```\n结果如下\n\n```bash\n0 1 2 3 4 0\n1 2 3 4\n```\n上面报错的原因是当我们找到要删除的元素时直接去arraylist里面删除了元素没有告诉 迭代器，导致迭代器失效了。而直接使用迭代器删除就不会有这个问题，迭代器维护了数组的 下表索引。 在Java中，有很多的数据容器上面是以arrarylist为例，对于这些容器的操作有很多的共性。Java采用了迭代器来为各种容器提供了公共的操作接口。这样使得对容器的遍历操作与其具体的底层实现相隔离，达到解耦的效果，同时也更安全更便捷。\n迭代器是一种设计模式，它是一个对象，它可以遍历并选择序列中的对象，而开发人员不需要了解该序列的底层结构。迭代器通常被称为“轻量级”对象，因为创建它的代价小。\n\n1. Java中的Iterator有以下几个特点：\n11. 使用方法iterator()要求容器返回一个Iterator。第一次调用Iterator的next()方法时，它返回序列的第一个元素。注意：iterator()方法是java.lang.Iterable接口,被Collection继承。\n11. 使用next()获得序列中的下一个元素。\n11. 使用hasNext()检查序列中是否还有元素。\n11. 使用remove()将迭代器新返回的元素删除。\n","slug":"iterator","published":1,"updated":"2017-03-08T13:31:12.349Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj70mt0q2000l60tu3a1pfoam","content":"<p>这是一道java基础题目，要求输出一个数组<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">1 3 6 10 15 21</div><div class=\"line\">2 5 9 14 20</div><div class=\"line\">4 8 13 19</div><div class=\"line\">7 12 18</div><div class=\"line\">11 17</div><div class=\"line\">16</div></pre></td></tr></table></figure></p>\n<a id=\"more\"></a>\n<p>基本上就是数组的操作，代码如下：<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">int</span> xx[][] = <span class=\"keyword\">new</span> <span class=\"keyword\">int</span>[<span class=\"number\">6</span>][<span class=\"number\">6</span>];</div><div class=\"line\"><span class=\"keyword\">int</span> num = <span class=\"number\">1</span>;</div><div class=\"line\"></div><div class=\"line\">     <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt;= <span class=\"number\">5</span>; i++) &#123;</div><div class=\"line\"></div><div class=\"line\">         <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> j = <span class=\"number\">0</span>; j &lt;= <span class=\"number\">5</span>; j++) &#123;</div><div class=\"line\"></div><div class=\"line\">             xx[i][j] = -<span class=\"number\">1</span>;</div><div class=\"line\"></div><div class=\"line\">         &#125;</div><div class=\"line\"></div><div class=\"line\">     &#125;</div><div class=\"line\">     <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt;= <span class=\"number\">5</span>; i++) &#123;</div><div class=\"line\">         <span class=\"keyword\">int</span> m = i;</div><div class=\"line\">         <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> j = <span class=\"number\">0</span>; j &lt;= i; j++) &#123;</div><div class=\"line\">             xx[j][m--] = num;</div><div class=\"line\">             num++;</div><div class=\"line\">         &#125;</div><div class=\"line\">         System.out.println();</div><div class=\"line\">     &#125;</div><div class=\"line\">     <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt;= <span class=\"number\">5</span>; i++) &#123;</div><div class=\"line\">         <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> j = <span class=\"number\">0</span>; j &lt;= <span class=\"number\">5</span>; j++) &#123;</div><div class=\"line\">             <span class=\"keyword\">if</span> (xx[i][j] != -<span class=\"number\">1</span>)</div><div class=\"line\">                 System.out.print(xx[j][i] + <span class=\"string\">\" \"</span>);</div><div class=\"line\">         &#125;</div><div class=\"line\">         System.out.println();</div><div class=\"line\">     &#125;</div></pre></td></tr></table></figure></p>\n<p>由这道题目引申出了数组的操作，上述题目我们自己定义了xx数组，自己定义的数组增、删、边界都要自己维护，但是一般项目开发中我们 都会选择array数组，array数组本身是一个动态数组，数组中定义了元素的增删操作也可以通过迭代器来判断边界。<br>具体举例如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</div><div class=\"line\">       ArrayList&lt;Integer&gt; arrlist = <span class=\"keyword\">new</span> ArrayList&lt;Integer&gt;(<span class=\"number\">5</span>);<span class=\"comment\">//ArrayList 类型参数不能为基本类型</span></div><div class=\"line\">       <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">5</span>; i++) &#123;</div><div class=\"line\">           arrlist.add(i);</div><div class=\"line\">       &#125;</div><div class=\"line\">       arrlist.add(<span class=\"number\">0</span>);</div><div class=\"line\">       Iterator&lt;Integer&gt; it;</div><div class=\"line\">       arrprint(arrlist.iterator());</div><div class=\"line\">       <span class=\"keyword\">for</span> (it = arrlist.iterator(); it.hasNext(); ) &#123;</div><div class=\"line\">           <span class=\"keyword\">if</span> (it.next() == <span class=\"number\">0</span>) &#123;</div><div class=\"line\">               it.remove();<span class=\"comment\">//如果是arrlist删除的话就会报错</span></div><div class=\"line\"><span class=\"comment\">//                arrlist.remove((Integer)0); //Exception in thread \"main\" java.util.ConcurrentModificationException</span></div><div class=\"line\">           &#125;</div><div class=\"line\">       &#125;</div><div class=\"line\">       System.out.println();</div><div class=\"line\">       arrprint(arrlist.iterator());</div><div class=\"line\">   &#125;</div><div class=\"line\"></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">arrprint</span><span class=\"params\">(Iterator&lt;Integer&gt; it)</span> </span>&#123;</div><div class=\"line\">       <span class=\"keyword\">for</span> (; it.hasNext(); ) &#123;</div><div class=\"line\">           System.out.print(it.next() + <span class=\"string\">\" \"</span>);</div><div class=\"line\">       &#125;</div><div class=\"line\">   &#125;</div></pre></td></tr></table></figure>\n<p>结果如下</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">0 1 2 3 4 0</div><div class=\"line\">1 2 3 4</div></pre></td></tr></table></figure>\n<p>上面报错的原因是当我们找到要删除的元素时直接去arraylist里面删除了元素没有告诉 迭代器，导致迭代器失效了。而直接使用迭代器删除就不会有这个问题，迭代器维护了数组的 下表索引。 在Java中，有很多的数据容器上面是以arrarylist为例，对于这些容器的操作有很多的共性。Java采用了迭代器来为各种容器提供了公共的操作接口。这样使得对容器的遍历操作与其具体的底层实现相隔离，达到解耦的效果，同时也更安全更便捷。<br>迭代器是一种设计模式，它是一个对象，它可以遍历并选择序列中的对象，而开发人员不需要了解该序列的底层结构。迭代器通常被称为“轻量级”对象，因为创建它的代价小。</p>\n<ol>\n<li>Java中的Iterator有以下几个特点：</li>\n<li>使用方法iterator()要求容器返回一个Iterator。第一次调用Iterator的next()方法时，它返回序列的第一个元素。注意：iterator()方法是java.lang.Iterable接口,被Collection继承。</li>\n<li>使用next()获得序列中的下一个元素。</li>\n<li>使用hasNext()检查序列中是否还有元素。</li>\n<li>使用remove()将迭代器新返回的元素删除。</li>\n</ol>\n","excerpt":"<p>这是一道java基础题目，要求输出一个数组<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">1 3 6 10 15 21</div><div class=\"line\">2 5 9 14 20</div><div class=\"line\">4 8 13 19</div><div class=\"line\">7 12 18</div><div class=\"line\">11 17</div><div class=\"line\">16</div></pre></td></tr></table></figure></p>","more":"<p>基本上就是数组的操作，代码如下：<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">int</span> xx[][] = <span class=\"keyword\">new</span> <span class=\"keyword\">int</span>[<span class=\"number\">6</span>][<span class=\"number\">6</span>];</div><div class=\"line\"><span class=\"keyword\">int</span> num = <span class=\"number\">1</span>;</div><div class=\"line\"></div><div class=\"line\">     <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt;= <span class=\"number\">5</span>; i++) &#123;</div><div class=\"line\"></div><div class=\"line\">         <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> j = <span class=\"number\">0</span>; j &lt;= <span class=\"number\">5</span>; j++) &#123;</div><div class=\"line\"></div><div class=\"line\">             xx[i][j] = -<span class=\"number\">1</span>;</div><div class=\"line\"></div><div class=\"line\">         &#125;</div><div class=\"line\"></div><div class=\"line\">     &#125;</div><div class=\"line\">     <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt;= <span class=\"number\">5</span>; i++) &#123;</div><div class=\"line\">         <span class=\"keyword\">int</span> m = i;</div><div class=\"line\">         <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> j = <span class=\"number\">0</span>; j &lt;= i; j++) &#123;</div><div class=\"line\">             xx[j][m--] = num;</div><div class=\"line\">             num++;</div><div class=\"line\">         &#125;</div><div class=\"line\">         System.out.println();</div><div class=\"line\">     &#125;</div><div class=\"line\">     <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt;= <span class=\"number\">5</span>; i++) &#123;</div><div class=\"line\">         <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> j = <span class=\"number\">0</span>; j &lt;= <span class=\"number\">5</span>; j++) &#123;</div><div class=\"line\">             <span class=\"keyword\">if</span> (xx[i][j] != -<span class=\"number\">1</span>)</div><div class=\"line\">                 System.out.print(xx[j][i] + <span class=\"string\">\" \"</span>);</div><div class=\"line\">         &#125;</div><div class=\"line\">         System.out.println();</div><div class=\"line\">     &#125;</div></pre></td></tr></table></figure></p>\n<p>由这道题目引申出了数组的操作，上述题目我们自己定义了xx数组，自己定义的数组增、删、边界都要自己维护，但是一般项目开发中我们 都会选择array数组，array数组本身是一个动态数组，数组中定义了元素的增删操作也可以通过迭代器来判断边界。<br>具体举例如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</div><div class=\"line\">       ArrayList&lt;Integer&gt; arrlist = <span class=\"keyword\">new</span> ArrayList&lt;Integer&gt;(<span class=\"number\">5</span>);<span class=\"comment\">//ArrayList 类型参数不能为基本类型</span></div><div class=\"line\">       <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"number\">5</span>; i++) &#123;</div><div class=\"line\">           arrlist.add(i);</div><div class=\"line\">       &#125;</div><div class=\"line\">       arrlist.add(<span class=\"number\">0</span>);</div><div class=\"line\">       Iterator&lt;Integer&gt; it;</div><div class=\"line\">       arrprint(arrlist.iterator());</div><div class=\"line\">       <span class=\"keyword\">for</span> (it = arrlist.iterator(); it.hasNext(); ) &#123;</div><div class=\"line\">           <span class=\"keyword\">if</span> (it.next() == <span class=\"number\">0</span>) &#123;</div><div class=\"line\">               it.remove();<span class=\"comment\">//如果是arrlist删除的话就会报错</span></div><div class=\"line\"><span class=\"comment\">//                arrlist.remove((Integer)0); //Exception in thread \"main\" java.util.ConcurrentModificationException</span></div><div class=\"line\">           &#125;</div><div class=\"line\">       &#125;</div><div class=\"line\">       System.out.println();</div><div class=\"line\">       arrprint(arrlist.iterator());</div><div class=\"line\">   &#125;</div><div class=\"line\"></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">arrprint</span><span class=\"params\">(Iterator&lt;Integer&gt; it)</span> </span>&#123;</div><div class=\"line\">       <span class=\"keyword\">for</span> (; it.hasNext(); ) &#123;</div><div class=\"line\">           System.out.print(it.next() + <span class=\"string\">\" \"</span>);</div><div class=\"line\">       &#125;</div><div class=\"line\">   &#125;</div></pre></td></tr></table></figure>\n<p>结果如下</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">0 1 2 3 4 0</div><div class=\"line\">1 2 3 4</div></pre></td></tr></table></figure>\n<p>上面报错的原因是当我们找到要删除的元素时直接去arraylist里面删除了元素没有告诉 迭代器，导致迭代器失效了。而直接使用迭代器删除就不会有这个问题，迭代器维护了数组的 下表索引。 在Java中，有很多的数据容器上面是以arrarylist为例，对于这些容器的操作有很多的共性。Java采用了迭代器来为各种容器提供了公共的操作接口。这样使得对容器的遍历操作与其具体的底层实现相隔离，达到解耦的效果，同时也更安全更便捷。<br>迭代器是一种设计模式，它是一个对象，它可以遍历并选择序列中的对象，而开发人员不需要了解该序列的底层结构。迭代器通常被称为“轻量级”对象，因为创建它的代价小。</p>\n<ol>\n<li>Java中的Iterator有以下几个特点：</li>\n<li>使用方法iterator()要求容器返回一个Iterator。第一次调用Iterator的next()方法时，它返回序列的第一个元素。注意：iterator()方法是java.lang.Iterable接口,被Collection继承。</li>\n<li>使用next()获得序列中的下一个元素。</li>\n<li>使用hasNext()检查序列中是否还有元素。</li>\n<li>使用remove()将迭代器新返回的元素删除。</li>\n</ol>"},{"title":"吾家有女（一）","date":"2017-05-08T16:00:00.000Z","_content":"我爱自己的孩子，则要为之计深远\n<!-- more -->\n每天下班都很高兴的往家里跑，家里有我的女儿在等我回去，以前妻子经常打电话给我说你女儿想你了，其实心里很美，偶尔她也会在电话的那头大喊爸爸我想你了，听到这些的时候任何坚强的心都会被融化。\n# 约定\n最近小宝变化很大，她有了自己的想法，很有主见，以前一般我带她出去逛公园我都会和她约法三章，出门需要自己走路，她也倒是基本能遵守自己的诺言，偶尔闹着要我抱起来的时候我会提醒她自己的约定，她就不再要求了，有时候她确实有些累了我答应她再走一百米就抱她起来。现在小宝有点耍赖，经常毁约，我有时候静下来想是我们做了什么让他学习了这种毁约，小孩的学习能力是很强的也很敏感，我和妻子现在出门前就让她录音为证，一般是她自己说：“我乖乖的听爸爸妈妈的话，不提要求，我自己走路”我会在边上提醒她，保证自己说的话算数，她会补充“我保证XXX”有时候会说“好吧”似乎很无奈的样子。有了录音我们在他耍赖皮的时候就会给她放出自己说的话来听，倒也能管用。小孩子的教育是潜移默化的，家庭教育至关重要，我们作为父母对孩子的影响太大了。\n# 家教\n孩子也是父母的老师，自从有了小宝我自己也改变了很多，以前我很急躁，小宝哭的越厉害我其实是越急躁，有时候甚至发脾气，其实这适得其反，倒是妻子开始的时候能够耐心的哄小宝，我从妻子的身上也学习到了挺多的，妻子性格是比较温和的，但是小宝的急脾气把妻子的耐心磨掉了很多，我觉得妻子现在也越来越没有耐心了，倒是我现在对小宝不像以前那么急躁，但是我也很严厉的，有时候她不听话不乖的时候，我生气了她也会怕，她会对我说爸爸别生气了，有时候很无助不知道如何恰到好处的把握分寸，太过放纵了怕她不听话学坏了，太过严厉了又担心束缚了她的天性。\n作人父母也是要学习的，为人父母是有责任的，这种责任不仅仅是给他吃饭穿衣，最重要的是对他的言传身教。\n# 后记\n古人云父母之爱子则为之计深远，我该如何做才能算是为之计深远呢？","source":"_posts/mybaby01.md","raw":"---\ntitle: 吾家有女（一）\ndate: 2017-05-09\ntags: 生活\n---\n我爱自己的孩子，则要为之计深远\n<!-- more -->\n每天下班都很高兴的往家里跑，家里有我的女儿在等我回去，以前妻子经常打电话给我说你女儿想你了，其实心里很美，偶尔她也会在电话的那头大喊爸爸我想你了，听到这些的时候任何坚强的心都会被融化。\n# 约定\n最近小宝变化很大，她有了自己的想法，很有主见，以前一般我带她出去逛公园我都会和她约法三章，出门需要自己走路，她也倒是基本能遵守自己的诺言，偶尔闹着要我抱起来的时候我会提醒她自己的约定，她就不再要求了，有时候她确实有些累了我答应她再走一百米就抱她起来。现在小宝有点耍赖，经常毁约，我有时候静下来想是我们做了什么让他学习了这种毁约，小孩的学习能力是很强的也很敏感，我和妻子现在出门前就让她录音为证，一般是她自己说：“我乖乖的听爸爸妈妈的话，不提要求，我自己走路”我会在边上提醒她，保证自己说的话算数，她会补充“我保证XXX”有时候会说“好吧”似乎很无奈的样子。有了录音我们在他耍赖皮的时候就会给她放出自己说的话来听，倒也能管用。小孩子的教育是潜移默化的，家庭教育至关重要，我们作为父母对孩子的影响太大了。\n# 家教\n孩子也是父母的老师，自从有了小宝我自己也改变了很多，以前我很急躁，小宝哭的越厉害我其实是越急躁，有时候甚至发脾气，其实这适得其反，倒是妻子开始的时候能够耐心的哄小宝，我从妻子的身上也学习到了挺多的，妻子性格是比较温和的，但是小宝的急脾气把妻子的耐心磨掉了很多，我觉得妻子现在也越来越没有耐心了，倒是我现在对小宝不像以前那么急躁，但是我也很严厉的，有时候她不听话不乖的时候，我生气了她也会怕，她会对我说爸爸别生气了，有时候很无助不知道如何恰到好处的把握分寸，太过放纵了怕她不听话学坏了，太过严厉了又担心束缚了她的天性。\n作人父母也是要学习的，为人父母是有责任的，这种责任不仅仅是给他吃饭穿衣，最重要的是对他的言传身教。\n# 后记\n古人云父母之爱子则为之计深远，我该如何做才能算是为之计深远呢？","slug":"mybaby01","published":1,"updated":"2017-05-10T16:31:52.609Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj70mt0q6000n60tu24fljvsv","content":"<p>我爱自己的孩子，则要为之计深远<br><a id=\"more\"></a><br>每天下班都很高兴的往家里跑，家里有我的女儿在等我回去，以前妻子经常打电话给我说你女儿想你了，其实心里很美，偶尔她也会在电话的那头大喊爸爸我想你了，听到这些的时候任何坚强的心都会被融化。</p>\n<h1 id=\"约定\"><a href=\"#约定\" class=\"headerlink\" title=\"约定\"></a>约定</h1><p>最近小宝变化很大，她有了自己的想法，很有主见，以前一般我带她出去逛公园我都会和她约法三章，出门需要自己走路，她也倒是基本能遵守自己的诺言，偶尔闹着要我抱起来的时候我会提醒她自己的约定，她就不再要求了，有时候她确实有些累了我答应她再走一百米就抱她起来。现在小宝有点耍赖，经常毁约，我有时候静下来想是我们做了什么让他学习了这种毁约，小孩的学习能力是很强的也很敏感，我和妻子现在出门前就让她录音为证，一般是她自己说：“我乖乖的听爸爸妈妈的话，不提要求，我自己走路”我会在边上提醒她，保证自己说的话算数，她会补充“我保证XXX”有时候会说“好吧”似乎很无奈的样子。有了录音我们在他耍赖皮的时候就会给她放出自己说的话来听，倒也能管用。小孩子的教育是潜移默化的，家庭教育至关重要，我们作为父母对孩子的影响太大了。</p>\n<h1 id=\"家教\"><a href=\"#家教\" class=\"headerlink\" title=\"家教\"></a>家教</h1><p>孩子也是父母的老师，自从有了小宝我自己也改变了很多，以前我很急躁，小宝哭的越厉害我其实是越急躁，有时候甚至发脾气，其实这适得其反，倒是妻子开始的时候能够耐心的哄小宝，我从妻子的身上也学习到了挺多的，妻子性格是比较温和的，但是小宝的急脾气把妻子的耐心磨掉了很多，我觉得妻子现在也越来越没有耐心了，倒是我现在对小宝不像以前那么急躁，但是我也很严厉的，有时候她不听话不乖的时候，我生气了她也会怕，她会对我说爸爸别生气了，有时候很无助不知道如何恰到好处的把握分寸，太过放纵了怕她不听话学坏了，太过严厉了又担心束缚了她的天性。<br>作人父母也是要学习的，为人父母是有责任的，这种责任不仅仅是给他吃饭穿衣，最重要的是对他的言传身教。</p>\n<h1 id=\"后记\"><a href=\"#后记\" class=\"headerlink\" title=\"后记\"></a>后记</h1><p>古人云父母之爱子则为之计深远，我该如何做才能算是为之计深远呢？</p>\n","excerpt":"<p>我爱自己的孩子，则要为之计深远<br>","more":"<br>每天下班都很高兴的往家里跑，家里有我的女儿在等我回去，以前妻子经常打电话给我说你女儿想你了，其实心里很美，偶尔她也会在电话的那头大喊爸爸我想你了，听到这些的时候任何坚强的心都会被融化。</p>\n<h1 id=\"约定\"><a href=\"#约定\" class=\"headerlink\" title=\"约定\"></a>约定</h1><p>最近小宝变化很大，她有了自己的想法，很有主见，以前一般我带她出去逛公园我都会和她约法三章，出门需要自己走路，她也倒是基本能遵守自己的诺言，偶尔闹着要我抱起来的时候我会提醒她自己的约定，她就不再要求了，有时候她确实有些累了我答应她再走一百米就抱她起来。现在小宝有点耍赖，经常毁约，我有时候静下来想是我们做了什么让他学习了这种毁约，小孩的学习能力是很强的也很敏感，我和妻子现在出门前就让她录音为证，一般是她自己说：“我乖乖的听爸爸妈妈的话，不提要求，我自己走路”我会在边上提醒她，保证自己说的话算数，她会补充“我保证XXX”有时候会说“好吧”似乎很无奈的样子。有了录音我们在他耍赖皮的时候就会给她放出自己说的话来听，倒也能管用。小孩子的教育是潜移默化的，家庭教育至关重要，我们作为父母对孩子的影响太大了。</p>\n<h1 id=\"家教\"><a href=\"#家教\" class=\"headerlink\" title=\"家教\"></a>家教</h1><p>孩子也是父母的老师，自从有了小宝我自己也改变了很多，以前我很急躁，小宝哭的越厉害我其实是越急躁，有时候甚至发脾气，其实这适得其反，倒是妻子开始的时候能够耐心的哄小宝，我从妻子的身上也学习到了挺多的，妻子性格是比较温和的，但是小宝的急脾气把妻子的耐心磨掉了很多，我觉得妻子现在也越来越没有耐心了，倒是我现在对小宝不像以前那么急躁，但是我也很严厉的，有时候她不听话不乖的时候，我生气了她也会怕，她会对我说爸爸别生气了，有时候很无助不知道如何恰到好处的把握分寸，太过放纵了怕她不听话学坏了，太过严厉了又担心束缚了她的天性。<br>作人父母也是要学习的，为人父母是有责任的，这种责任不仅仅是给他吃饭穿衣，最重要的是对他的言传身教。</p>\n<h1 id=\"后记\"><a href=\"#后记\" class=\"headerlink\" title=\"后记\"></a>后记</h1><p>古人云父母之爱子则为之计深远，我该如何做才能算是为之计深远呢？</p>"},{"title":"scala Option、Some、None","date":"2017-08-31T15:09:46.000Z","_content":"null空指针异常的根源\n\n大多数语言都有一个特殊的关键字或者对象来表示一个对象引用的是“无”大部分都是NULL或者null，在写c/c++中是NULL，NULL是个常量与0等价，在Java，它是null。在Java 里，null 是一个关键字，不管是字符串常量还是关键字他们不是一个对象，所以对它调用任何方法都是非法的。但是对程序的设计者来说这似乎又是必须的，如过我们的程序不返回任何内容那就是空，但是空也是引发问题最多的原因，c/c++的好多bug与NULL有关，java的null可以引起指针异常导致程序崩溃。scala设计了option优雅的解决了这个问题。\n<!-- more -->\n\nScala的Option类型\n\n为了让所有东西都是对象的目标更加一致，也为了遵循函数式编程的习惯，Scala鼓励你在变量和函数返回值可能不会引用任何值的时候使用Option类型。在没有值的时候，使用None，这是Option的一个子类。如果有值可以引用，就使用Some来包含这个值。Some也是Option的子类。\nNone被声明为一个对象，而不是一个类，因为我们只需要它的一个实例。这样，它多少有点像null关键字，但它却是一个实实在在的，有方法的对象。如此以来我们即使对None对象进行了操作也不过是抛出异常直接导致崩溃的概率底了好的。\n\n比如scala的sdk中就有大量的应用\nOption类型的值通常作为Scala集合类型（List,Map等）操作的返回类型。比如Map的get方法：\n```scala\nscala> val capitals = Map(\"France\"->\"Paris\", \"Japan\"->\"Tokyo\", \"China\"->\"Beijing\")\ncapitals: scala.collection.immutable.Map[String,String] = Map(France -> Paris, Japan -> Tokyo, China -> Beijing)\n\nscala> capitals get \"France\"\nres0: Option[String] = Some(Paris)\n\nscala> capitals get \"North Pole\"\nres1: Option[String] = None\n```\nOption有两个子类别，Some和None。当程序回传Some的时候，代表这个函式成功地给了你一个String，而你可以透过get()函数拿到那个String，如果程序返回的是None，则代表没有字符串可以给你。\n在返回None，也就是没有String给你的时候，如果你还硬要调用get()来取得 String 的话，Scala一样是会抛出一个NoSuchElementException异常给你的。\n我们也可以选用另外一个方法，getOrElse。这个方法在这个Option是Some的实例时返回对应的值，而在是None的实例时返回传入的参数。换句话说，传入getOrElse的参数实际上是默认返回值。\n```scala\nscala> capitals get \"North Pole\" get\nwarning: there was one feature warning; re-run with -feature for details\njava.util.NoSuchElementException: None.get\n  at scala.None$.get(Option.scala:347)\n  at scala.None$.get(Option.scala:345)\n  ... 33 elided\n\nscala> capitals get \"France\" get\nwarning: there was one feature warning; re-run with -feature for details\nres3: String = Paris\n\nscala> (capitals get \"North Pole\") getOrElse \"Oops\"\nres7: String = Oops\n\nscala> capitals get \"France\" getOrElse \"Oops\"\nres8: String = Paris\n```\n通过模式匹配分离可选值，如果匹配的值是Some的话，将Some里的值抽出赋给x变量：\n```scala\ndef showCapital(x: Option[String]) = x match {\n    case Some(s) => s\n    case None => \"?\"\n}\n```\n\nScala程序使用Option非常频繁，在Java中使用null来表示空值，因此Java程序需要关心那些变量可能是null,尽管而这些变量出现null的可能性很低，但一但出现，很难查出为什么出现NullPointerException。\nScala的Option类型可以避免这种情况，因此Scala应用推荐使用Option类型来代表一些可选值。使用Option类型，读者一眼就可以看出这种类型的值可能为None。\n\n实际上，多亏Scala的静态类型，你并不能错误地尝试在一个可能为null的值上调用方法。虽然在Java中这是个很容易犯的错误，它在Scala却通不过编译，这是因为Java中没有检查变量是否为null的编程作为变成Scala中的类型错误（不能将Option[String]当做String来使用）。所以，Option的使用极强地鼓励了更加弹性的编程习惯。\n\n详解Option[T]\n\n在Scala里Option[T]实际上是一个容器，就像数组或是List一样，你可以把他看成是一个可能有零到一个元素的List。\n当你的Option里面有东西的时候，这个List的长度是1（也就是 Some），而当你的Option里没有东西的时候，它的长度是0（也就是 None）。\n\nfor循环\n\n如果我们把Option当成一般的List来用，并且用一个for循环来走访这个Option的时候，如果Option是None，那这个for循环里的程序代码自然不会执行，于是我们就达到了「不用检查Option是否为None这件事。\n```scala\nscala> val map1 = Map(\"key1\" -> \"value1\")\nmap1: scala.collection.immutable.Map[String,String] = Map(key1 -> value1)\n\nscala> val value1 = map1.get(\"key1\")\nvalue1: Option[String] = Some(value1)\n\nscala> val value2 = map1.get(\"key2\")\nvalue2: Option[String] = None\n\nscala> def printContentLength(x: Option[String]) {\n     |   for (c <- x){\n     |     println(c.length)\n     |   }\n     | }\nprintContentLength: (x: Option[String])Unit\n\nscala> printContentLength(value1)\n6\n\nscala> printContentLength(value2)\n```\nmap操作\n\n在函数式编程中有一个核心的概念之一是转换，所以大部份支持函数式编程语言，都支持一种叫map()的动作，这个动作是可以帮你把某个容器的内容，套上一些动作之后，变成另一个新的容器。\n现在我们考虑如何用Option的map方法实现length: xxx的输出形式：\n\n先算出 Option 容器内字符串的长度\n然后在长度前面加上 \"length: \" 字样\n最后把容器走访一次，印出容器内的东西\n```scala\nscala> value1.map(_.length).map(\"length: \" + _).foreach(println)\nlength: 6\n\nscala> value1.map(\"length: \" + _.length).foreach(println)\nlength: 6\n```\n","source":"_posts/scala-none.md","raw":"---\ntitle: scala Option、Some、None\ndate: 2017-08-31 23:09:46\n\ncategory: 技术\ntags: scala\n---\nnull空指针异常的根源\n\n大多数语言都有一个特殊的关键字或者对象来表示一个对象引用的是“无”大部分都是NULL或者null，在写c/c++中是NULL，NULL是个常量与0等价，在Java，它是null。在Java 里，null 是一个关键字，不管是字符串常量还是关键字他们不是一个对象，所以对它调用任何方法都是非法的。但是对程序的设计者来说这似乎又是必须的，如过我们的程序不返回任何内容那就是空，但是空也是引发问题最多的原因，c/c++的好多bug与NULL有关，java的null可以引起指针异常导致程序崩溃。scala设计了option优雅的解决了这个问题。\n<!-- more -->\n\nScala的Option类型\n\n为了让所有东西都是对象的目标更加一致，也为了遵循函数式编程的习惯，Scala鼓励你在变量和函数返回值可能不会引用任何值的时候使用Option类型。在没有值的时候，使用None，这是Option的一个子类。如果有值可以引用，就使用Some来包含这个值。Some也是Option的子类。\nNone被声明为一个对象，而不是一个类，因为我们只需要它的一个实例。这样，它多少有点像null关键字，但它却是一个实实在在的，有方法的对象。如此以来我们即使对None对象进行了操作也不过是抛出异常直接导致崩溃的概率底了好的。\n\n比如scala的sdk中就有大量的应用\nOption类型的值通常作为Scala集合类型（List,Map等）操作的返回类型。比如Map的get方法：\n```scala\nscala> val capitals = Map(\"France\"->\"Paris\", \"Japan\"->\"Tokyo\", \"China\"->\"Beijing\")\ncapitals: scala.collection.immutable.Map[String,String] = Map(France -> Paris, Japan -> Tokyo, China -> Beijing)\n\nscala> capitals get \"France\"\nres0: Option[String] = Some(Paris)\n\nscala> capitals get \"North Pole\"\nres1: Option[String] = None\n```\nOption有两个子类别，Some和None。当程序回传Some的时候，代表这个函式成功地给了你一个String，而你可以透过get()函数拿到那个String，如果程序返回的是None，则代表没有字符串可以给你。\n在返回None，也就是没有String给你的时候，如果你还硬要调用get()来取得 String 的话，Scala一样是会抛出一个NoSuchElementException异常给你的。\n我们也可以选用另外一个方法，getOrElse。这个方法在这个Option是Some的实例时返回对应的值，而在是None的实例时返回传入的参数。换句话说，传入getOrElse的参数实际上是默认返回值。\n```scala\nscala> capitals get \"North Pole\" get\nwarning: there was one feature warning; re-run with -feature for details\njava.util.NoSuchElementException: None.get\n  at scala.None$.get(Option.scala:347)\n  at scala.None$.get(Option.scala:345)\n  ... 33 elided\n\nscala> capitals get \"France\" get\nwarning: there was one feature warning; re-run with -feature for details\nres3: String = Paris\n\nscala> (capitals get \"North Pole\") getOrElse \"Oops\"\nres7: String = Oops\n\nscala> capitals get \"France\" getOrElse \"Oops\"\nres8: String = Paris\n```\n通过模式匹配分离可选值，如果匹配的值是Some的话，将Some里的值抽出赋给x变量：\n```scala\ndef showCapital(x: Option[String]) = x match {\n    case Some(s) => s\n    case None => \"?\"\n}\n```\n\nScala程序使用Option非常频繁，在Java中使用null来表示空值，因此Java程序需要关心那些变量可能是null,尽管而这些变量出现null的可能性很低，但一但出现，很难查出为什么出现NullPointerException。\nScala的Option类型可以避免这种情况，因此Scala应用推荐使用Option类型来代表一些可选值。使用Option类型，读者一眼就可以看出这种类型的值可能为None。\n\n实际上，多亏Scala的静态类型，你并不能错误地尝试在一个可能为null的值上调用方法。虽然在Java中这是个很容易犯的错误，它在Scala却通不过编译，这是因为Java中没有检查变量是否为null的编程作为变成Scala中的类型错误（不能将Option[String]当做String来使用）。所以，Option的使用极强地鼓励了更加弹性的编程习惯。\n\n详解Option[T]\n\n在Scala里Option[T]实际上是一个容器，就像数组或是List一样，你可以把他看成是一个可能有零到一个元素的List。\n当你的Option里面有东西的时候，这个List的长度是1（也就是 Some），而当你的Option里没有东西的时候，它的长度是0（也就是 None）。\n\nfor循环\n\n如果我们把Option当成一般的List来用，并且用一个for循环来走访这个Option的时候，如果Option是None，那这个for循环里的程序代码自然不会执行，于是我们就达到了「不用检查Option是否为None这件事。\n```scala\nscala> val map1 = Map(\"key1\" -> \"value1\")\nmap1: scala.collection.immutable.Map[String,String] = Map(key1 -> value1)\n\nscala> val value1 = map1.get(\"key1\")\nvalue1: Option[String] = Some(value1)\n\nscala> val value2 = map1.get(\"key2\")\nvalue2: Option[String] = None\n\nscala> def printContentLength(x: Option[String]) {\n     |   for (c <- x){\n     |     println(c.length)\n     |   }\n     | }\nprintContentLength: (x: Option[String])Unit\n\nscala> printContentLength(value1)\n6\n\nscala> printContentLength(value2)\n```\nmap操作\n\n在函数式编程中有一个核心的概念之一是转换，所以大部份支持函数式编程语言，都支持一种叫map()的动作，这个动作是可以帮你把某个容器的内容，套上一些动作之后，变成另一个新的容器。\n现在我们考虑如何用Option的map方法实现length: xxx的输出形式：\n\n先算出 Option 容器内字符串的长度\n然后在长度前面加上 \"length: \" 字样\n最后把容器走访一次，印出容器内的东西\n```scala\nscala> value1.map(_.length).map(\"length: \" + _).foreach(println)\nlength: 6\n\nscala> value1.map(\"length: \" + _.length).foreach(println)\nlength: 6\n```\n","slug":"scala-none","published":1,"updated":"2017-08-31T15:38:26.685Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj70mt0qd000p60tui7nopoch","content":"<p>null空指针异常的根源</p>\n<p>大多数语言都有一个特殊的关键字或者对象来表示一个对象引用的是“无”大部分都是NULL或者null，在写c/c++中是NULL，NULL是个常量与0等价，在Java，它是null。在Java 里，null 是一个关键字，不管是字符串常量还是关键字他们不是一个对象，所以对它调用任何方法都是非法的。但是对程序的设计者来说这似乎又是必须的，如过我们的程序不返回任何内容那就是空，但是空也是引发问题最多的原因，c/c++的好多bug与NULL有关，java的null可以引起指针异常导致程序崩溃。scala设计了option优雅的解决了这个问题。<br><a id=\"more\"></a></p>\n<p>Scala的Option类型</p>\n<p>为了让所有东西都是对象的目标更加一致，也为了遵循函数式编程的习惯，Scala鼓励你在变量和函数返回值可能不会引用任何值的时候使用Option类型。在没有值的时候，使用None，这是Option的一个子类。如果有值可以引用，就使用Some来包含这个值。Some也是Option的子类。<br>None被声明为一个对象，而不是一个类，因为我们只需要它的一个实例。这样，它多少有点像null关键字，但它却是一个实实在在的，有方法的对象。如此以来我们即使对None对象进行了操作也不过是抛出异常直接导致崩溃的概率底了好的。</p>\n<p>比如scala的sdk中就有大量的应用<br>Option类型的值通常作为Scala集合类型（List,Map等）操作的返回类型。比如Map的get方法：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> capitals = <span class=\"type\">Map</span>(<span class=\"string\">\"France\"</span>-&gt;<span class=\"string\">\"Paris\"</span>, <span class=\"string\">\"Japan\"</span>-&gt;<span class=\"string\">\"Tokyo\"</span>, <span class=\"string\">\"China\"</span>-&gt;<span class=\"string\">\"Beijing\"</span>)</div><div class=\"line\">capitals: scala.collection.immutable.<span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">String</span>] = <span class=\"type\">Map</span>(<span class=\"type\">France</span> -&gt; <span class=\"type\">Paris</span>, <span class=\"type\">Japan</span> -&gt; <span class=\"type\">Tokyo</span>, <span class=\"type\">China</span> -&gt; <span class=\"type\">Beijing</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; capitals get <span class=\"string\">\"France\"</span></div><div class=\"line\">res0: <span class=\"type\">Option</span>[<span class=\"type\">String</span>] = <span class=\"type\">Some</span>(<span class=\"type\">Paris</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; capitals get <span class=\"string\">\"North Pole\"</span></div><div class=\"line\">res1: <span class=\"type\">Option</span>[<span class=\"type\">String</span>] = <span class=\"type\">None</span></div></pre></td></tr></table></figure></p>\n<p>Option有两个子类别，Some和None。当程序回传Some的时候，代表这个函式成功地给了你一个String，而你可以透过get()函数拿到那个String，如果程序返回的是None，则代表没有字符串可以给你。<br>在返回None，也就是没有String给你的时候，如果你还硬要调用get()来取得 String 的话，Scala一样是会抛出一个NoSuchElementException异常给你的。<br>我们也可以选用另外一个方法，getOrElse。这个方法在这个Option是Some的实例时返回对应的值，而在是None的实例时返回传入的参数。换句话说，传入getOrElse的参数实际上是默认返回值。<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; capitals get <span class=\"string\">\"North Pole\"</span> get</div><div class=\"line\">warning: there was one feature warning; re-run <span class=\"keyword\">with</span> -feature <span class=\"keyword\">for</span> details</div><div class=\"line\">java.util.<span class=\"type\">NoSuchElementException</span>: <span class=\"type\">None</span>.get</div><div class=\"line\">  at scala.<span class=\"type\">None</span>$.get(<span class=\"type\">Option</span>.scala:<span class=\"number\">347</span>)</div><div class=\"line\">  at scala.<span class=\"type\">None</span>$.get(<span class=\"type\">Option</span>.scala:<span class=\"number\">345</span>)</div><div class=\"line\">  ... <span class=\"number\">33</span> elided</div><div class=\"line\"></div><div class=\"line\">scala&gt; capitals get <span class=\"string\">\"France\"</span> get</div><div class=\"line\">warning: there was one feature warning; re-run <span class=\"keyword\">with</span> -feature <span class=\"keyword\">for</span> details</div><div class=\"line\">res3: <span class=\"type\">String</span> = <span class=\"type\">Paris</span></div><div class=\"line\"></div><div class=\"line\">scala&gt; (capitals get <span class=\"string\">\"North Pole\"</span>) getOrElse <span class=\"string\">\"Oops\"</span></div><div class=\"line\">res7: <span class=\"type\">String</span> = <span class=\"type\">Oops</span></div><div class=\"line\"></div><div class=\"line\">scala&gt; capitals get <span class=\"string\">\"France\"</span> getOrElse <span class=\"string\">\"Oops\"</span></div><div class=\"line\">res8: <span class=\"type\">String</span> = <span class=\"type\">Paris</span></div></pre></td></tr></table></figure></p>\n<p>通过模式匹配分离可选值，如果匹配的值是Some的话，将Some里的值抽出赋给x变量：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">showCapital</span></span>(x: <span class=\"type\">Option</span>[<span class=\"type\">String</span>]) = x <span class=\"keyword\">match</span> &#123;</div><div class=\"line\">    <span class=\"keyword\">case</span> <span class=\"type\">Some</span>(s) =&gt; s</div><div class=\"line\">    <span class=\"keyword\">case</span> <span class=\"type\">None</span> =&gt; <span class=\"string\">\"?\"</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>Scala程序使用Option非常频繁，在Java中使用null来表示空值，因此Java程序需要关心那些变量可能是null,尽管而这些变量出现null的可能性很低，但一但出现，很难查出为什么出现NullPointerException。<br>Scala的Option类型可以避免这种情况，因此Scala应用推荐使用Option类型来代表一些可选值。使用Option类型，读者一眼就可以看出这种类型的值可能为None。</p>\n<p>实际上，多亏Scala的静态类型，你并不能错误地尝试在一个可能为null的值上调用方法。虽然在Java中这是个很容易犯的错误，它在Scala却通不过编译，这是因为Java中没有检查变量是否为null的编程作为变成Scala中的类型错误（不能将Option[String]当做String来使用）。所以，Option的使用极强地鼓励了更加弹性的编程习惯。</p>\n<p>详解Option[T]</p>\n<p>在Scala里Option[T]实际上是一个容器，就像数组或是List一样，你可以把他看成是一个可能有零到一个元素的List。<br>当你的Option里面有东西的时候，这个List的长度是1（也就是 Some），而当你的Option里没有东西的时候，它的长度是0（也就是 None）。</p>\n<p>for循环</p>\n<p>如果我们把Option当成一般的List来用，并且用一个for循环来走访这个Option的时候，如果Option是None，那这个for循环里的程序代码自然不会执行，于是我们就达到了「不用检查Option是否为None这件事。<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> map1 = <span class=\"type\">Map</span>(<span class=\"string\">\"key1\"</span> -&gt; <span class=\"string\">\"value1\"</span>)</div><div class=\"line\">map1: scala.collection.immutable.<span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">String</span>] = <span class=\"type\">Map</span>(key1 -&gt; value1)</div><div class=\"line\"></div><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> value1 = map1.get(<span class=\"string\">\"key1\"</span>)</div><div class=\"line\">value1: <span class=\"type\">Option</span>[<span class=\"type\">String</span>] = <span class=\"type\">Some</span>(value1)</div><div class=\"line\"></div><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> value2 = map1.get(<span class=\"string\">\"key2\"</span>)</div><div class=\"line\">value2: <span class=\"type\">Option</span>[<span class=\"type\">String</span>] = <span class=\"type\">None</span></div><div class=\"line\"></div><div class=\"line\">scala&gt; <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">printContentLength</span></span>(x: <span class=\"type\">Option</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">     |   <span class=\"keyword\">for</span> (c &lt;- x)&#123;</div><div class=\"line\">     |     println(c.length)</div><div class=\"line\">     |   &#125;</div><div class=\"line\">     | &#125;</div><div class=\"line\">printContentLength: (x: <span class=\"type\">Option</span>[<span class=\"type\">String</span>])<span class=\"type\">Unit</span></div><div class=\"line\"></div><div class=\"line\">scala&gt; printContentLength(value1)</div><div class=\"line\"><span class=\"number\">6</span></div><div class=\"line\"></div><div class=\"line\">scala&gt; printContentLength(value2)</div></pre></td></tr></table></figure></p>\n<p>map操作</p>\n<p>在函数式编程中有一个核心的概念之一是转换，所以大部份支持函数式编程语言，都支持一种叫map()的动作，这个动作是可以帮你把某个容器的内容，套上一些动作之后，变成另一个新的容器。<br>现在我们考虑如何用Option的map方法实现length: xxx的输出形式：</p>\n<p>先算出 Option 容器内字符串的长度<br>然后在长度前面加上 “length: “ 字样<br>最后把容器走访一次，印出容器内的东西<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; value1.map(_.length).map(<span class=\"string\">\"length: \"</span> + _).foreach(println)</div><div class=\"line\">length: <span class=\"number\">6</span></div><div class=\"line\"></div><div class=\"line\">scala&gt; value1.map(<span class=\"string\">\"length: \"</span> + _.length).foreach(println)</div><div class=\"line\">length: <span class=\"number\">6</span></div></pre></td></tr></table></figure></p>\n","excerpt":"<p>null空指针异常的根源</p>\n<p>大多数语言都有一个特殊的关键字或者对象来表示一个对象引用的是“无”大部分都是NULL或者null，在写c/c++中是NULL，NULL是个常量与0等价，在Java，它是null。在Java 里，null 是一个关键字，不管是字符串常量还是关键字他们不是一个对象，所以对它调用任何方法都是非法的。但是对程序的设计者来说这似乎又是必须的，如过我们的程序不返回任何内容那就是空，但是空也是引发问题最多的原因，c/c++的好多bug与NULL有关，java的null可以引起指针异常导致程序崩溃。scala设计了option优雅的解决了这个问题。<br>","more":"</p>\n<p>Scala的Option类型</p>\n<p>为了让所有东西都是对象的目标更加一致，也为了遵循函数式编程的习惯，Scala鼓励你在变量和函数返回值可能不会引用任何值的时候使用Option类型。在没有值的时候，使用None，这是Option的一个子类。如果有值可以引用，就使用Some来包含这个值。Some也是Option的子类。<br>None被声明为一个对象，而不是一个类，因为我们只需要它的一个实例。这样，它多少有点像null关键字，但它却是一个实实在在的，有方法的对象。如此以来我们即使对None对象进行了操作也不过是抛出异常直接导致崩溃的概率底了好的。</p>\n<p>比如scala的sdk中就有大量的应用<br>Option类型的值通常作为Scala集合类型（List,Map等）操作的返回类型。比如Map的get方法：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> capitals = <span class=\"type\">Map</span>(<span class=\"string\">\"France\"</span>-&gt;<span class=\"string\">\"Paris\"</span>, <span class=\"string\">\"Japan\"</span>-&gt;<span class=\"string\">\"Tokyo\"</span>, <span class=\"string\">\"China\"</span>-&gt;<span class=\"string\">\"Beijing\"</span>)</div><div class=\"line\">capitals: scala.collection.immutable.<span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">String</span>] = <span class=\"type\">Map</span>(<span class=\"type\">France</span> -&gt; <span class=\"type\">Paris</span>, <span class=\"type\">Japan</span> -&gt; <span class=\"type\">Tokyo</span>, <span class=\"type\">China</span> -&gt; <span class=\"type\">Beijing</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; capitals get <span class=\"string\">\"France\"</span></div><div class=\"line\">res0: <span class=\"type\">Option</span>[<span class=\"type\">String</span>] = <span class=\"type\">Some</span>(<span class=\"type\">Paris</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; capitals get <span class=\"string\">\"North Pole\"</span></div><div class=\"line\">res1: <span class=\"type\">Option</span>[<span class=\"type\">String</span>] = <span class=\"type\">None</span></div></pre></td></tr></table></figure></p>\n<p>Option有两个子类别，Some和None。当程序回传Some的时候，代表这个函式成功地给了你一个String，而你可以透过get()函数拿到那个String，如果程序返回的是None，则代表没有字符串可以给你。<br>在返回None，也就是没有String给你的时候，如果你还硬要调用get()来取得 String 的话，Scala一样是会抛出一个NoSuchElementException异常给你的。<br>我们也可以选用另外一个方法，getOrElse。这个方法在这个Option是Some的实例时返回对应的值，而在是None的实例时返回传入的参数。换句话说，传入getOrElse的参数实际上是默认返回值。<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; capitals get <span class=\"string\">\"North Pole\"</span> get</div><div class=\"line\">warning: there was one feature warning; re-run <span class=\"keyword\">with</span> -feature <span class=\"keyword\">for</span> details</div><div class=\"line\">java.util.<span class=\"type\">NoSuchElementException</span>: <span class=\"type\">None</span>.get</div><div class=\"line\">  at scala.<span class=\"type\">None</span>$.get(<span class=\"type\">Option</span>.scala:<span class=\"number\">347</span>)</div><div class=\"line\">  at scala.<span class=\"type\">None</span>$.get(<span class=\"type\">Option</span>.scala:<span class=\"number\">345</span>)</div><div class=\"line\">  ... <span class=\"number\">33</span> elided</div><div class=\"line\"></div><div class=\"line\">scala&gt; capitals get <span class=\"string\">\"France\"</span> get</div><div class=\"line\">warning: there was one feature warning; re-run <span class=\"keyword\">with</span> -feature <span class=\"keyword\">for</span> details</div><div class=\"line\">res3: <span class=\"type\">String</span> = <span class=\"type\">Paris</span></div><div class=\"line\"></div><div class=\"line\">scala&gt; (capitals get <span class=\"string\">\"North Pole\"</span>) getOrElse <span class=\"string\">\"Oops\"</span></div><div class=\"line\">res7: <span class=\"type\">String</span> = <span class=\"type\">Oops</span></div><div class=\"line\"></div><div class=\"line\">scala&gt; capitals get <span class=\"string\">\"France\"</span> getOrElse <span class=\"string\">\"Oops\"</span></div><div class=\"line\">res8: <span class=\"type\">String</span> = <span class=\"type\">Paris</span></div></pre></td></tr></table></figure></p>\n<p>通过模式匹配分离可选值，如果匹配的值是Some的话，将Some里的值抽出赋给x变量：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">showCapital</span></span>(x: <span class=\"type\">Option</span>[<span class=\"type\">String</span>]) = x <span class=\"keyword\">match</span> &#123;</div><div class=\"line\">    <span class=\"keyword\">case</span> <span class=\"type\">Some</span>(s) =&gt; s</div><div class=\"line\">    <span class=\"keyword\">case</span> <span class=\"type\">None</span> =&gt; <span class=\"string\">\"?\"</span></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>Scala程序使用Option非常频繁，在Java中使用null来表示空值，因此Java程序需要关心那些变量可能是null,尽管而这些变量出现null的可能性很低，但一但出现，很难查出为什么出现NullPointerException。<br>Scala的Option类型可以避免这种情况，因此Scala应用推荐使用Option类型来代表一些可选值。使用Option类型，读者一眼就可以看出这种类型的值可能为None。</p>\n<p>实际上，多亏Scala的静态类型，你并不能错误地尝试在一个可能为null的值上调用方法。虽然在Java中这是个很容易犯的错误，它在Scala却通不过编译，这是因为Java中没有检查变量是否为null的编程作为变成Scala中的类型错误（不能将Option[String]当做String来使用）。所以，Option的使用极强地鼓励了更加弹性的编程习惯。</p>\n<p>详解Option[T]</p>\n<p>在Scala里Option[T]实际上是一个容器，就像数组或是List一样，你可以把他看成是一个可能有零到一个元素的List。<br>当你的Option里面有东西的时候，这个List的长度是1（也就是 Some），而当你的Option里没有东西的时候，它的长度是0（也就是 None）。</p>\n<p>for循环</p>\n<p>如果我们把Option当成一般的List来用，并且用一个for循环来走访这个Option的时候，如果Option是None，那这个for循环里的程序代码自然不会执行，于是我们就达到了「不用检查Option是否为None这件事。<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> map1 = <span class=\"type\">Map</span>(<span class=\"string\">\"key1\"</span> -&gt; <span class=\"string\">\"value1\"</span>)</div><div class=\"line\">map1: scala.collection.immutable.<span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">String</span>] = <span class=\"type\">Map</span>(key1 -&gt; value1)</div><div class=\"line\"></div><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> value1 = map1.get(<span class=\"string\">\"key1\"</span>)</div><div class=\"line\">value1: <span class=\"type\">Option</span>[<span class=\"type\">String</span>] = <span class=\"type\">Some</span>(value1)</div><div class=\"line\"></div><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> value2 = map1.get(<span class=\"string\">\"key2\"</span>)</div><div class=\"line\">value2: <span class=\"type\">Option</span>[<span class=\"type\">String</span>] = <span class=\"type\">None</span></div><div class=\"line\"></div><div class=\"line\">scala&gt; <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">printContentLength</span></span>(x: <span class=\"type\">Option</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">     |   <span class=\"keyword\">for</span> (c &lt;- x)&#123;</div><div class=\"line\">     |     println(c.length)</div><div class=\"line\">     |   &#125;</div><div class=\"line\">     | &#125;</div><div class=\"line\">printContentLength: (x: <span class=\"type\">Option</span>[<span class=\"type\">String</span>])<span class=\"type\">Unit</span></div><div class=\"line\"></div><div class=\"line\">scala&gt; printContentLength(value1)</div><div class=\"line\"><span class=\"number\">6</span></div><div class=\"line\"></div><div class=\"line\">scala&gt; printContentLength(value2)</div></pre></td></tr></table></figure></p>\n<p>map操作</p>\n<p>在函数式编程中有一个核心的概念之一是转换，所以大部份支持函数式编程语言，都支持一种叫map()的动作，这个动作是可以帮你把某个容器的内容，套上一些动作之后，变成另一个新的容器。<br>现在我们考虑如何用Option的map方法实现length: xxx的输出形式：</p>\n<p>先算出 Option 容器内字符串的长度<br>然后在长度前面加上 “length: “ 字样<br>最后把容器走访一次，印出容器内的东西<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; value1.map(_.length).map(<span class=\"string\">\"length: \"</span> + _).foreach(println)</div><div class=\"line\">length: <span class=\"number\">6</span></div><div class=\"line\"></div><div class=\"line\">scala&gt; value1.map(<span class=\"string\">\"length: \"</span> + _.length).foreach(println)</div><div class=\"line\">length: <span class=\"number\">6</span></div></pre></td></tr></table></figure></p>"},{"title":"scala的fold和reduce","date":"2017-08-30T12:21:16.000Z","_content":"scala 基于java，但是与java不同scala是函数式的编程语言，scala与java之间的集合框架可以相互转换调用，但是scala的集合框架封装的更完善加上scala对函数式编程的支持，在scala中使用起集合来非常方便，最近用到了一些比较有意思的内容记录如下主要是关于reduce和fold：\n<!-- more -->\n对于scala的list关于fold函数的使用，如果list中是整数似乎Foldleft和Foldright都一样没啥区别，以及Fold函数本身都没啥区别\n```scala\n\ncala> val x = List(1,2,3,4)\nx: List[Int] = List(1, 2, 3, 4)\n\nscala> x.foldLeft(0)((x,y)=>x+y)\nres5: Int = 10\n\ncala> x.fold(0)((x,y)=>x+y)\nres6: Int = 10\n\nscala> x.foldRight(0)((x,y)=>x+y)\nres7: Int = 10\n\n```\n但是如果是字符串的话就很有意思了\n\n```scala\n\nscala> val x = List(\"this\",\"is\",\"an\",\"String\",\"Aarray\")\nx: List[String] = List(this, is, an, String, Aarray)\n\nscala> x.fold(\"where is * \")((x,y)=>x+y)\nres8: String = where is * thisisanStringAarray\n\nscala> x.fold(\"where is * \")((x,y)=>x+\",\"+y)\nres9: String = where is * ,this,is,an,String,Aarray\n\nscala> x.foldLeft(\"where is * \")((x,y)=>x+\",\"+y)\nres10: String = where is * ,this,is,an,String,Aarray\n\nscala> x.foldRight(\"where is * \")((x,y)=>x+\",\"+y)\nres11: String = \"this,is,an,String,Aarray,where is * \"\n\n```\n从上面可以看出来如果不指定Right或者left的话默认是left\n上面只是一些表面现象，本质是什么呢\nfold, foldLeft, and foldRight 　　主要的区别是fold函数操作遍历问题集合的顺序。foldLeft是从左开始计算，然后往右遍历。foldRight是从右开始算，然后往左遍历。而fold遍历的顺序没有特殊的次序。来看下这三个函数的实现吧（在TraversableOnce特质里面实现）\n```scala\ndef fold[A1 >: A](z: A1)(op: (A1, A1) => A1): A1 = foldLeft(z)(op)\n \ndef foldLeft[B](z: B)(op: (B, A) => B): B = {\n var result = z\n this.seq foreach (x => result = op(result, x))\n result\n }\n  \ndef foldRight[B](z: B)(op: (A, B) => B): B =\n    reversed.foldLeft(z)((x, y) => op(y, x))\n```\n由于fold函数遍历没有特殊的次序，所以对fold的初始化参数和返回值都有限制。在这三个函数中，初始化参数和返回值的参数类型必须相同。\n1. 初始值的类型必须是list中元素类型的超类。在我们的例子中，我们的对List[Int]进行fold计算，而初始值是Int类型的，它是List[Int]的超类。\n2. 初始值必须是中立的(neutral)。也就是它不能改变结果。比如对加法来说，中立的值是0；而对于乘法来说则是1，对于list来说则是Nil,但这也并不是绝对的，比如字符串我们可以在初始化的时候设置一个特殊的值，不一定是中立的空串。\n\n\n\n顺便说下，其实foldLeft和foldRight函数还有两个缩写的函数：\n\n\n```scala\ndef /:[B](z: B)(op: (B, A) => B): B = foldLeft(z)(op)\n\ndef :\\[B](z: B)(op: (A, B) => B): B = foldRight(z)(op)\n\nscala> (0/:(1 to 100))(_+_)  \nres32: Int = 5050\n\nscala> ((1 to 100):\\0)(_+_) \nres24: Int = 5050\n```\n\nreduce 与之最大的不同在于reduce不需要设置一个初始值，下面这段代码详细的展示了scala的reduce和fold的运行原理\n\n```scala\nval list = List(\"A\",\"B\",\"C\",\"D\",\"E\")\nprintln(\"reduce (a+b) \"+list.reduce((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (a+b)+\"  \")\na+b\n}))\n\nprintln(\"reduceLeft (a+b) \"+list.reduceLeft((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (a+b)+\"  \")\na+b\n}))\n\nprintln(\"reduceLeft (b+a) \"+list.reduceLeft((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (b+a)+\"  \" )\nb+a\n}))\n\nprintln(\"reduceRight (a+b) \"+list.reduceRight((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (a+b)+\"  \" )\na+b\n}))\n\nprintln(\"reduceRight (b+a) \"+list.reduceRight((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (b+a)+\"  \")\nb+a\n}))\n\nprintln(\"scan            \"+list.scan(\"[\")((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (a+b)+\"  \" )\na+b\n}))\nprintln(\"scanLeft (a+b)  \"+list.scanLeft(\"[\")((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (a+b)+\"  \" )\na+b\n}))\nprintln(\"scanLeft (b+a)  \"+list.scanLeft(\"[\")((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (b+a)+\"  \" )\nb+a\n}))\nprintln(\"scanRight (a+b) \"+list.scanRight(\"[\")((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (a+b)+\"  \" )\na+b\n}))\nprintln(\"scanRight (b+a) \"+list.scanRight(\"[\")((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (b+a)+\"  \" )\nb+a\n}))\nval list1 = List(-2,-1,0,1,2)\n\nprintln(\"reduce (a+b) \"+list1.reduce((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (a+b)+\"  \")\na+b\n}))\n\nprintln(\"reduceLeft (a+b) \"+list1.reduceLeft((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (a+b)+\"  \")\na+b\n}))\n\nprintln(\"reduceLeft (b+a) \"+list1.reduceLeft((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (b+a)+\"  \" )\nb+a\n}))\n\nprintln(\"      reduceRight (a+b) \"+list1.reduceRight((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (a+b)+\"  \" )\na+b\n}))\n\nprintln(\"      reduceRight (b+a) \"+list1.reduceRight((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (b+a)+\"  \")\nb+a\n}))\n\nprintln(\"scan            \"+list1.scan(0)((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (a+b)+\"  \" )\na+b\n}))\n\nprintln(\"scanLeft (a+b)  \"+list1.scanLeft(0)((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (a+b)+\"  \" )\na+b\n}))\n\nprintln(\"scanLeft (b+a)  \"+list1.scanLeft(0)((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (b+a)+\"  \" )\nb+a\n}))\n\nprintln(\"scanRight (a+b)         \"+list1.scanRight(0)((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (a+b)+\"  \" )\na+b}))\n\nprintln(\"scanRight (b+a)         \"+list1.scanRight(0)((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (a+b)+\"  \" )\nb+a}))\n```\n结果如下详细的显示了reduce 和fold的运行过程,一目了然\n\n\n```scala\n{A,B}=>AB  {AB,C}=>ABC  {ABC,D}=>ABCD  {ABCD,E}=>ABCDE  reduce (a+b) ABCDE\n{A,B}=>AB  {AB,C}=>ABC  {ABC,D}=>ABCD  {ABCD,E}=>ABCDE  reduceLeft (a+b) ABCDE\n{A,B}=>BA  {BA,C}=>CBA  {CBA,D}=>DCBA  {DCBA,E}=>EDCBA  reduceLeft (b+a) EDCBA\n{D,E}=>DE  {C,DE}=>CDE  {B,CDE}=>BCDE  {A,BCDE}=>ABCDE  reduceRight (a+b) ABCDE\n{D,E}=>ED  {C,ED}=>EDC  {B,EDC}=>EDCB  {A,EDCB}=>EDCBA  reduceRight (b+a) EDCBA\n{[,A}=>[A  {[A,B}=>[AB  {[AB,C}=>[ABC  {[ABC,D}=>[ABCD  {[ABCD,E}=>[ABCDE  scan            List([, [A, [AB, [ABC, [ABCD, [ABCDE)\n{[,A}=>[A  {[A,B}=>[AB  {[AB,C}=>[ABC  {[ABC,D}=>[ABCD  {[ABCD,E}=>[ABCDE  scanLeft (a+b)  List([, [A, [AB, [ABC, [ABCD, [ABCDE)\n{[,A}=>A[  {A[,B}=>BA[  {BA[,C}=>CBA[  {CBA[,D}=>DCBA[  {DCBA[,E}=>EDCBA[  scanLeft (b+a)  List([, A[, BA[, CBA[, DCBA[, EDCBA[)\n{E,[}=>E[  {D,E[}=>DE[  {C,DE[}=>CDE[  {B,CDE[}=>BCDE[  {A,BCDE[}=>ABCDE[  scanRight (a+b) List(ABCDE[, BCDE[, CDE[, DE[, E[, [)\n{E,[}=>[E  {D,[E}=>[ED  {C,[ED}=>[EDC  {B,[EDC}=>[EDCB  {A,[EDCB}=>[EDCBA  scanRight (b+a) List([EDCBA, [EDCB, [EDC, [ED, [E, [)\n{-2,-1}=>-3  {-3,0}=>-3  {-3,1}=>-2  {-2,2}=>0  reduce (a+b) 0\n{-2,-1}=>-3  {-3,0}=>-3  {-3,1}=>-2  {-2,2}=>0  reduceLeft (a+b) 0\n{-2,-1}=>-3  {-3,0}=>-3  {-3,1}=>-2  {-2,2}=>0  reduceLeft (b+a) 0\n{1,2}=>3  {0,3}=>3  {-1,3}=>2  {-2,2}=>0        reduceRight (a+b) 0\n{1,2}=>3  {0,3}=>3  {-1,3}=>2  {-2,2}=>0        reduceRight (b+a) 0\n{0,-2}=>-2  {-2,-1}=>-3  {-3,0}=>-3  {-3,1}=>-2  {-2,2}=>0  scan            List(0, -2, -3, -3, -2, 0)\n{0,-2}=>-2  {-2,-1}=>-3  {-3,0}=>-3  {-3,1}=>-2  {-2,2}=>0  scanLeft (a+b)  List(0, -2, -3, -3, -2, 0)\n{0,-2}=>-2  {-2,-1}=>-3  {-3,0}=>-3  {-3,1}=>-2  {-2,2}=>0  scanLeft (b+a)  List(0, -2, -3, -3, -2, 0)\n{2,0}=>2  {1,2}=>3  {0,3}=>3  {-1,3}=>2  {-2,2}=>0  scanRight (a+b)         List(0, 2, 3, 3, 2, 0)\n{2,0}=>2  {1,2}=>3  {0,3}=>3  {-1,3}=>2  {-2,2}=>0  scanRight (b+a)         List(0, 2, 3, 3, 2, 0)\n\n```\n","source":"_posts/scalaa.md","raw":"---\ntitle: scala的fold和reduce\ndate: 2017-08-30 20:21:16\ntags: scala\n---\nscala 基于java，但是与java不同scala是函数式的编程语言，scala与java之间的集合框架可以相互转换调用，但是scala的集合框架封装的更完善加上scala对函数式编程的支持，在scala中使用起集合来非常方便，最近用到了一些比较有意思的内容记录如下主要是关于reduce和fold：\n<!-- more -->\n对于scala的list关于fold函数的使用，如果list中是整数似乎Foldleft和Foldright都一样没啥区别，以及Fold函数本身都没啥区别\n```scala\n\ncala> val x = List(1,2,3,4)\nx: List[Int] = List(1, 2, 3, 4)\n\nscala> x.foldLeft(0)((x,y)=>x+y)\nres5: Int = 10\n\ncala> x.fold(0)((x,y)=>x+y)\nres6: Int = 10\n\nscala> x.foldRight(0)((x,y)=>x+y)\nres7: Int = 10\n\n```\n但是如果是字符串的话就很有意思了\n\n```scala\n\nscala> val x = List(\"this\",\"is\",\"an\",\"String\",\"Aarray\")\nx: List[String] = List(this, is, an, String, Aarray)\n\nscala> x.fold(\"where is * \")((x,y)=>x+y)\nres8: String = where is * thisisanStringAarray\n\nscala> x.fold(\"where is * \")((x,y)=>x+\",\"+y)\nres9: String = where is * ,this,is,an,String,Aarray\n\nscala> x.foldLeft(\"where is * \")((x,y)=>x+\",\"+y)\nres10: String = where is * ,this,is,an,String,Aarray\n\nscala> x.foldRight(\"where is * \")((x,y)=>x+\",\"+y)\nres11: String = \"this,is,an,String,Aarray,where is * \"\n\n```\n从上面可以看出来如果不指定Right或者left的话默认是left\n上面只是一些表面现象，本质是什么呢\nfold, foldLeft, and foldRight 　　主要的区别是fold函数操作遍历问题集合的顺序。foldLeft是从左开始计算，然后往右遍历。foldRight是从右开始算，然后往左遍历。而fold遍历的顺序没有特殊的次序。来看下这三个函数的实现吧（在TraversableOnce特质里面实现）\n```scala\ndef fold[A1 >: A](z: A1)(op: (A1, A1) => A1): A1 = foldLeft(z)(op)\n \ndef foldLeft[B](z: B)(op: (B, A) => B): B = {\n var result = z\n this.seq foreach (x => result = op(result, x))\n result\n }\n  \ndef foldRight[B](z: B)(op: (A, B) => B): B =\n    reversed.foldLeft(z)((x, y) => op(y, x))\n```\n由于fold函数遍历没有特殊的次序，所以对fold的初始化参数和返回值都有限制。在这三个函数中，初始化参数和返回值的参数类型必须相同。\n1. 初始值的类型必须是list中元素类型的超类。在我们的例子中，我们的对List[Int]进行fold计算，而初始值是Int类型的，它是List[Int]的超类。\n2. 初始值必须是中立的(neutral)。也就是它不能改变结果。比如对加法来说，中立的值是0；而对于乘法来说则是1，对于list来说则是Nil,但这也并不是绝对的，比如字符串我们可以在初始化的时候设置一个特殊的值，不一定是中立的空串。\n\n\n\n顺便说下，其实foldLeft和foldRight函数还有两个缩写的函数：\n\n\n```scala\ndef /:[B](z: B)(op: (B, A) => B): B = foldLeft(z)(op)\n\ndef :\\[B](z: B)(op: (A, B) => B): B = foldRight(z)(op)\n\nscala> (0/:(1 to 100))(_+_)  \nres32: Int = 5050\n\nscala> ((1 to 100):\\0)(_+_) \nres24: Int = 5050\n```\n\nreduce 与之最大的不同在于reduce不需要设置一个初始值，下面这段代码详细的展示了scala的reduce和fold的运行原理\n\n```scala\nval list = List(\"A\",\"B\",\"C\",\"D\",\"E\")\nprintln(\"reduce (a+b) \"+list.reduce((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (a+b)+\"  \")\na+b\n}))\n\nprintln(\"reduceLeft (a+b) \"+list.reduceLeft((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (a+b)+\"  \")\na+b\n}))\n\nprintln(\"reduceLeft (b+a) \"+list.reduceLeft((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (b+a)+\"  \" )\nb+a\n}))\n\nprintln(\"reduceRight (a+b) \"+list.reduceRight((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (a+b)+\"  \" )\na+b\n}))\n\nprintln(\"reduceRight (b+a) \"+list.reduceRight((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (b+a)+\"  \")\nb+a\n}))\n\nprintln(\"scan            \"+list.scan(\"[\")((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (a+b)+\"  \" )\na+b\n}))\nprintln(\"scanLeft (a+b)  \"+list.scanLeft(\"[\")((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (a+b)+\"  \" )\na+b\n}))\nprintln(\"scanLeft (b+a)  \"+list.scanLeft(\"[\")((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (b+a)+\"  \" )\nb+a\n}))\nprintln(\"scanRight (a+b) \"+list.scanRight(\"[\")((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (a+b)+\"  \" )\na+b\n}))\nprintln(\"scanRight (b+a) \"+list.scanRight(\"[\")((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (b+a)+\"  \" )\nb+a\n}))\nval list1 = List(-2,-1,0,1,2)\n\nprintln(\"reduce (a+b) \"+list1.reduce((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (a+b)+\"  \")\na+b\n}))\n\nprintln(\"reduceLeft (a+b) \"+list1.reduceLeft((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (a+b)+\"  \")\na+b\n}))\n\nprintln(\"reduceLeft (b+a) \"+list1.reduceLeft((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (b+a)+\"  \" )\nb+a\n}))\n\nprintln(\"      reduceRight (a+b) \"+list1.reduceRight((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (a+b)+\"  \" )\na+b\n}))\n\nprintln(\"      reduceRight (b+a) \"+list1.reduceRight((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (b+a)+\"  \")\nb+a\n}))\n\nprintln(\"scan            \"+list1.scan(0)((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (a+b)+\"  \" )\na+b\n}))\n\nprintln(\"scanLeft (a+b)  \"+list1.scanLeft(0)((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (a+b)+\"  \" )\na+b\n}))\n\nprintln(\"scanLeft (b+a)  \"+list1.scanLeft(0)((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (b+a)+\"  \" )\nb+a\n}))\n\nprintln(\"scanRight (a+b)         \"+list1.scanRight(0)((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (a+b)+\"  \" )\na+b}))\n\nprintln(\"scanRight (b+a)         \"+list1.scanRight(0)((a,b)=>{\nprint(\"{\"+a+\",\"+b+\"}=>\"+ (a+b)+\"  \" )\nb+a}))\n```\n结果如下详细的显示了reduce 和fold的运行过程,一目了然\n\n\n```scala\n{A,B}=>AB  {AB,C}=>ABC  {ABC,D}=>ABCD  {ABCD,E}=>ABCDE  reduce (a+b) ABCDE\n{A,B}=>AB  {AB,C}=>ABC  {ABC,D}=>ABCD  {ABCD,E}=>ABCDE  reduceLeft (a+b) ABCDE\n{A,B}=>BA  {BA,C}=>CBA  {CBA,D}=>DCBA  {DCBA,E}=>EDCBA  reduceLeft (b+a) EDCBA\n{D,E}=>DE  {C,DE}=>CDE  {B,CDE}=>BCDE  {A,BCDE}=>ABCDE  reduceRight (a+b) ABCDE\n{D,E}=>ED  {C,ED}=>EDC  {B,EDC}=>EDCB  {A,EDCB}=>EDCBA  reduceRight (b+a) EDCBA\n{[,A}=>[A  {[A,B}=>[AB  {[AB,C}=>[ABC  {[ABC,D}=>[ABCD  {[ABCD,E}=>[ABCDE  scan            List([, [A, [AB, [ABC, [ABCD, [ABCDE)\n{[,A}=>[A  {[A,B}=>[AB  {[AB,C}=>[ABC  {[ABC,D}=>[ABCD  {[ABCD,E}=>[ABCDE  scanLeft (a+b)  List([, [A, [AB, [ABC, [ABCD, [ABCDE)\n{[,A}=>A[  {A[,B}=>BA[  {BA[,C}=>CBA[  {CBA[,D}=>DCBA[  {DCBA[,E}=>EDCBA[  scanLeft (b+a)  List([, A[, BA[, CBA[, DCBA[, EDCBA[)\n{E,[}=>E[  {D,E[}=>DE[  {C,DE[}=>CDE[  {B,CDE[}=>BCDE[  {A,BCDE[}=>ABCDE[  scanRight (a+b) List(ABCDE[, BCDE[, CDE[, DE[, E[, [)\n{E,[}=>[E  {D,[E}=>[ED  {C,[ED}=>[EDC  {B,[EDC}=>[EDCB  {A,[EDCB}=>[EDCBA  scanRight (b+a) List([EDCBA, [EDCB, [EDC, [ED, [E, [)\n{-2,-1}=>-3  {-3,0}=>-3  {-3,1}=>-2  {-2,2}=>0  reduce (a+b) 0\n{-2,-1}=>-3  {-3,0}=>-3  {-3,1}=>-2  {-2,2}=>0  reduceLeft (a+b) 0\n{-2,-1}=>-3  {-3,0}=>-3  {-3,1}=>-2  {-2,2}=>0  reduceLeft (b+a) 0\n{1,2}=>3  {0,3}=>3  {-1,3}=>2  {-2,2}=>0        reduceRight (a+b) 0\n{1,2}=>3  {0,3}=>3  {-1,3}=>2  {-2,2}=>0        reduceRight (b+a) 0\n{0,-2}=>-2  {-2,-1}=>-3  {-3,0}=>-3  {-3,1}=>-2  {-2,2}=>0  scan            List(0, -2, -3, -3, -2, 0)\n{0,-2}=>-2  {-2,-1}=>-3  {-3,0}=>-3  {-3,1}=>-2  {-2,2}=>0  scanLeft (a+b)  List(0, -2, -3, -3, -2, 0)\n{0,-2}=>-2  {-2,-1}=>-3  {-3,0}=>-3  {-3,1}=>-2  {-2,2}=>0  scanLeft (b+a)  List(0, -2, -3, -3, -2, 0)\n{2,0}=>2  {1,2}=>3  {0,3}=>3  {-1,3}=>2  {-2,2}=>0  scanRight (a+b)         List(0, 2, 3, 3, 2, 0)\n{2,0}=>2  {1,2}=>3  {0,3}=>3  {-1,3}=>2  {-2,2}=>0  scanRight (b+a)         List(0, 2, 3, 3, 2, 0)\n\n```\n","slug":"scalaa","published":1,"updated":"2017-08-30T14:14:45.138Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj70mt0qi000s60tuo9h0op03","content":"<p>scala 基于java，但是与java不同scala是函数式的编程语言，scala与java之间的集合框架可以相互转换调用，但是scala的集合框架封装的更完善加上scala对函数式编程的支持，在scala中使用起集合来非常方便，最近用到了一些比较有意思的内容记录如下主要是关于reduce和fold：<br><a id=\"more\"></a><br>对于scala的list关于fold函数的使用，如果list中是整数似乎Foldleft和Foldright都一样没啥区别，以及Fold函数本身都没啥区别<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\">cala&gt; <span class=\"keyword\">val</span> x = <span class=\"type\">List</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>)</div><div class=\"line\">x: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; x.foldLeft(<span class=\"number\">0</span>)((x,y)=&gt;x+y)</div><div class=\"line\">res5: <span class=\"type\">Int</span> = <span class=\"number\">10</span></div><div class=\"line\"></div><div class=\"line\">cala&gt; x.fold(<span class=\"number\">0</span>)((x,y)=&gt;x+y)</div><div class=\"line\">res6: <span class=\"type\">Int</span> = <span class=\"number\">10</span></div><div class=\"line\"></div><div class=\"line\">scala&gt; x.foldRight(<span class=\"number\">0</span>)((x,y)=&gt;x+y)</div><div class=\"line\">res7: <span class=\"type\">Int</span> = <span class=\"number\">10</span></div></pre></td></tr></table></figure></p>\n<p>但是如果是字符串的话就很有意思了</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> x = <span class=\"type\">List</span>(<span class=\"string\">\"this\"</span>,<span class=\"string\">\"is\"</span>,<span class=\"string\">\"an\"</span>,<span class=\"string\">\"String\"</span>,<span class=\"string\">\"Aarray\"</span>)</div><div class=\"line\">x: <span class=\"type\">List</span>[<span class=\"type\">String</span>] = <span class=\"type\">List</span>(<span class=\"keyword\">this</span>, is, an, <span class=\"type\">String</span>, <span class=\"type\">Aarray</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; x.fold(<span class=\"string\">\"where is * \"</span>)((x,y)=&gt;x+y)</div><div class=\"line\">res8: <span class=\"type\">String</span> = where is * thisisanStringAarray</div><div class=\"line\"></div><div class=\"line\">scala&gt; x.fold(<span class=\"string\">\"where is * \"</span>)((x,y)=&gt;x+<span class=\"string\">\",\"</span>+y)</div><div class=\"line\">res9: <span class=\"type\">String</span> = where is * ,<span class=\"keyword\">this</span>,is,an,<span class=\"type\">String</span>,<span class=\"type\">Aarray</span></div><div class=\"line\"></div><div class=\"line\">scala&gt; x.foldLeft(<span class=\"string\">\"where is * \"</span>)((x,y)=&gt;x+<span class=\"string\">\",\"</span>+y)</div><div class=\"line\">res10: <span class=\"type\">String</span> = where is * ,<span class=\"keyword\">this</span>,is,an,<span class=\"type\">String</span>,<span class=\"type\">Aarray</span></div><div class=\"line\"></div><div class=\"line\">scala&gt; x.foldRight(<span class=\"string\">\"where is * \"</span>)((x,y)=&gt;x+<span class=\"string\">\",\"</span>+y)</div><div class=\"line\">res11: <span class=\"type\">String</span> = <span class=\"string\">\"this,is,an,String,Aarray,where is * \"</span></div></pre></td></tr></table></figure>\n<p>从上面可以看出来如果不指定Right或者left的话默认是left<br>上面只是一些表面现象，本质是什么呢<br>fold, foldLeft, and foldRight 　　主要的区别是fold函数操作遍历问题集合的顺序。foldLeft是从左开始计算，然后往右遍历。foldRight是从右开始算，然后往左遍历。而fold遍历的顺序没有特殊的次序。来看下这三个函数的实现吧（在TraversableOnce特质里面实现）<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fold</span></span>[<span class=\"type\">A1</span> &gt;: <span class=\"type\">A</span>](z: <span class=\"type\">A1</span>)(op: (<span class=\"type\">A1</span>, <span class=\"type\">A1</span>) =&gt; <span class=\"type\">A1</span>): <span class=\"type\">A1</span> = foldLeft(z)(op)</div><div class=\"line\"> </div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">foldLeft</span></span>[<span class=\"type\">B</span>](z: <span class=\"type\">B</span>)(op: (<span class=\"type\">B</span>, <span class=\"type\">A</span>) =&gt; <span class=\"type\">B</span>): <span class=\"type\">B</span> = &#123;</div><div class=\"line\"> <span class=\"keyword\">var</span> result = z</div><div class=\"line\"> <span class=\"keyword\">this</span>.seq foreach (x =&gt; result = op(result, x))</div><div class=\"line\"> result</div><div class=\"line\"> &#125;</div><div class=\"line\">  </div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">foldRight</span></span>[<span class=\"type\">B</span>](z: <span class=\"type\">B</span>)(op: (<span class=\"type\">A</span>, <span class=\"type\">B</span>) =&gt; <span class=\"type\">B</span>): <span class=\"type\">B</span> =</div><div class=\"line\">    reversed.foldLeft(z)((x, y) =&gt; op(y, x))</div></pre></td></tr></table></figure></p>\n<p>由于fold函数遍历没有特殊的次序，所以对fold的初始化参数和返回值都有限制。在这三个函数中，初始化参数和返回值的参数类型必须相同。</p>\n<ol>\n<li>初始值的类型必须是list中元素类型的超类。在我们的例子中，我们的对List[Int]进行fold计算，而初始值是Int类型的，它是List[Int]的超类。</li>\n<li>初始值必须是中立的(neutral)。也就是它不能改变结果。比如对加法来说，中立的值是0；而对于乘法来说则是1，对于list来说则是Nil,但这也并不是绝对的，比如字符串我们可以在初始化的时候设置一个特殊的值，不一定是中立的空串。</li>\n</ol>\n<p>顺便说下，其实foldLeft和foldRight函数还有两个缩写的函数：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">/</span></span>:[<span class=\"type\">B</span>](z: <span class=\"type\">B</span>)(op: (<span class=\"type\">B</span>, <span class=\"type\">A</span>) =&gt; <span class=\"type\">B</span>): <span class=\"type\">B</span> = foldLeft(z)(op)</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> </span>:\\[<span class=\"type\">B</span>](z: <span class=\"type\">B</span>)(op: (<span class=\"type\">A</span>, <span class=\"type\">B</span>) =&gt; <span class=\"type\">B</span>): <span class=\"type\">B</span> = foldRight(z)(op)</div><div class=\"line\"></div><div class=\"line\">scala&gt; (<span class=\"number\">0</span>/:(<span class=\"number\">1</span> to <span class=\"number\">100</span>))(_+_)  </div><div class=\"line\">res32: <span class=\"type\">Int</span> = <span class=\"number\">5050</span></div><div class=\"line\"></div><div class=\"line\">scala&gt; ((<span class=\"number\">1</span> to <span class=\"number\">100</span>):\\<span class=\"number\">0</span>)(_+_) </div><div class=\"line\">res24: <span class=\"type\">Int</span> = <span class=\"number\">5050</span></div></pre></td></tr></table></figure>\n<p>reduce 与之最大的不同在于reduce不需要设置一个初始值，下面这段代码详细的展示了scala的reduce和fold的运行原理</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> list = <span class=\"type\">List</span>(<span class=\"string\">\"A\"</span>,<span class=\"string\">\"B\"</span>,<span class=\"string\">\"C\"</span>,<span class=\"string\">\"D\"</span>,<span class=\"string\">\"E\"</span>)</div><div class=\"line\">println(<span class=\"string\">\"reduce (a+b) \"</span>+list.reduce((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (a+b)+<span class=\"string\">\"  \"</span>)</div><div class=\"line\">a+b</div><div class=\"line\">&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"reduceLeft (a+b) \"</span>+list.reduceLeft((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (a+b)+<span class=\"string\">\"  \"</span>)</div><div class=\"line\">a+b</div><div class=\"line\">&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"reduceLeft (b+a) \"</span>+list.reduceLeft((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (b+a)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">b+a</div><div class=\"line\">&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"reduceRight (a+b) \"</span>+list.reduceRight((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (a+b)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">a+b</div><div class=\"line\">&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"reduceRight (b+a) \"</span>+list.reduceRight((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (b+a)+<span class=\"string\">\"  \"</span>)</div><div class=\"line\">b+a</div><div class=\"line\">&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"scan            \"</span>+list.scan(<span class=\"string\">\"[\"</span>)((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (a+b)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">a+b</div><div class=\"line\">&#125;))</div><div class=\"line\">println(<span class=\"string\">\"scanLeft (a+b)  \"</span>+list.scanLeft(<span class=\"string\">\"[\"</span>)((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (a+b)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">a+b</div><div class=\"line\">&#125;))</div><div class=\"line\">println(<span class=\"string\">\"scanLeft (b+a)  \"</span>+list.scanLeft(<span class=\"string\">\"[\"</span>)((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (b+a)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">b+a</div><div class=\"line\">&#125;))</div><div class=\"line\">println(<span class=\"string\">\"scanRight (a+b) \"</span>+list.scanRight(<span class=\"string\">\"[\"</span>)((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (a+b)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">a+b</div><div class=\"line\">&#125;))</div><div class=\"line\">println(<span class=\"string\">\"scanRight (b+a) \"</span>+list.scanRight(<span class=\"string\">\"[\"</span>)((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (b+a)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">b+a</div><div class=\"line\">&#125;))</div><div class=\"line\"><span class=\"keyword\">val</span> list1 = <span class=\"type\">List</span>(<span class=\"number\">-2</span>,<span class=\"number\">-1</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>)</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"reduce (a+b) \"</span>+list1.reduce((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (a+b)+<span class=\"string\">\"  \"</span>)</div><div class=\"line\">a+b</div><div class=\"line\">&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"reduceLeft (a+b) \"</span>+list1.reduceLeft((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (a+b)+<span class=\"string\">\"  \"</span>)</div><div class=\"line\">a+b</div><div class=\"line\">&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"reduceLeft (b+a) \"</span>+list1.reduceLeft((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (b+a)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">b+a</div><div class=\"line\">&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"      reduceRight (a+b) \"</span>+list1.reduceRight((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (a+b)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">a+b</div><div class=\"line\">&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"      reduceRight (b+a) \"</span>+list1.reduceRight((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (b+a)+<span class=\"string\">\"  \"</span>)</div><div class=\"line\">b+a</div><div class=\"line\">&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"scan            \"</span>+list1.scan(<span class=\"number\">0</span>)((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (a+b)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">a+b</div><div class=\"line\">&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"scanLeft (a+b)  \"</span>+list1.scanLeft(<span class=\"number\">0</span>)((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (a+b)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">a+b</div><div class=\"line\">&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"scanLeft (b+a)  \"</span>+list1.scanLeft(<span class=\"number\">0</span>)((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (b+a)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">b+a</div><div class=\"line\">&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"scanRight (a+b)         \"</span>+list1.scanRight(<span class=\"number\">0</span>)((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (a+b)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">a+b&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"scanRight (b+a)         \"</span>+list1.scanRight(<span class=\"number\">0</span>)((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (a+b)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">b+a&#125;))</div></pre></td></tr></table></figure>\n<p>结果如下详细的显示了reduce 和fold的运行过程,一目了然</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;<span class=\"type\">A</span>,<span class=\"type\">B</span>&#125;=&gt;<span class=\"type\">AB</span>  &#123;<span class=\"type\">AB</span>,<span class=\"type\">C</span>&#125;=&gt;<span class=\"type\">ABC</span>  &#123;<span class=\"type\">ABC</span>,<span class=\"type\">D</span>&#125;=&gt;<span class=\"type\">ABCD</span>  &#123;<span class=\"type\">ABCD</span>,<span class=\"type\">E</span>&#125;=&gt;<span class=\"type\">ABCDE</span>  reduce (a+b) <span class=\"type\">ABCDE</span></div><div class=\"line\">&#123;<span class=\"type\">A</span>,<span class=\"type\">B</span>&#125;=&gt;<span class=\"type\">AB</span>  &#123;<span class=\"type\">AB</span>,<span class=\"type\">C</span>&#125;=&gt;<span class=\"type\">ABC</span>  &#123;<span class=\"type\">ABC</span>,<span class=\"type\">D</span>&#125;=&gt;<span class=\"type\">ABCD</span>  &#123;<span class=\"type\">ABCD</span>,<span class=\"type\">E</span>&#125;=&gt;<span class=\"type\">ABCDE</span>  reduceLeft (a+b) <span class=\"type\">ABCDE</span></div><div class=\"line\">&#123;<span class=\"type\">A</span>,<span class=\"type\">B</span>&#125;=&gt;<span class=\"type\">BA</span>  &#123;<span class=\"type\">BA</span>,<span class=\"type\">C</span>&#125;=&gt;<span class=\"type\">CBA</span>  &#123;<span class=\"type\">CBA</span>,<span class=\"type\">D</span>&#125;=&gt;<span class=\"type\">DCBA</span>  &#123;<span class=\"type\">DCBA</span>,<span class=\"type\">E</span>&#125;=&gt;<span class=\"type\">EDCBA</span>  reduceLeft (b+a) <span class=\"type\">EDCBA</span></div><div class=\"line\">&#123;<span class=\"type\">D</span>,<span class=\"type\">E</span>&#125;=&gt;<span class=\"type\">DE</span>  &#123;<span class=\"type\">C</span>,<span class=\"type\">DE</span>&#125;=&gt;<span class=\"type\">CDE</span>  &#123;<span class=\"type\">B</span>,<span class=\"type\">CDE</span>&#125;=&gt;<span class=\"type\">BCDE</span>  &#123;<span class=\"type\">A</span>,<span class=\"type\">BCDE</span>&#125;=&gt;<span class=\"type\">ABCDE</span>  reduceRight (a+b) <span class=\"type\">ABCDE</span></div><div class=\"line\">&#123;<span class=\"type\">D</span>,<span class=\"type\">E</span>&#125;=&gt;<span class=\"type\">ED</span>  &#123;<span class=\"type\">C</span>,<span class=\"type\">ED</span>&#125;=&gt;<span class=\"type\">EDC</span>  &#123;<span class=\"type\">B</span>,<span class=\"type\">EDC</span>&#125;=&gt;<span class=\"type\">EDCB</span>  &#123;<span class=\"type\">A</span>,<span class=\"type\">EDCB</span>&#125;=&gt;<span class=\"type\">EDCBA</span>  reduceRight (b+a) <span class=\"type\">EDCBA</span></div><div class=\"line\">&#123;[,<span class=\"type\">A</span>&#125;=&gt;[<span class=\"type\">A</span>  &#123;[<span class=\"type\">A</span>,<span class=\"type\">B</span>&#125;=&gt;[<span class=\"type\">AB</span>  &#123;[<span class=\"type\">AB</span>,<span class=\"type\">C</span>&#125;=&gt;[<span class=\"type\">ABC</span>  &#123;[<span class=\"type\">ABC</span>,<span class=\"type\">D</span>&#125;=&gt;[<span class=\"type\">ABCD</span>  &#123;[<span class=\"type\">ABCD</span>,<span class=\"type\">E</span>&#125;=&gt;[<span class=\"type\">ABCDE</span>  scan            <span class=\"type\">List</span>([, [<span class=\"type\">A</span>, [<span class=\"type\">AB</span>, [<span class=\"type\">ABC</span>, [<span class=\"type\">ABCD</span>, [<span class=\"type\">ABCDE</span>)</div><div class=\"line\">&#123;[,<span class=\"type\">A</span>&#125;=&gt;[<span class=\"type\">A</span>  &#123;[<span class=\"type\">A</span>,<span class=\"type\">B</span>&#125;=&gt;[<span class=\"type\">AB</span>  &#123;[<span class=\"type\">AB</span>,<span class=\"type\">C</span>&#125;=&gt;[<span class=\"type\">ABC</span>  &#123;[<span class=\"type\">ABC</span>,<span class=\"type\">D</span>&#125;=&gt;[<span class=\"type\">ABCD</span>  &#123;[<span class=\"type\">ABCD</span>,<span class=\"type\">E</span>&#125;=&gt;[<span class=\"type\">ABCDE</span>  scanLeft (a+b)  <span class=\"type\">List</span>([, [<span class=\"type\">A</span>, [<span class=\"type\">AB</span>, [<span class=\"type\">ABC</span>, [<span class=\"type\">ABCD</span>, [<span class=\"type\">ABCDE</span>)</div><div class=\"line\">&#123;[,<span class=\"type\">A</span>&#125;=&gt;<span class=\"type\">A</span>[  &#123;<span class=\"type\">A</span>[,<span class=\"type\">B</span>&#125;=&gt;<span class=\"type\">BA</span>[  &#123;<span class=\"type\">BA</span>[,<span class=\"type\">C</span>&#125;=&gt;<span class=\"type\">CBA</span>[  &#123;<span class=\"type\">CBA</span>[,<span class=\"type\">D</span>&#125;=&gt;<span class=\"type\">DCBA</span>[  &#123;<span class=\"type\">DCBA</span>[,<span class=\"type\">E</span>&#125;=&gt;<span class=\"type\">EDCBA</span>[  scanLeft (b+a)  <span class=\"type\">List</span>([, <span class=\"type\">A</span>[, <span class=\"type\">BA</span>[, <span class=\"type\">CBA</span>[, <span class=\"type\">DCBA</span>[, <span class=\"type\">EDCBA</span>[)</div><div class=\"line\">&#123;<span class=\"type\">E</span>,[&#125;=&gt;<span class=\"type\">E</span>[  &#123;<span class=\"type\">D</span>,<span class=\"type\">E</span>[&#125;=&gt;<span class=\"type\">DE</span>[  &#123;<span class=\"type\">C</span>,<span class=\"type\">DE</span>[&#125;=&gt;<span class=\"type\">CDE</span>[  &#123;<span class=\"type\">B</span>,<span class=\"type\">CDE</span>[&#125;=&gt;<span class=\"type\">BCDE</span>[  &#123;<span class=\"type\">A</span>,<span class=\"type\">BCDE</span>[&#125;=&gt;<span class=\"type\">ABCDE</span>[  scanRight (a+b) <span class=\"type\">List</span>(<span class=\"type\">ABCDE</span>[, <span class=\"type\">BCDE</span>[, <span class=\"type\">CDE</span>[, <span class=\"type\">DE</span>[, <span class=\"type\">E</span>[, [)</div><div class=\"line\">&#123;<span class=\"type\">E</span>,[&#125;=&gt;[<span class=\"type\">E</span>  &#123;<span class=\"type\">D</span>,[<span class=\"type\">E</span>&#125;=&gt;[<span class=\"type\">ED</span>  &#123;<span class=\"type\">C</span>,[<span class=\"type\">ED</span>&#125;=&gt;[<span class=\"type\">EDC</span>  &#123;<span class=\"type\">B</span>,[<span class=\"type\">EDC</span>&#125;=&gt;[<span class=\"type\">EDCB</span>  &#123;<span class=\"type\">A</span>,[<span class=\"type\">EDCB</span>&#125;=&gt;[<span class=\"type\">EDCBA</span>  scanRight (b+a) <span class=\"type\">List</span>([<span class=\"type\">EDCBA</span>, [<span class=\"type\">EDCB</span>, [<span class=\"type\">EDC</span>, [<span class=\"type\">ED</span>, [<span class=\"type\">E</span>, [)</div><div class=\"line\">&#123;<span class=\"number\">-2</span>,<span class=\"number\">-1</span>&#125;=&gt;<span class=\"number\">-3</span>  &#123;<span class=\"number\">-3</span>,<span class=\"number\">0</span>&#125;=&gt;<span class=\"number\">-3</span>  &#123;<span class=\"number\">-3</span>,<span class=\"number\">1</span>&#125;=&gt;<span class=\"number\">-2</span>  &#123;<span class=\"number\">-2</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">0</span>  reduce (a+b) <span class=\"number\">0</span></div><div class=\"line\">&#123;<span class=\"number\">-2</span>,<span class=\"number\">-1</span>&#125;=&gt;<span class=\"number\">-3</span>  &#123;<span class=\"number\">-3</span>,<span class=\"number\">0</span>&#125;=&gt;<span class=\"number\">-3</span>  &#123;<span class=\"number\">-3</span>,<span class=\"number\">1</span>&#125;=&gt;<span class=\"number\">-2</span>  &#123;<span class=\"number\">-2</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">0</span>  reduceLeft (a+b) <span class=\"number\">0</span></div><div class=\"line\">&#123;<span class=\"number\">-2</span>,<span class=\"number\">-1</span>&#125;=&gt;<span class=\"number\">-3</span>  &#123;<span class=\"number\">-3</span>,<span class=\"number\">0</span>&#125;=&gt;<span class=\"number\">-3</span>  &#123;<span class=\"number\">-3</span>,<span class=\"number\">1</span>&#125;=&gt;<span class=\"number\">-2</span>  &#123;<span class=\"number\">-2</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">0</span>  reduceLeft (b+a) <span class=\"number\">0</span></div><div class=\"line\">&#123;<span class=\"number\">1</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">3</span>  &#123;<span class=\"number\">0</span>,<span class=\"number\">3</span>&#125;=&gt;<span class=\"number\">3</span>  &#123;<span class=\"number\">-1</span>,<span class=\"number\">3</span>&#125;=&gt;<span class=\"number\">2</span>  &#123;<span class=\"number\">-2</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">0</span>        reduceRight (a+b) <span class=\"number\">0</span></div><div class=\"line\">&#123;<span class=\"number\">1</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">3</span>  &#123;<span class=\"number\">0</span>,<span class=\"number\">3</span>&#125;=&gt;<span class=\"number\">3</span>  &#123;<span class=\"number\">-1</span>,<span class=\"number\">3</span>&#125;=&gt;<span class=\"number\">2</span>  &#123;<span class=\"number\">-2</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">0</span>        reduceRight (b+a) <span class=\"number\">0</span></div><div class=\"line\">&#123;<span class=\"number\">0</span>,<span class=\"number\">-2</span>&#125;=&gt;<span class=\"number\">-2</span>  &#123;<span class=\"number\">-2</span>,<span class=\"number\">-1</span>&#125;=&gt;<span class=\"number\">-3</span>  &#123;<span class=\"number\">-3</span>,<span class=\"number\">0</span>&#125;=&gt;<span class=\"number\">-3</span>  &#123;<span class=\"number\">-3</span>,<span class=\"number\">1</span>&#125;=&gt;<span class=\"number\">-2</span>  &#123;<span class=\"number\">-2</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">0</span>  scan            <span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">-2</span>, <span class=\"number\">-3</span>, <span class=\"number\">-3</span>, <span class=\"number\">-2</span>, <span class=\"number\">0</span>)</div><div class=\"line\">&#123;<span class=\"number\">0</span>,<span class=\"number\">-2</span>&#125;=&gt;<span class=\"number\">-2</span>  &#123;<span class=\"number\">-2</span>,<span class=\"number\">-1</span>&#125;=&gt;<span class=\"number\">-3</span>  &#123;<span class=\"number\">-3</span>,<span class=\"number\">0</span>&#125;=&gt;<span class=\"number\">-3</span>  &#123;<span class=\"number\">-3</span>,<span class=\"number\">1</span>&#125;=&gt;<span class=\"number\">-2</span>  &#123;<span class=\"number\">-2</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">0</span>  scanLeft (a+b)  <span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">-2</span>, <span class=\"number\">-3</span>, <span class=\"number\">-3</span>, <span class=\"number\">-2</span>, <span class=\"number\">0</span>)</div><div class=\"line\">&#123;<span class=\"number\">0</span>,<span class=\"number\">-2</span>&#125;=&gt;<span class=\"number\">-2</span>  &#123;<span class=\"number\">-2</span>,<span class=\"number\">-1</span>&#125;=&gt;<span class=\"number\">-3</span>  &#123;<span class=\"number\">-3</span>,<span class=\"number\">0</span>&#125;=&gt;<span class=\"number\">-3</span>  &#123;<span class=\"number\">-3</span>,<span class=\"number\">1</span>&#125;=&gt;<span class=\"number\">-2</span>  &#123;<span class=\"number\">-2</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">0</span>  scanLeft (b+a)  <span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">-2</span>, <span class=\"number\">-3</span>, <span class=\"number\">-3</span>, <span class=\"number\">-2</span>, <span class=\"number\">0</span>)</div><div class=\"line\">&#123;<span class=\"number\">2</span>,<span class=\"number\">0</span>&#125;=&gt;<span class=\"number\">2</span>  &#123;<span class=\"number\">1</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">3</span>  &#123;<span class=\"number\">0</span>,<span class=\"number\">3</span>&#125;=&gt;<span class=\"number\">3</span>  &#123;<span class=\"number\">-1</span>,<span class=\"number\">3</span>&#125;=&gt;<span class=\"number\">2</span>  &#123;<span class=\"number\">-2</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">0</span>  scanRight (a+b)         <span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>, <span class=\"number\">0</span>)</div><div class=\"line\">&#123;<span class=\"number\">2</span>,<span class=\"number\">0</span>&#125;=&gt;<span class=\"number\">2</span>  &#123;<span class=\"number\">1</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">3</span>  &#123;<span class=\"number\">0</span>,<span class=\"number\">3</span>&#125;=&gt;<span class=\"number\">3</span>  &#123;<span class=\"number\">-1</span>,<span class=\"number\">3</span>&#125;=&gt;<span class=\"number\">2</span>  &#123;<span class=\"number\">-2</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">0</span>  scanRight (b+a)         <span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>, <span class=\"number\">0</span>)</div></pre></td></tr></table></figure>\n","excerpt":"<p>scala 基于java，但是与java不同scala是函数式的编程语言，scala与java之间的集合框架可以相互转换调用，但是scala的集合框架封装的更完善加上scala对函数式编程的支持，在scala中使用起集合来非常方便，最近用到了一些比较有意思的内容记录如下主要是关于reduce和fold：<br>","more":"<br>对于scala的list关于fold函数的使用，如果list中是整数似乎Foldleft和Foldright都一样没啥区别，以及Fold函数本身都没啥区别<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\">cala&gt; <span class=\"keyword\">val</span> x = <span class=\"type\">List</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>)</div><div class=\"line\">x: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; x.foldLeft(<span class=\"number\">0</span>)((x,y)=&gt;x+y)</div><div class=\"line\">res5: <span class=\"type\">Int</span> = <span class=\"number\">10</span></div><div class=\"line\"></div><div class=\"line\">cala&gt; x.fold(<span class=\"number\">0</span>)((x,y)=&gt;x+y)</div><div class=\"line\">res6: <span class=\"type\">Int</span> = <span class=\"number\">10</span></div><div class=\"line\"></div><div class=\"line\">scala&gt; x.foldRight(<span class=\"number\">0</span>)((x,y)=&gt;x+y)</div><div class=\"line\">res7: <span class=\"type\">Int</span> = <span class=\"number\">10</span></div></pre></td></tr></table></figure></p>\n<p>但是如果是字符串的话就很有意思了</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> x = <span class=\"type\">List</span>(<span class=\"string\">\"this\"</span>,<span class=\"string\">\"is\"</span>,<span class=\"string\">\"an\"</span>,<span class=\"string\">\"String\"</span>,<span class=\"string\">\"Aarray\"</span>)</div><div class=\"line\">x: <span class=\"type\">List</span>[<span class=\"type\">String</span>] = <span class=\"type\">List</span>(<span class=\"keyword\">this</span>, is, an, <span class=\"type\">String</span>, <span class=\"type\">Aarray</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; x.fold(<span class=\"string\">\"where is * \"</span>)((x,y)=&gt;x+y)</div><div class=\"line\">res8: <span class=\"type\">String</span> = where is * thisisanStringAarray</div><div class=\"line\"></div><div class=\"line\">scala&gt; x.fold(<span class=\"string\">\"where is * \"</span>)((x,y)=&gt;x+<span class=\"string\">\",\"</span>+y)</div><div class=\"line\">res9: <span class=\"type\">String</span> = where is * ,<span class=\"keyword\">this</span>,is,an,<span class=\"type\">String</span>,<span class=\"type\">Aarray</span></div><div class=\"line\"></div><div class=\"line\">scala&gt; x.foldLeft(<span class=\"string\">\"where is * \"</span>)((x,y)=&gt;x+<span class=\"string\">\",\"</span>+y)</div><div class=\"line\">res10: <span class=\"type\">String</span> = where is * ,<span class=\"keyword\">this</span>,is,an,<span class=\"type\">String</span>,<span class=\"type\">Aarray</span></div><div class=\"line\"></div><div class=\"line\">scala&gt; x.foldRight(<span class=\"string\">\"where is * \"</span>)((x,y)=&gt;x+<span class=\"string\">\",\"</span>+y)</div><div class=\"line\">res11: <span class=\"type\">String</span> = <span class=\"string\">\"this,is,an,String,Aarray,where is * \"</span></div></pre></td></tr></table></figure>\n<p>从上面可以看出来如果不指定Right或者left的话默认是left<br>上面只是一些表面现象，本质是什么呢<br>fold, foldLeft, and foldRight 　　主要的区别是fold函数操作遍历问题集合的顺序。foldLeft是从左开始计算，然后往右遍历。foldRight是从右开始算，然后往左遍历。而fold遍历的顺序没有特殊的次序。来看下这三个函数的实现吧（在TraversableOnce特质里面实现）<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fold</span></span>[<span class=\"type\">A1</span> &gt;: <span class=\"type\">A</span>](z: <span class=\"type\">A1</span>)(op: (<span class=\"type\">A1</span>, <span class=\"type\">A1</span>) =&gt; <span class=\"type\">A1</span>): <span class=\"type\">A1</span> = foldLeft(z)(op)</div><div class=\"line\"> </div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">foldLeft</span></span>[<span class=\"type\">B</span>](z: <span class=\"type\">B</span>)(op: (<span class=\"type\">B</span>, <span class=\"type\">A</span>) =&gt; <span class=\"type\">B</span>): <span class=\"type\">B</span> = &#123;</div><div class=\"line\"> <span class=\"keyword\">var</span> result = z</div><div class=\"line\"> <span class=\"keyword\">this</span>.seq foreach (x =&gt; result = op(result, x))</div><div class=\"line\"> result</div><div class=\"line\"> &#125;</div><div class=\"line\">  </div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">foldRight</span></span>[<span class=\"type\">B</span>](z: <span class=\"type\">B</span>)(op: (<span class=\"type\">A</span>, <span class=\"type\">B</span>) =&gt; <span class=\"type\">B</span>): <span class=\"type\">B</span> =</div><div class=\"line\">    reversed.foldLeft(z)((x, y) =&gt; op(y, x))</div></pre></td></tr></table></figure></p>\n<p>由于fold函数遍历没有特殊的次序，所以对fold的初始化参数和返回值都有限制。在这三个函数中，初始化参数和返回值的参数类型必须相同。</p>\n<ol>\n<li>初始值的类型必须是list中元素类型的超类。在我们的例子中，我们的对List[Int]进行fold计算，而初始值是Int类型的，它是List[Int]的超类。</li>\n<li>初始值必须是中立的(neutral)。也就是它不能改变结果。比如对加法来说，中立的值是0；而对于乘法来说则是1，对于list来说则是Nil,但这也并不是绝对的，比如字符串我们可以在初始化的时候设置一个特殊的值，不一定是中立的空串。</li>\n</ol>\n<p>顺便说下，其实foldLeft和foldRight函数还有两个缩写的函数：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">/</span></span>:[<span class=\"type\">B</span>](z: <span class=\"type\">B</span>)(op: (<span class=\"type\">B</span>, <span class=\"type\">A</span>) =&gt; <span class=\"type\">B</span>): <span class=\"type\">B</span> = foldLeft(z)(op)</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> </span>:\\[<span class=\"type\">B</span>](z: <span class=\"type\">B</span>)(op: (<span class=\"type\">A</span>, <span class=\"type\">B</span>) =&gt; <span class=\"type\">B</span>): <span class=\"type\">B</span> = foldRight(z)(op)</div><div class=\"line\"></div><div class=\"line\">scala&gt; (<span class=\"number\">0</span>/:(<span class=\"number\">1</span> to <span class=\"number\">100</span>))(_+_)  </div><div class=\"line\">res32: <span class=\"type\">Int</span> = <span class=\"number\">5050</span></div><div class=\"line\"></div><div class=\"line\">scala&gt; ((<span class=\"number\">1</span> to <span class=\"number\">100</span>):\\<span class=\"number\">0</span>)(_+_) </div><div class=\"line\">res24: <span class=\"type\">Int</span> = <span class=\"number\">5050</span></div></pre></td></tr></table></figure>\n<p>reduce 与之最大的不同在于reduce不需要设置一个初始值，下面这段代码详细的展示了scala的reduce和fold的运行原理</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> list = <span class=\"type\">List</span>(<span class=\"string\">\"A\"</span>,<span class=\"string\">\"B\"</span>,<span class=\"string\">\"C\"</span>,<span class=\"string\">\"D\"</span>,<span class=\"string\">\"E\"</span>)</div><div class=\"line\">println(<span class=\"string\">\"reduce (a+b) \"</span>+list.reduce((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (a+b)+<span class=\"string\">\"  \"</span>)</div><div class=\"line\">a+b</div><div class=\"line\">&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"reduceLeft (a+b) \"</span>+list.reduceLeft((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (a+b)+<span class=\"string\">\"  \"</span>)</div><div class=\"line\">a+b</div><div class=\"line\">&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"reduceLeft (b+a) \"</span>+list.reduceLeft((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (b+a)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">b+a</div><div class=\"line\">&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"reduceRight (a+b) \"</span>+list.reduceRight((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (a+b)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">a+b</div><div class=\"line\">&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"reduceRight (b+a) \"</span>+list.reduceRight((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (b+a)+<span class=\"string\">\"  \"</span>)</div><div class=\"line\">b+a</div><div class=\"line\">&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"scan            \"</span>+list.scan(<span class=\"string\">\"[\"</span>)((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (a+b)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">a+b</div><div class=\"line\">&#125;))</div><div class=\"line\">println(<span class=\"string\">\"scanLeft (a+b)  \"</span>+list.scanLeft(<span class=\"string\">\"[\"</span>)((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (a+b)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">a+b</div><div class=\"line\">&#125;))</div><div class=\"line\">println(<span class=\"string\">\"scanLeft (b+a)  \"</span>+list.scanLeft(<span class=\"string\">\"[\"</span>)((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (b+a)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">b+a</div><div class=\"line\">&#125;))</div><div class=\"line\">println(<span class=\"string\">\"scanRight (a+b) \"</span>+list.scanRight(<span class=\"string\">\"[\"</span>)((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (a+b)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">a+b</div><div class=\"line\">&#125;))</div><div class=\"line\">println(<span class=\"string\">\"scanRight (b+a) \"</span>+list.scanRight(<span class=\"string\">\"[\"</span>)((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (b+a)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">b+a</div><div class=\"line\">&#125;))</div><div class=\"line\"><span class=\"keyword\">val</span> list1 = <span class=\"type\">List</span>(<span class=\"number\">-2</span>,<span class=\"number\">-1</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">2</span>)</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"reduce (a+b) \"</span>+list1.reduce((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (a+b)+<span class=\"string\">\"  \"</span>)</div><div class=\"line\">a+b</div><div class=\"line\">&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"reduceLeft (a+b) \"</span>+list1.reduceLeft((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (a+b)+<span class=\"string\">\"  \"</span>)</div><div class=\"line\">a+b</div><div class=\"line\">&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"reduceLeft (b+a) \"</span>+list1.reduceLeft((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (b+a)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">b+a</div><div class=\"line\">&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"      reduceRight (a+b) \"</span>+list1.reduceRight((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (a+b)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">a+b</div><div class=\"line\">&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"      reduceRight (b+a) \"</span>+list1.reduceRight((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (b+a)+<span class=\"string\">\"  \"</span>)</div><div class=\"line\">b+a</div><div class=\"line\">&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"scan            \"</span>+list1.scan(<span class=\"number\">0</span>)((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (a+b)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">a+b</div><div class=\"line\">&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"scanLeft (a+b)  \"</span>+list1.scanLeft(<span class=\"number\">0</span>)((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (a+b)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">a+b</div><div class=\"line\">&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"scanLeft (b+a)  \"</span>+list1.scanLeft(<span class=\"number\">0</span>)((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (b+a)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">b+a</div><div class=\"line\">&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"scanRight (a+b)         \"</span>+list1.scanRight(<span class=\"number\">0</span>)((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (a+b)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">a+b&#125;))</div><div class=\"line\"></div><div class=\"line\">println(<span class=\"string\">\"scanRight (b+a)         \"</span>+list1.scanRight(<span class=\"number\">0</span>)((a,b)=&gt;&#123;</div><div class=\"line\">print(<span class=\"string\">\"&#123;\"</span>+a+<span class=\"string\">\",\"</span>+b+<span class=\"string\">\"&#125;=&gt;\"</span>+ (a+b)+<span class=\"string\">\"  \"</span> )</div><div class=\"line\">b+a&#125;))</div></pre></td></tr></table></figure>\n<p>结果如下详细的显示了reduce 和fold的运行过程,一目了然</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\">&#123;<span class=\"type\">A</span>,<span class=\"type\">B</span>&#125;=&gt;<span class=\"type\">AB</span>  &#123;<span class=\"type\">AB</span>,<span class=\"type\">C</span>&#125;=&gt;<span class=\"type\">ABC</span>  &#123;<span class=\"type\">ABC</span>,<span class=\"type\">D</span>&#125;=&gt;<span class=\"type\">ABCD</span>  &#123;<span class=\"type\">ABCD</span>,<span class=\"type\">E</span>&#125;=&gt;<span class=\"type\">ABCDE</span>  reduce (a+b) <span class=\"type\">ABCDE</span></div><div class=\"line\">&#123;<span class=\"type\">A</span>,<span class=\"type\">B</span>&#125;=&gt;<span class=\"type\">AB</span>  &#123;<span class=\"type\">AB</span>,<span class=\"type\">C</span>&#125;=&gt;<span class=\"type\">ABC</span>  &#123;<span class=\"type\">ABC</span>,<span class=\"type\">D</span>&#125;=&gt;<span class=\"type\">ABCD</span>  &#123;<span class=\"type\">ABCD</span>,<span class=\"type\">E</span>&#125;=&gt;<span class=\"type\">ABCDE</span>  reduceLeft (a+b) <span class=\"type\">ABCDE</span></div><div class=\"line\">&#123;<span class=\"type\">A</span>,<span class=\"type\">B</span>&#125;=&gt;<span class=\"type\">BA</span>  &#123;<span class=\"type\">BA</span>,<span class=\"type\">C</span>&#125;=&gt;<span class=\"type\">CBA</span>  &#123;<span class=\"type\">CBA</span>,<span class=\"type\">D</span>&#125;=&gt;<span class=\"type\">DCBA</span>  &#123;<span class=\"type\">DCBA</span>,<span class=\"type\">E</span>&#125;=&gt;<span class=\"type\">EDCBA</span>  reduceLeft (b+a) <span class=\"type\">EDCBA</span></div><div class=\"line\">&#123;<span class=\"type\">D</span>,<span class=\"type\">E</span>&#125;=&gt;<span class=\"type\">DE</span>  &#123;<span class=\"type\">C</span>,<span class=\"type\">DE</span>&#125;=&gt;<span class=\"type\">CDE</span>  &#123;<span class=\"type\">B</span>,<span class=\"type\">CDE</span>&#125;=&gt;<span class=\"type\">BCDE</span>  &#123;<span class=\"type\">A</span>,<span class=\"type\">BCDE</span>&#125;=&gt;<span class=\"type\">ABCDE</span>  reduceRight (a+b) <span class=\"type\">ABCDE</span></div><div class=\"line\">&#123;<span class=\"type\">D</span>,<span class=\"type\">E</span>&#125;=&gt;<span class=\"type\">ED</span>  &#123;<span class=\"type\">C</span>,<span class=\"type\">ED</span>&#125;=&gt;<span class=\"type\">EDC</span>  &#123;<span class=\"type\">B</span>,<span class=\"type\">EDC</span>&#125;=&gt;<span class=\"type\">EDCB</span>  &#123;<span class=\"type\">A</span>,<span class=\"type\">EDCB</span>&#125;=&gt;<span class=\"type\">EDCBA</span>  reduceRight (b+a) <span class=\"type\">EDCBA</span></div><div class=\"line\">&#123;[,<span class=\"type\">A</span>&#125;=&gt;[<span class=\"type\">A</span>  &#123;[<span class=\"type\">A</span>,<span class=\"type\">B</span>&#125;=&gt;[<span class=\"type\">AB</span>  &#123;[<span class=\"type\">AB</span>,<span class=\"type\">C</span>&#125;=&gt;[<span class=\"type\">ABC</span>  &#123;[<span class=\"type\">ABC</span>,<span class=\"type\">D</span>&#125;=&gt;[<span class=\"type\">ABCD</span>  &#123;[<span class=\"type\">ABCD</span>,<span class=\"type\">E</span>&#125;=&gt;[<span class=\"type\">ABCDE</span>  scan            <span class=\"type\">List</span>([, [<span class=\"type\">A</span>, [<span class=\"type\">AB</span>, [<span class=\"type\">ABC</span>, [<span class=\"type\">ABCD</span>, [<span class=\"type\">ABCDE</span>)</div><div class=\"line\">&#123;[,<span class=\"type\">A</span>&#125;=&gt;[<span class=\"type\">A</span>  &#123;[<span class=\"type\">A</span>,<span class=\"type\">B</span>&#125;=&gt;[<span class=\"type\">AB</span>  &#123;[<span class=\"type\">AB</span>,<span class=\"type\">C</span>&#125;=&gt;[<span class=\"type\">ABC</span>  &#123;[<span class=\"type\">ABC</span>,<span class=\"type\">D</span>&#125;=&gt;[<span class=\"type\">ABCD</span>  &#123;[<span class=\"type\">ABCD</span>,<span class=\"type\">E</span>&#125;=&gt;[<span class=\"type\">ABCDE</span>  scanLeft (a+b)  <span class=\"type\">List</span>([, [<span class=\"type\">A</span>, [<span class=\"type\">AB</span>, [<span class=\"type\">ABC</span>, [<span class=\"type\">ABCD</span>, [<span class=\"type\">ABCDE</span>)</div><div class=\"line\">&#123;[,<span class=\"type\">A</span>&#125;=&gt;<span class=\"type\">A</span>[  &#123;<span class=\"type\">A</span>[,<span class=\"type\">B</span>&#125;=&gt;<span class=\"type\">BA</span>[  &#123;<span class=\"type\">BA</span>[,<span class=\"type\">C</span>&#125;=&gt;<span class=\"type\">CBA</span>[  &#123;<span class=\"type\">CBA</span>[,<span class=\"type\">D</span>&#125;=&gt;<span class=\"type\">DCBA</span>[  &#123;<span class=\"type\">DCBA</span>[,<span class=\"type\">E</span>&#125;=&gt;<span class=\"type\">EDCBA</span>[  scanLeft (b+a)  <span class=\"type\">List</span>([, <span class=\"type\">A</span>[, <span class=\"type\">BA</span>[, <span class=\"type\">CBA</span>[, <span class=\"type\">DCBA</span>[, <span class=\"type\">EDCBA</span>[)</div><div class=\"line\">&#123;<span class=\"type\">E</span>,[&#125;=&gt;<span class=\"type\">E</span>[  &#123;<span class=\"type\">D</span>,<span class=\"type\">E</span>[&#125;=&gt;<span class=\"type\">DE</span>[  &#123;<span class=\"type\">C</span>,<span class=\"type\">DE</span>[&#125;=&gt;<span class=\"type\">CDE</span>[  &#123;<span class=\"type\">B</span>,<span class=\"type\">CDE</span>[&#125;=&gt;<span class=\"type\">BCDE</span>[  &#123;<span class=\"type\">A</span>,<span class=\"type\">BCDE</span>[&#125;=&gt;<span class=\"type\">ABCDE</span>[  scanRight (a+b) <span class=\"type\">List</span>(<span class=\"type\">ABCDE</span>[, <span class=\"type\">BCDE</span>[, <span class=\"type\">CDE</span>[, <span class=\"type\">DE</span>[, <span class=\"type\">E</span>[, [)</div><div class=\"line\">&#123;<span class=\"type\">E</span>,[&#125;=&gt;[<span class=\"type\">E</span>  &#123;<span class=\"type\">D</span>,[<span class=\"type\">E</span>&#125;=&gt;[<span class=\"type\">ED</span>  &#123;<span class=\"type\">C</span>,[<span class=\"type\">ED</span>&#125;=&gt;[<span class=\"type\">EDC</span>  &#123;<span class=\"type\">B</span>,[<span class=\"type\">EDC</span>&#125;=&gt;[<span class=\"type\">EDCB</span>  &#123;<span class=\"type\">A</span>,[<span class=\"type\">EDCB</span>&#125;=&gt;[<span class=\"type\">EDCBA</span>  scanRight (b+a) <span class=\"type\">List</span>([<span class=\"type\">EDCBA</span>, [<span class=\"type\">EDCB</span>, [<span class=\"type\">EDC</span>, [<span class=\"type\">ED</span>, [<span class=\"type\">E</span>, [)</div><div class=\"line\">&#123;<span class=\"number\">-2</span>,<span class=\"number\">-1</span>&#125;=&gt;<span class=\"number\">-3</span>  &#123;<span class=\"number\">-3</span>,<span class=\"number\">0</span>&#125;=&gt;<span class=\"number\">-3</span>  &#123;<span class=\"number\">-3</span>,<span class=\"number\">1</span>&#125;=&gt;<span class=\"number\">-2</span>  &#123;<span class=\"number\">-2</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">0</span>  reduce (a+b) <span class=\"number\">0</span></div><div class=\"line\">&#123;<span class=\"number\">-2</span>,<span class=\"number\">-1</span>&#125;=&gt;<span class=\"number\">-3</span>  &#123;<span class=\"number\">-3</span>,<span class=\"number\">0</span>&#125;=&gt;<span class=\"number\">-3</span>  &#123;<span class=\"number\">-3</span>,<span class=\"number\">1</span>&#125;=&gt;<span class=\"number\">-2</span>  &#123;<span class=\"number\">-2</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">0</span>  reduceLeft (a+b) <span class=\"number\">0</span></div><div class=\"line\">&#123;<span class=\"number\">-2</span>,<span class=\"number\">-1</span>&#125;=&gt;<span class=\"number\">-3</span>  &#123;<span class=\"number\">-3</span>,<span class=\"number\">0</span>&#125;=&gt;<span class=\"number\">-3</span>  &#123;<span class=\"number\">-3</span>,<span class=\"number\">1</span>&#125;=&gt;<span class=\"number\">-2</span>  &#123;<span class=\"number\">-2</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">0</span>  reduceLeft (b+a) <span class=\"number\">0</span></div><div class=\"line\">&#123;<span class=\"number\">1</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">3</span>  &#123;<span class=\"number\">0</span>,<span class=\"number\">3</span>&#125;=&gt;<span class=\"number\">3</span>  &#123;<span class=\"number\">-1</span>,<span class=\"number\">3</span>&#125;=&gt;<span class=\"number\">2</span>  &#123;<span class=\"number\">-2</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">0</span>        reduceRight (a+b) <span class=\"number\">0</span></div><div class=\"line\">&#123;<span class=\"number\">1</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">3</span>  &#123;<span class=\"number\">0</span>,<span class=\"number\">3</span>&#125;=&gt;<span class=\"number\">3</span>  &#123;<span class=\"number\">-1</span>,<span class=\"number\">3</span>&#125;=&gt;<span class=\"number\">2</span>  &#123;<span class=\"number\">-2</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">0</span>        reduceRight (b+a) <span class=\"number\">0</span></div><div class=\"line\">&#123;<span class=\"number\">0</span>,<span class=\"number\">-2</span>&#125;=&gt;<span class=\"number\">-2</span>  &#123;<span class=\"number\">-2</span>,<span class=\"number\">-1</span>&#125;=&gt;<span class=\"number\">-3</span>  &#123;<span class=\"number\">-3</span>,<span class=\"number\">0</span>&#125;=&gt;<span class=\"number\">-3</span>  &#123;<span class=\"number\">-3</span>,<span class=\"number\">1</span>&#125;=&gt;<span class=\"number\">-2</span>  &#123;<span class=\"number\">-2</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">0</span>  scan            <span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">-2</span>, <span class=\"number\">-3</span>, <span class=\"number\">-3</span>, <span class=\"number\">-2</span>, <span class=\"number\">0</span>)</div><div class=\"line\">&#123;<span class=\"number\">0</span>,<span class=\"number\">-2</span>&#125;=&gt;<span class=\"number\">-2</span>  &#123;<span class=\"number\">-2</span>,<span class=\"number\">-1</span>&#125;=&gt;<span class=\"number\">-3</span>  &#123;<span class=\"number\">-3</span>,<span class=\"number\">0</span>&#125;=&gt;<span class=\"number\">-3</span>  &#123;<span class=\"number\">-3</span>,<span class=\"number\">1</span>&#125;=&gt;<span class=\"number\">-2</span>  &#123;<span class=\"number\">-2</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">0</span>  scanLeft (a+b)  <span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">-2</span>, <span class=\"number\">-3</span>, <span class=\"number\">-3</span>, <span class=\"number\">-2</span>, <span class=\"number\">0</span>)</div><div class=\"line\">&#123;<span class=\"number\">0</span>,<span class=\"number\">-2</span>&#125;=&gt;<span class=\"number\">-2</span>  &#123;<span class=\"number\">-2</span>,<span class=\"number\">-1</span>&#125;=&gt;<span class=\"number\">-3</span>  &#123;<span class=\"number\">-3</span>,<span class=\"number\">0</span>&#125;=&gt;<span class=\"number\">-3</span>  &#123;<span class=\"number\">-3</span>,<span class=\"number\">1</span>&#125;=&gt;<span class=\"number\">-2</span>  &#123;<span class=\"number\">-2</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">0</span>  scanLeft (b+a)  <span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">-2</span>, <span class=\"number\">-3</span>, <span class=\"number\">-3</span>, <span class=\"number\">-2</span>, <span class=\"number\">0</span>)</div><div class=\"line\">&#123;<span class=\"number\">2</span>,<span class=\"number\">0</span>&#125;=&gt;<span class=\"number\">2</span>  &#123;<span class=\"number\">1</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">3</span>  &#123;<span class=\"number\">0</span>,<span class=\"number\">3</span>&#125;=&gt;<span class=\"number\">3</span>  &#123;<span class=\"number\">-1</span>,<span class=\"number\">3</span>&#125;=&gt;<span class=\"number\">2</span>  &#123;<span class=\"number\">-2</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">0</span>  scanRight (a+b)         <span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>, <span class=\"number\">0</span>)</div><div class=\"line\">&#123;<span class=\"number\">2</span>,<span class=\"number\">0</span>&#125;=&gt;<span class=\"number\">2</span>  &#123;<span class=\"number\">1</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">3</span>  &#123;<span class=\"number\">0</span>,<span class=\"number\">3</span>&#125;=&gt;<span class=\"number\">3</span>  &#123;<span class=\"number\">-1</span>,<span class=\"number\">3</span>&#125;=&gt;<span class=\"number\">2</span>  &#123;<span class=\"number\">-2</span>,<span class=\"number\">2</span>&#125;=&gt;<span class=\"number\">0</span>  scanRight (b+a)         <span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>, <span class=\"number\">2</span>, <span class=\"number\">0</span>)</div></pre></td></tr></table></figure>"},{"title":"Scala 片段3：列表的map，flatMap，zip和reduce","date":"2017-08-30T23:27:05.000Z","_content":"\n如果不了解map，flatMap，zip和reduce函数，你就不能真正地谈论scala。通过这些函数，我们可以非常容易地处理列表的内容并结合Option对象工作。\n<!--more -->\n让我们从map开始，通过map我们可以将一个函数应用于列表的每一个元素并且将其作为一个新的列表返回。\n我们可以这样对列表的元素进行平方：\n```scala\nscala> list1\nres3: List[Int] = List(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\nscala> list1.map(x=>x*x)\nres4: List[Int] = List(0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100)\n```\n一些函数可能返回Option元素。例如：\n```scala\nscala> val evenify = (x:Int) => if (x % 2 == 0) Some(x) else None\nevenify: Int => Option[Int] = <function1>\n\nscala> list1.map(evenify)\nres6: List[Option[Int]] = List(Some(0), None, Some(2), None, Some(4), None, Some(6), None, Some(8), None, Some(10))\n```\n这个例子的问题是我们常常并不关心None。但我们怎么轻松地把他排除出去呢？对于此我们可以使用flatMap。通过flatMap我们可以处理元素是序列的列表。将提供的函数应用于每个序列元素会返回包含原始列表所有序列内的元素的列表。通过以下的例子会更好理解：\n```scala\nscala> val list3 = 10 to 20 toList\nlist3: List[Int] = List(10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)\n\nscala> val list2 = 1 to 10 toList\nlist2: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\nscala> val list4 = List(list2, list3)\nlist4: List[List[Int]] = List(List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), List(10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20))\n\nscala> list4.flatMap(x=>x.map(y=>y*2))\nres2: List[Int] = List(2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40) \n```\n我们可以看到有list4的元素是两个列表。我们调用flatMap分别处理这两个列表，并用map将这两个列表的元素平方，最后的结果是一个包含所有元素的平坦的列表。\n\n译者注：flatMap并不一定用于元素是序列的列表，他只需要应用的函数返回的结果是GenTraversableOnce即可（列表的父类），例如：\n```scala\nscala> List(1,2,3,4,5)\nres0: List[Int] = List(1, 2, 3, 4, 5)\n\nscala> res0.flatMap(x => 1 to x )\nres1: List[Int] = List(1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5)\n```\n让我们回过头看一下一直看过的evenify函数和之前返回的Option列表：\n```scala\nscala> val list1 = 1 to 10 toList\nlist1: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\nscala> list1.map(evenify)\nres3: List[Option[Int]] = List(None, Some(2), None, Some(4), None, Some(6), None, Some(8), None, Some(10))\n\nscala> val list2 = list1.map(evenify)\nlist2: List[Option[Int]] = List(None, Some(2), None, Some(4), None, Some(6), None, Some(8), None, Some(10))\n\nscala> list2.flatMap(x => x)\nres6: List[Int] = List(2, 4, 6, 8, 10)  \n```\n简单吧。我们也可以将这个写在一行：\n```scala\nscala> list1.flatMap(x=>evenify(x))\nres14: List[Int] = List(2, 4, 6, 8, 10)\n```\n正如你看到的，这并不困难。接下来让我们看一下其他两个可以用于列表的函数。第一个是zip，从它的名字就可以知道我们可以用此合并两个列表：\n```scala\nscala> val list = \"Hello.World\".toCharArray\nlist: Array[Char] = Array(H, e, l, l, o, ., W, o, r, l, d)\n\nscala> val list1 = 1 to 20 toList\nlist1: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)\n\nscala> list.zip(list1)\nres30: Array[(Char, Int)] = Array((H,1), (e,2), (l,3), (l,4), (o,5), (.,6), (W,7), (o,8), (r,9), (l,10), (d,11))\n\nscala> list1.zip(list)\nres31: List[(Int, Char)] = List((1,H), (2,e), (3,l), (4,l), (5,o), (6,.), (7,W), (8,o), (9,r), (10,l), (11,d))\n```\n返回的列表长度取决于较短的列表，只要有一个列表到达了末尾zip函数就停止了。我们可以使用zipAll函数来对较长列表的剩余元素进行处理：\n```scala\nscala> list.zipAll(list1,'a','1')\nres33: Array[(Char, AnyVal)] = Array((H,1), (e,2), (l,3), (l,4), (o,5), (.,6), (W,7), (o,8), (r,9), (l,10), (d,11), (a,12), (a,13), (a,14), (a,15), (a,16), (a,17), (a,18), (a,19), (a,20))\n```\n(译者注：最后一个参数为1，让返回类型是Array[(Char,Int)]对于这个例子更好点)  \n如果字母的列表比较短，那么用'a'来补充，反之用1来补充。最后一个要介绍的zip函数是zipWithIndex。就像他的名字一样，元素的下标（从0开始）会被增加进去：\n```scala\nscala> list.zipWithIndex\nres36: Array[(Char, Int)] = Array((H,0), (e,1), (l,2), (l,3), (o,4), (.,5), (W,6), (o,7), (r,8), (l,9), (d,10))  \n```\n我们来看看最后一个函数：reduce。使用reduce我们可以处理列表的每个元素并返回一个值。通过使用reduceLeft和reduceRight我们可以强制处理元素的方向。（使用reduce方向是不被保证的）\n译者注：reduce和fold很像，但reduce返回的值的类型必须和列表的元素类型相关（类型本身或其父类），但fold没有这种限制（但与此同时fold必须给定一个初始值），可以说reduce是fold的一种特殊情况。\n```scala\nscala> list1\nres51: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)\n\nscala> val sum = (x:Int, y:Int) => {println(x,y) ; x + y}\nsum: (Int, Int) => Int = <function2>\n\nscala> list1.reduce(sum)\n(1,2)\n(3,3)\n(6,4)\n(10,5)\n(15,6)\n(21,7)\n(28,8)\n(36,9)\n(45,10)\n(55,11)\n(66,12)\n(78,13)\n(91,14)\n(105,15)\n(120,16)\n(136,17)\n(153,18)\n(171,19)\n(190,20)\nres52: Int = 210\n\nscala> list1.reduceLeft(sum)\n(1,2)\n(3,3)\n(6,4)\n(10,5)\n(15,6)\n(21,7)\n(28,8)\n(36,9)\n(45,10)\n(55,11)\n(66,12)\n(78,13)\n(91,14)\n(105,15)\n(120,16)\n(136,17)\n(153,18)\n(171,19)\n(190,20)\nres53: Int = 210\n\nscala> list1.reduceRight(sum)\n(19,20)\n(18,39)\n(17,57)\n(16,74)\n(15,90)\n(14,105)\n(13,119)\n(12,132)\n(11,144)\n(10,155)\n(9,165)\n(8,174)\n(7,182)\n(6,189)\n(5,195)\n(4,200)\n(3,204)\n(2,207)\n(1,209)\nres54: Int = 210\n```\n对于这个片段来说这些就足够了，是时候你自己探索一下List/Collections的API了。在下一个片段中，我们将看一些Scalaz的东西，虽然因为复杂对于这个库有些负面的声音，但它的确提供了一些很棒的特性。\n","source":"_posts/scalab-md.md","raw":"---\ntitle: Scala 片段3：列表的map，flatMap，zip和reduce\ndate: 2017-08-31 07:27:05\ntags: scala\n---\n\n如果不了解map，flatMap，zip和reduce函数，你就不能真正地谈论scala。通过这些函数，我们可以非常容易地处理列表的内容并结合Option对象工作。\n<!--more -->\n让我们从map开始，通过map我们可以将一个函数应用于列表的每一个元素并且将其作为一个新的列表返回。\n我们可以这样对列表的元素进行平方：\n```scala\nscala> list1\nres3: List[Int] = List(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\nscala> list1.map(x=>x*x)\nres4: List[Int] = List(0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100)\n```\n一些函数可能返回Option元素。例如：\n```scala\nscala> val evenify = (x:Int) => if (x % 2 == 0) Some(x) else None\nevenify: Int => Option[Int] = <function1>\n\nscala> list1.map(evenify)\nres6: List[Option[Int]] = List(Some(0), None, Some(2), None, Some(4), None, Some(6), None, Some(8), None, Some(10))\n```\n这个例子的问题是我们常常并不关心None。但我们怎么轻松地把他排除出去呢？对于此我们可以使用flatMap。通过flatMap我们可以处理元素是序列的列表。将提供的函数应用于每个序列元素会返回包含原始列表所有序列内的元素的列表。通过以下的例子会更好理解：\n```scala\nscala> val list3 = 10 to 20 toList\nlist3: List[Int] = List(10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)\n\nscala> val list2 = 1 to 10 toList\nlist2: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\nscala> val list4 = List(list2, list3)\nlist4: List[List[Int]] = List(List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), List(10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20))\n\nscala> list4.flatMap(x=>x.map(y=>y*2))\nres2: List[Int] = List(2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40) \n```\n我们可以看到有list4的元素是两个列表。我们调用flatMap分别处理这两个列表，并用map将这两个列表的元素平方，最后的结果是一个包含所有元素的平坦的列表。\n\n译者注：flatMap并不一定用于元素是序列的列表，他只需要应用的函数返回的结果是GenTraversableOnce即可（列表的父类），例如：\n```scala\nscala> List(1,2,3,4,5)\nres0: List[Int] = List(1, 2, 3, 4, 5)\n\nscala> res0.flatMap(x => 1 to x )\nres1: List[Int] = List(1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5)\n```\n让我们回过头看一下一直看过的evenify函数和之前返回的Option列表：\n```scala\nscala> val list1 = 1 to 10 toList\nlist1: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\nscala> list1.map(evenify)\nres3: List[Option[Int]] = List(None, Some(2), None, Some(4), None, Some(6), None, Some(8), None, Some(10))\n\nscala> val list2 = list1.map(evenify)\nlist2: List[Option[Int]] = List(None, Some(2), None, Some(4), None, Some(6), None, Some(8), None, Some(10))\n\nscala> list2.flatMap(x => x)\nres6: List[Int] = List(2, 4, 6, 8, 10)  \n```\n简单吧。我们也可以将这个写在一行：\n```scala\nscala> list1.flatMap(x=>evenify(x))\nres14: List[Int] = List(2, 4, 6, 8, 10)\n```\n正如你看到的，这并不困难。接下来让我们看一下其他两个可以用于列表的函数。第一个是zip，从它的名字就可以知道我们可以用此合并两个列表：\n```scala\nscala> val list = \"Hello.World\".toCharArray\nlist: Array[Char] = Array(H, e, l, l, o, ., W, o, r, l, d)\n\nscala> val list1 = 1 to 20 toList\nlist1: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)\n\nscala> list.zip(list1)\nres30: Array[(Char, Int)] = Array((H,1), (e,2), (l,3), (l,4), (o,5), (.,6), (W,7), (o,8), (r,9), (l,10), (d,11))\n\nscala> list1.zip(list)\nres31: List[(Int, Char)] = List((1,H), (2,e), (3,l), (4,l), (5,o), (6,.), (7,W), (8,o), (9,r), (10,l), (11,d))\n```\n返回的列表长度取决于较短的列表，只要有一个列表到达了末尾zip函数就停止了。我们可以使用zipAll函数来对较长列表的剩余元素进行处理：\n```scala\nscala> list.zipAll(list1,'a','1')\nres33: Array[(Char, AnyVal)] = Array((H,1), (e,2), (l,3), (l,4), (o,5), (.,6), (W,7), (o,8), (r,9), (l,10), (d,11), (a,12), (a,13), (a,14), (a,15), (a,16), (a,17), (a,18), (a,19), (a,20))\n```\n(译者注：最后一个参数为1，让返回类型是Array[(Char,Int)]对于这个例子更好点)  \n如果字母的列表比较短，那么用'a'来补充，反之用1来补充。最后一个要介绍的zip函数是zipWithIndex。就像他的名字一样，元素的下标（从0开始）会被增加进去：\n```scala\nscala> list.zipWithIndex\nres36: Array[(Char, Int)] = Array((H,0), (e,1), (l,2), (l,3), (o,4), (.,5), (W,6), (o,7), (r,8), (l,9), (d,10))  \n```\n我们来看看最后一个函数：reduce。使用reduce我们可以处理列表的每个元素并返回一个值。通过使用reduceLeft和reduceRight我们可以强制处理元素的方向。（使用reduce方向是不被保证的）\n译者注：reduce和fold很像，但reduce返回的值的类型必须和列表的元素类型相关（类型本身或其父类），但fold没有这种限制（但与此同时fold必须给定一个初始值），可以说reduce是fold的一种特殊情况。\n```scala\nscala> list1\nres51: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)\n\nscala> val sum = (x:Int, y:Int) => {println(x,y) ; x + y}\nsum: (Int, Int) => Int = <function2>\n\nscala> list1.reduce(sum)\n(1,2)\n(3,3)\n(6,4)\n(10,5)\n(15,6)\n(21,7)\n(28,8)\n(36,9)\n(45,10)\n(55,11)\n(66,12)\n(78,13)\n(91,14)\n(105,15)\n(120,16)\n(136,17)\n(153,18)\n(171,19)\n(190,20)\nres52: Int = 210\n\nscala> list1.reduceLeft(sum)\n(1,2)\n(3,3)\n(6,4)\n(10,5)\n(15,6)\n(21,7)\n(28,8)\n(36,9)\n(45,10)\n(55,11)\n(66,12)\n(78,13)\n(91,14)\n(105,15)\n(120,16)\n(136,17)\n(153,18)\n(171,19)\n(190,20)\nres53: Int = 210\n\nscala> list1.reduceRight(sum)\n(19,20)\n(18,39)\n(17,57)\n(16,74)\n(15,90)\n(14,105)\n(13,119)\n(12,132)\n(11,144)\n(10,155)\n(9,165)\n(8,174)\n(7,182)\n(6,189)\n(5,195)\n(4,200)\n(3,204)\n(2,207)\n(1,209)\nres54: Int = 210\n```\n对于这个片段来说这些就足够了，是时候你自己探索一下List/Collections的API了。在下一个片段中，我们将看一些Scalaz的东西，虽然因为复杂对于这个库有些负面的声音，但它的确提供了一些很棒的特性。\n","slug":"scalab-md","published":1,"updated":"2017-08-30T23:34:57.748Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj70mt0qm000w60tusjmyn6zp","content":"<p>如果不了解map，flatMap，zip和reduce函数，你就不能真正地谈论scala。通过这些函数，我们可以非常容易地处理列表的内容并结合Option对象工作。<br><a id=\"more\"></a><br>让我们从map开始，通过map我们可以将一个函数应用于列表的每一个元素并且将其作为一个新的列表返回。<br>我们可以这样对列表的元素进行平方：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; list1</div><div class=\"line\">res3: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>, <span class=\"number\">10</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; list1.map(x=&gt;x*x)</div><div class=\"line\">res4: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">4</span>, <span class=\"number\">9</span>, <span class=\"number\">16</span>, <span class=\"number\">25</span>, <span class=\"number\">36</span>, <span class=\"number\">49</span>, <span class=\"number\">64</span>, <span class=\"number\">81</span>, <span class=\"number\">100</span>)</div></pre></td></tr></table></figure></p>\n<p>一些函数可能返回Option元素。例如：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> evenify = (x:<span class=\"type\">Int</span>) =&gt; <span class=\"keyword\">if</span> (x % <span class=\"number\">2</span> == <span class=\"number\">0</span>) <span class=\"type\">Some</span>(x) <span class=\"keyword\">else</span> <span class=\"type\">None</span></div><div class=\"line\">evenify: <span class=\"type\">Int</span> =&gt; <span class=\"type\">Option</span>[<span class=\"type\">Int</span>] = &lt;function1&gt;</div><div class=\"line\"></div><div class=\"line\">scala&gt; list1.map(evenify)</div><div class=\"line\">res6: <span class=\"type\">List</span>[<span class=\"type\">Option</span>[<span class=\"type\">Int</span>]] = <span class=\"type\">List</span>(<span class=\"type\">Some</span>(<span class=\"number\">0</span>), <span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">2</span>), <span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">4</span>), <span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">6</span>), <span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">8</span>), <span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">10</span>))</div></pre></td></tr></table></figure></p>\n<p>这个例子的问题是我们常常并不关心None。但我们怎么轻松地把他排除出去呢？对于此我们可以使用flatMap。通过flatMap我们可以处理元素是序列的列表。将提供的函数应用于每个序列元素会返回包含原始列表所有序列内的元素的列表。通过以下的例子会更好理解：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> list3 = <span class=\"number\">10</span> to <span class=\"number\">20</span> toList</div><div class=\"line\">list3: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">10</span>, <span class=\"number\">11</span>, <span class=\"number\">12</span>, <span class=\"number\">13</span>, <span class=\"number\">14</span>, <span class=\"number\">15</span>, <span class=\"number\">16</span>, <span class=\"number\">17</span>, <span class=\"number\">18</span>, <span class=\"number\">19</span>, <span class=\"number\">20</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> list2 = <span class=\"number\">1</span> to <span class=\"number\">10</span> toList</div><div class=\"line\">list2: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>, <span class=\"number\">10</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> list4 = <span class=\"type\">List</span>(list2, list3)</div><div class=\"line\">list4: <span class=\"type\">List</span>[<span class=\"type\">List</span>[<span class=\"type\">Int</span>]] = <span class=\"type\">List</span>(<span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>, <span class=\"number\">10</span>), <span class=\"type\">List</span>(<span class=\"number\">10</span>, <span class=\"number\">11</span>, <span class=\"number\">12</span>, <span class=\"number\">13</span>, <span class=\"number\">14</span>, <span class=\"number\">15</span>, <span class=\"number\">16</span>, <span class=\"number\">17</span>, <span class=\"number\">18</span>, <span class=\"number\">19</span>, <span class=\"number\">20</span>))</div><div class=\"line\"></div><div class=\"line\">scala&gt; list4.flatMap(x=&gt;x.map(y=&gt;y*<span class=\"number\">2</span>))</div><div class=\"line\">res2: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">6</span>, <span class=\"number\">8</span>, <span class=\"number\">10</span>, <span class=\"number\">12</span>, <span class=\"number\">14</span>, <span class=\"number\">16</span>, <span class=\"number\">18</span>, <span class=\"number\">20</span>, <span class=\"number\">20</span>, <span class=\"number\">22</span>, <span class=\"number\">24</span>, <span class=\"number\">26</span>, <span class=\"number\">28</span>, <span class=\"number\">30</span>, <span class=\"number\">32</span>, <span class=\"number\">34</span>, <span class=\"number\">36</span>, <span class=\"number\">38</span>, <span class=\"number\">40</span>)</div></pre></td></tr></table></figure></p>\n<p>我们可以看到有list4的元素是两个列表。我们调用flatMap分别处理这两个列表，并用map将这两个列表的元素平方，最后的结果是一个包含所有元素的平坦的列表。</p>\n<p>译者注：flatMap并不一定用于元素是序列的列表，他只需要应用的函数返回的结果是GenTraversableOnce即可（列表的父类），例如：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"type\">List</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>)</div><div class=\"line\">res0: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; res0.flatMap(x =&gt; <span class=\"number\">1</span> to x )</div><div class=\"line\">res1: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>)</div></pre></td></tr></table></figure></p>\n<p>让我们回过头看一下一直看过的evenify函数和之前返回的Option列表：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> list1 = <span class=\"number\">1</span> to <span class=\"number\">10</span> toList</div><div class=\"line\">list1: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>, <span class=\"number\">10</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; list1.map(evenify)</div><div class=\"line\">res3: <span class=\"type\">List</span>[<span class=\"type\">Option</span>[<span class=\"type\">Int</span>]] = <span class=\"type\">List</span>(<span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">2</span>), <span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">4</span>), <span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">6</span>), <span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">8</span>), <span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">10</span>))</div><div class=\"line\"></div><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> list2 = list1.map(evenify)</div><div class=\"line\">list2: <span class=\"type\">List</span>[<span class=\"type\">Option</span>[<span class=\"type\">Int</span>]] = <span class=\"type\">List</span>(<span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">2</span>), <span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">4</span>), <span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">6</span>), <span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">8</span>), <span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">10</span>))</div><div class=\"line\"></div><div class=\"line\">scala&gt; list2.flatMap(x =&gt; x)</div><div class=\"line\">res6: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">6</span>, <span class=\"number\">8</span>, <span class=\"number\">10</span>)</div></pre></td></tr></table></figure></p>\n<p>简单吧。我们也可以将这个写在一行：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; list1.flatMap(x=&gt;evenify(x))</div><div class=\"line\">res14: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">6</span>, <span class=\"number\">8</span>, <span class=\"number\">10</span>)</div></pre></td></tr></table></figure></p>\n<p>正如你看到的，这并不困难。接下来让我们看一下其他两个可以用于列表的函数。第一个是zip，从它的名字就可以知道我们可以用此合并两个列表：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> list = <span class=\"string\">\"Hello.World\"</span>.toCharArray</div><div class=\"line\">list: <span class=\"type\">Array</span>[<span class=\"type\">Char</span>] = <span class=\"type\">Array</span>(<span class=\"type\">H</span>, e, l, l, o, ., <span class=\"type\">W</span>, o, r, l, d)</div><div class=\"line\"></div><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> list1 = <span class=\"number\">1</span> to <span class=\"number\">20</span> toList</div><div class=\"line\">list1: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>, <span class=\"number\">10</span>, <span class=\"number\">11</span>, <span class=\"number\">12</span>, <span class=\"number\">13</span>, <span class=\"number\">14</span>, <span class=\"number\">15</span>, <span class=\"number\">16</span>, <span class=\"number\">17</span>, <span class=\"number\">18</span>, <span class=\"number\">19</span>, <span class=\"number\">20</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; list.zip(list1)</div><div class=\"line\">res30: <span class=\"type\">Array</span>[(<span class=\"type\">Char</span>, <span class=\"type\">Int</span>)] = <span class=\"type\">Array</span>((<span class=\"type\">H</span>,<span class=\"number\">1</span>), (e,<span class=\"number\">2</span>), (l,<span class=\"number\">3</span>), (l,<span class=\"number\">4</span>), (o,<span class=\"number\">5</span>), (.,<span class=\"number\">6</span>), (<span class=\"type\">W</span>,<span class=\"number\">7</span>), (o,<span class=\"number\">8</span>), (r,<span class=\"number\">9</span>), (l,<span class=\"number\">10</span>), (d,<span class=\"number\">11</span>))</div><div class=\"line\"></div><div class=\"line\">scala&gt; list1.zip(list)</div><div class=\"line\">res31: <span class=\"type\">List</span>[(<span class=\"type\">Int</span>, <span class=\"type\">Char</span>)] = <span class=\"type\">List</span>((<span class=\"number\">1</span>,<span class=\"type\">H</span>), (<span class=\"number\">2</span>,e), (<span class=\"number\">3</span>,l), (<span class=\"number\">4</span>,l), (<span class=\"number\">5</span>,o), (<span class=\"number\">6</span>,.), (<span class=\"number\">7</span>,<span class=\"type\">W</span>), (<span class=\"number\">8</span>,o), (<span class=\"number\">9</span>,r), (<span class=\"number\">10</span>,l), (<span class=\"number\">11</span>,d))</div></pre></td></tr></table></figure></p>\n<p>返回的列表长度取决于较短的列表，只要有一个列表到达了末尾zip函数就停止了。我们可以使用zipAll函数来对较长列表的剩余元素进行处理：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; list.zipAll(list1,'a','<span class=\"number\">1</span>')</div><div class=\"line\">res33: <span class=\"type\">Array</span>[(<span class=\"type\">Char</span>, <span class=\"type\">AnyVal</span>)] = <span class=\"type\">Array</span>((<span class=\"type\">H</span>,<span class=\"number\">1</span>), (e,<span class=\"number\">2</span>), (l,<span class=\"number\">3</span>), (l,<span class=\"number\">4</span>), (o,<span class=\"number\">5</span>), (.,<span class=\"number\">6</span>), (<span class=\"type\">W</span>,<span class=\"number\">7</span>), (o,<span class=\"number\">8</span>), (r,<span class=\"number\">9</span>), (l,<span class=\"number\">10</span>), (d,<span class=\"number\">11</span>), (a,<span class=\"number\">12</span>), (a,<span class=\"number\">13</span>), (a,<span class=\"number\">14</span>), (a,<span class=\"number\">15</span>), (a,<span class=\"number\">16</span>), (a,<span class=\"number\">17</span>), (a,<span class=\"number\">18</span>), (a,<span class=\"number\">19</span>), (a,<span class=\"number\">20</span>))</div></pre></td></tr></table></figure></p>\n<p>(译者注：最后一个参数为1，让返回类型是Array[(Char,Int)]对于这个例子更好点)<br>如果字母的列表比较短，那么用’a’来补充，反之用1来补充。最后一个要介绍的zip函数是zipWithIndex。就像他的名字一样，元素的下标（从0开始）会被增加进去：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; list.zipWithIndex</div><div class=\"line\">res36: <span class=\"type\">Array</span>[(<span class=\"type\">Char</span>, <span class=\"type\">Int</span>)] = <span class=\"type\">Array</span>((<span class=\"type\">H</span>,<span class=\"number\">0</span>), (e,<span class=\"number\">1</span>), (l,<span class=\"number\">2</span>), (l,<span class=\"number\">3</span>), (o,<span class=\"number\">4</span>), (.,<span class=\"number\">5</span>), (<span class=\"type\">W</span>,<span class=\"number\">6</span>), (o,<span class=\"number\">7</span>), (r,<span class=\"number\">8</span>), (l,<span class=\"number\">9</span>), (d,<span class=\"number\">10</span>))</div></pre></td></tr></table></figure></p>\n<p>我们来看看最后一个函数：reduce。使用reduce我们可以处理列表的每个元素并返回一个值。通过使用reduceLeft和reduceRight我们可以强制处理元素的方向。（使用reduce方向是不被保证的）<br>译者注：reduce和fold很像，但reduce返回的值的类型必须和列表的元素类型相关（类型本身或其父类），但fold没有这种限制（但与此同时fold必须给定一个初始值），可以说reduce是fold的一种特殊情况。<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; list1</div><div class=\"line\">res51: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>, <span class=\"number\">10</span>, <span class=\"number\">11</span>, <span class=\"number\">12</span>, <span class=\"number\">13</span>, <span class=\"number\">14</span>, <span class=\"number\">15</span>, <span class=\"number\">16</span>, <span class=\"number\">17</span>, <span class=\"number\">18</span>, <span class=\"number\">19</span>, <span class=\"number\">20</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> sum = (x:<span class=\"type\">Int</span>, y:<span class=\"type\">Int</span>) =&gt; &#123;println(x,y) ; x + y&#125;</div><div class=\"line\">sum: (<span class=\"type\">Int</span>, <span class=\"type\">Int</span>) =&gt; <span class=\"type\">Int</span> = &lt;function2&gt;</div><div class=\"line\"></div><div class=\"line\">scala&gt; list1.reduce(sum)</div><div class=\"line\">(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</div><div class=\"line\">(<span class=\"number\">3</span>,<span class=\"number\">3</span>)</div><div class=\"line\">(<span class=\"number\">6</span>,<span class=\"number\">4</span>)</div><div class=\"line\">(<span class=\"number\">10</span>,<span class=\"number\">5</span>)</div><div class=\"line\">(<span class=\"number\">15</span>,<span class=\"number\">6</span>)</div><div class=\"line\">(<span class=\"number\">21</span>,<span class=\"number\">7</span>)</div><div class=\"line\">(<span class=\"number\">28</span>,<span class=\"number\">8</span>)</div><div class=\"line\">(<span class=\"number\">36</span>,<span class=\"number\">9</span>)</div><div class=\"line\">(<span class=\"number\">45</span>,<span class=\"number\">10</span>)</div><div class=\"line\">(<span class=\"number\">55</span>,<span class=\"number\">11</span>)</div><div class=\"line\">(<span class=\"number\">66</span>,<span class=\"number\">12</span>)</div><div class=\"line\">(<span class=\"number\">78</span>,<span class=\"number\">13</span>)</div><div class=\"line\">(<span class=\"number\">91</span>,<span class=\"number\">14</span>)</div><div class=\"line\">(<span class=\"number\">105</span>,<span class=\"number\">15</span>)</div><div class=\"line\">(<span class=\"number\">120</span>,<span class=\"number\">16</span>)</div><div class=\"line\">(<span class=\"number\">136</span>,<span class=\"number\">17</span>)</div><div class=\"line\">(<span class=\"number\">153</span>,<span class=\"number\">18</span>)</div><div class=\"line\">(<span class=\"number\">171</span>,<span class=\"number\">19</span>)</div><div class=\"line\">(<span class=\"number\">190</span>,<span class=\"number\">20</span>)</div><div class=\"line\">res52: <span class=\"type\">Int</span> = <span class=\"number\">210</span></div><div class=\"line\"></div><div class=\"line\">scala&gt; list1.reduceLeft(sum)</div><div class=\"line\">(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</div><div class=\"line\">(<span class=\"number\">3</span>,<span class=\"number\">3</span>)</div><div class=\"line\">(<span class=\"number\">6</span>,<span class=\"number\">4</span>)</div><div class=\"line\">(<span class=\"number\">10</span>,<span class=\"number\">5</span>)</div><div class=\"line\">(<span class=\"number\">15</span>,<span class=\"number\">6</span>)</div><div class=\"line\">(<span class=\"number\">21</span>,<span class=\"number\">7</span>)</div><div class=\"line\">(<span class=\"number\">28</span>,<span class=\"number\">8</span>)</div><div class=\"line\">(<span class=\"number\">36</span>,<span class=\"number\">9</span>)</div><div class=\"line\">(<span class=\"number\">45</span>,<span class=\"number\">10</span>)</div><div class=\"line\">(<span class=\"number\">55</span>,<span class=\"number\">11</span>)</div><div class=\"line\">(<span class=\"number\">66</span>,<span class=\"number\">12</span>)</div><div class=\"line\">(<span class=\"number\">78</span>,<span class=\"number\">13</span>)</div><div class=\"line\">(<span class=\"number\">91</span>,<span class=\"number\">14</span>)</div><div class=\"line\">(<span class=\"number\">105</span>,<span class=\"number\">15</span>)</div><div class=\"line\">(<span class=\"number\">120</span>,<span class=\"number\">16</span>)</div><div class=\"line\">(<span class=\"number\">136</span>,<span class=\"number\">17</span>)</div><div class=\"line\">(<span class=\"number\">153</span>,<span class=\"number\">18</span>)</div><div class=\"line\">(<span class=\"number\">171</span>,<span class=\"number\">19</span>)</div><div class=\"line\">(<span class=\"number\">190</span>,<span class=\"number\">20</span>)</div><div class=\"line\">res53: <span class=\"type\">Int</span> = <span class=\"number\">210</span></div><div class=\"line\"></div><div class=\"line\">scala&gt; list1.reduceRight(sum)</div><div class=\"line\">(<span class=\"number\">19</span>,<span class=\"number\">20</span>)</div><div class=\"line\">(<span class=\"number\">18</span>,<span class=\"number\">39</span>)</div><div class=\"line\">(<span class=\"number\">17</span>,<span class=\"number\">57</span>)</div><div class=\"line\">(<span class=\"number\">16</span>,<span class=\"number\">74</span>)</div><div class=\"line\">(<span class=\"number\">15</span>,<span class=\"number\">90</span>)</div><div class=\"line\">(<span class=\"number\">14</span>,<span class=\"number\">105</span>)</div><div class=\"line\">(<span class=\"number\">13</span>,<span class=\"number\">119</span>)</div><div class=\"line\">(<span class=\"number\">12</span>,<span class=\"number\">132</span>)</div><div class=\"line\">(<span class=\"number\">11</span>,<span class=\"number\">144</span>)</div><div class=\"line\">(<span class=\"number\">10</span>,<span class=\"number\">155</span>)</div><div class=\"line\">(<span class=\"number\">9</span>,<span class=\"number\">165</span>)</div><div class=\"line\">(<span class=\"number\">8</span>,<span class=\"number\">174</span>)</div><div class=\"line\">(<span class=\"number\">7</span>,<span class=\"number\">182</span>)</div><div class=\"line\">(<span class=\"number\">6</span>,<span class=\"number\">189</span>)</div><div class=\"line\">(<span class=\"number\">5</span>,<span class=\"number\">195</span>)</div><div class=\"line\">(<span class=\"number\">4</span>,<span class=\"number\">200</span>)</div><div class=\"line\">(<span class=\"number\">3</span>,<span class=\"number\">204</span>)</div><div class=\"line\">(<span class=\"number\">2</span>,<span class=\"number\">207</span>)</div><div class=\"line\">(<span class=\"number\">1</span>,<span class=\"number\">209</span>)</div><div class=\"line\">res54: <span class=\"type\">Int</span> = <span class=\"number\">210</span></div></pre></td></tr></table></figure></p>\n<p>对于这个片段来说这些就足够了，是时候你自己探索一下List/Collections的API了。在下一个片段中，我们将看一些Scalaz的东西，虽然因为复杂对于这个库有些负面的声音，但它的确提供了一些很棒的特性。</p>\n","excerpt":"<p>如果不了解map，flatMap，zip和reduce函数，你就不能真正地谈论scala。通过这些函数，我们可以非常容易地处理列表的内容并结合Option对象工作。<br>","more":"<br>让我们从map开始，通过map我们可以将一个函数应用于列表的每一个元素并且将其作为一个新的列表返回。<br>我们可以这样对列表的元素进行平方：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; list1</div><div class=\"line\">res3: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>, <span class=\"number\">10</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; list1.map(x=&gt;x*x)</div><div class=\"line\">res4: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">4</span>, <span class=\"number\">9</span>, <span class=\"number\">16</span>, <span class=\"number\">25</span>, <span class=\"number\">36</span>, <span class=\"number\">49</span>, <span class=\"number\">64</span>, <span class=\"number\">81</span>, <span class=\"number\">100</span>)</div></pre></td></tr></table></figure></p>\n<p>一些函数可能返回Option元素。例如：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> evenify = (x:<span class=\"type\">Int</span>) =&gt; <span class=\"keyword\">if</span> (x % <span class=\"number\">2</span> == <span class=\"number\">0</span>) <span class=\"type\">Some</span>(x) <span class=\"keyword\">else</span> <span class=\"type\">None</span></div><div class=\"line\">evenify: <span class=\"type\">Int</span> =&gt; <span class=\"type\">Option</span>[<span class=\"type\">Int</span>] = &lt;function1&gt;</div><div class=\"line\"></div><div class=\"line\">scala&gt; list1.map(evenify)</div><div class=\"line\">res6: <span class=\"type\">List</span>[<span class=\"type\">Option</span>[<span class=\"type\">Int</span>]] = <span class=\"type\">List</span>(<span class=\"type\">Some</span>(<span class=\"number\">0</span>), <span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">2</span>), <span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">4</span>), <span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">6</span>), <span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">8</span>), <span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">10</span>))</div></pre></td></tr></table></figure></p>\n<p>这个例子的问题是我们常常并不关心None。但我们怎么轻松地把他排除出去呢？对于此我们可以使用flatMap。通过flatMap我们可以处理元素是序列的列表。将提供的函数应用于每个序列元素会返回包含原始列表所有序列内的元素的列表。通过以下的例子会更好理解：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> list3 = <span class=\"number\">10</span> to <span class=\"number\">20</span> toList</div><div class=\"line\">list3: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">10</span>, <span class=\"number\">11</span>, <span class=\"number\">12</span>, <span class=\"number\">13</span>, <span class=\"number\">14</span>, <span class=\"number\">15</span>, <span class=\"number\">16</span>, <span class=\"number\">17</span>, <span class=\"number\">18</span>, <span class=\"number\">19</span>, <span class=\"number\">20</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> list2 = <span class=\"number\">1</span> to <span class=\"number\">10</span> toList</div><div class=\"line\">list2: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>, <span class=\"number\">10</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> list4 = <span class=\"type\">List</span>(list2, list3)</div><div class=\"line\">list4: <span class=\"type\">List</span>[<span class=\"type\">List</span>[<span class=\"type\">Int</span>]] = <span class=\"type\">List</span>(<span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>, <span class=\"number\">10</span>), <span class=\"type\">List</span>(<span class=\"number\">10</span>, <span class=\"number\">11</span>, <span class=\"number\">12</span>, <span class=\"number\">13</span>, <span class=\"number\">14</span>, <span class=\"number\">15</span>, <span class=\"number\">16</span>, <span class=\"number\">17</span>, <span class=\"number\">18</span>, <span class=\"number\">19</span>, <span class=\"number\">20</span>))</div><div class=\"line\"></div><div class=\"line\">scala&gt; list4.flatMap(x=&gt;x.map(y=&gt;y*<span class=\"number\">2</span>))</div><div class=\"line\">res2: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">6</span>, <span class=\"number\">8</span>, <span class=\"number\">10</span>, <span class=\"number\">12</span>, <span class=\"number\">14</span>, <span class=\"number\">16</span>, <span class=\"number\">18</span>, <span class=\"number\">20</span>, <span class=\"number\">20</span>, <span class=\"number\">22</span>, <span class=\"number\">24</span>, <span class=\"number\">26</span>, <span class=\"number\">28</span>, <span class=\"number\">30</span>, <span class=\"number\">32</span>, <span class=\"number\">34</span>, <span class=\"number\">36</span>, <span class=\"number\">38</span>, <span class=\"number\">40</span>)</div></pre></td></tr></table></figure></p>\n<p>我们可以看到有list4的元素是两个列表。我们调用flatMap分别处理这两个列表，并用map将这两个列表的元素平方，最后的结果是一个包含所有元素的平坦的列表。</p>\n<p>译者注：flatMap并不一定用于元素是序列的列表，他只需要应用的函数返回的结果是GenTraversableOnce即可（列表的父类），例如：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"type\">List</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>)</div><div class=\"line\">res0: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; res0.flatMap(x =&gt; <span class=\"number\">1</span> to x )</div><div class=\"line\">res1: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>)</div></pre></td></tr></table></figure></p>\n<p>让我们回过头看一下一直看过的evenify函数和之前返回的Option列表：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> list1 = <span class=\"number\">1</span> to <span class=\"number\">10</span> toList</div><div class=\"line\">list1: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>, <span class=\"number\">10</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; list1.map(evenify)</div><div class=\"line\">res3: <span class=\"type\">List</span>[<span class=\"type\">Option</span>[<span class=\"type\">Int</span>]] = <span class=\"type\">List</span>(<span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">2</span>), <span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">4</span>), <span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">6</span>), <span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">8</span>), <span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">10</span>))</div><div class=\"line\"></div><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> list2 = list1.map(evenify)</div><div class=\"line\">list2: <span class=\"type\">List</span>[<span class=\"type\">Option</span>[<span class=\"type\">Int</span>]] = <span class=\"type\">List</span>(<span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">2</span>), <span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">4</span>), <span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">6</span>), <span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">8</span>), <span class=\"type\">None</span>, <span class=\"type\">Some</span>(<span class=\"number\">10</span>))</div><div class=\"line\"></div><div class=\"line\">scala&gt; list2.flatMap(x =&gt; x)</div><div class=\"line\">res6: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">6</span>, <span class=\"number\">8</span>, <span class=\"number\">10</span>)</div></pre></td></tr></table></figure></p>\n<p>简单吧。我们也可以将这个写在一行：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; list1.flatMap(x=&gt;evenify(x))</div><div class=\"line\">res14: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">6</span>, <span class=\"number\">8</span>, <span class=\"number\">10</span>)</div></pre></td></tr></table></figure></p>\n<p>正如你看到的，这并不困难。接下来让我们看一下其他两个可以用于列表的函数。第一个是zip，从它的名字就可以知道我们可以用此合并两个列表：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> list = <span class=\"string\">\"Hello.World\"</span>.toCharArray</div><div class=\"line\">list: <span class=\"type\">Array</span>[<span class=\"type\">Char</span>] = <span class=\"type\">Array</span>(<span class=\"type\">H</span>, e, l, l, o, ., <span class=\"type\">W</span>, o, r, l, d)</div><div class=\"line\"></div><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> list1 = <span class=\"number\">1</span> to <span class=\"number\">20</span> toList</div><div class=\"line\">list1: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>, <span class=\"number\">10</span>, <span class=\"number\">11</span>, <span class=\"number\">12</span>, <span class=\"number\">13</span>, <span class=\"number\">14</span>, <span class=\"number\">15</span>, <span class=\"number\">16</span>, <span class=\"number\">17</span>, <span class=\"number\">18</span>, <span class=\"number\">19</span>, <span class=\"number\">20</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; list.zip(list1)</div><div class=\"line\">res30: <span class=\"type\">Array</span>[(<span class=\"type\">Char</span>, <span class=\"type\">Int</span>)] = <span class=\"type\">Array</span>((<span class=\"type\">H</span>,<span class=\"number\">1</span>), (e,<span class=\"number\">2</span>), (l,<span class=\"number\">3</span>), (l,<span class=\"number\">4</span>), (o,<span class=\"number\">5</span>), (.,<span class=\"number\">6</span>), (<span class=\"type\">W</span>,<span class=\"number\">7</span>), (o,<span class=\"number\">8</span>), (r,<span class=\"number\">9</span>), (l,<span class=\"number\">10</span>), (d,<span class=\"number\">11</span>))</div><div class=\"line\"></div><div class=\"line\">scala&gt; list1.zip(list)</div><div class=\"line\">res31: <span class=\"type\">List</span>[(<span class=\"type\">Int</span>, <span class=\"type\">Char</span>)] = <span class=\"type\">List</span>((<span class=\"number\">1</span>,<span class=\"type\">H</span>), (<span class=\"number\">2</span>,e), (<span class=\"number\">3</span>,l), (<span class=\"number\">4</span>,l), (<span class=\"number\">5</span>,o), (<span class=\"number\">6</span>,.), (<span class=\"number\">7</span>,<span class=\"type\">W</span>), (<span class=\"number\">8</span>,o), (<span class=\"number\">9</span>,r), (<span class=\"number\">10</span>,l), (<span class=\"number\">11</span>,d))</div></pre></td></tr></table></figure></p>\n<p>返回的列表长度取决于较短的列表，只要有一个列表到达了末尾zip函数就停止了。我们可以使用zipAll函数来对较长列表的剩余元素进行处理：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; list.zipAll(list1,'a','<span class=\"number\">1</span>')</div><div class=\"line\">res33: <span class=\"type\">Array</span>[(<span class=\"type\">Char</span>, <span class=\"type\">AnyVal</span>)] = <span class=\"type\">Array</span>((<span class=\"type\">H</span>,<span class=\"number\">1</span>), (e,<span class=\"number\">2</span>), (l,<span class=\"number\">3</span>), (l,<span class=\"number\">4</span>), (o,<span class=\"number\">5</span>), (.,<span class=\"number\">6</span>), (<span class=\"type\">W</span>,<span class=\"number\">7</span>), (o,<span class=\"number\">8</span>), (r,<span class=\"number\">9</span>), (l,<span class=\"number\">10</span>), (d,<span class=\"number\">11</span>), (a,<span class=\"number\">12</span>), (a,<span class=\"number\">13</span>), (a,<span class=\"number\">14</span>), (a,<span class=\"number\">15</span>), (a,<span class=\"number\">16</span>), (a,<span class=\"number\">17</span>), (a,<span class=\"number\">18</span>), (a,<span class=\"number\">19</span>), (a,<span class=\"number\">20</span>))</div></pre></td></tr></table></figure></p>\n<p>(译者注：最后一个参数为1，让返回类型是Array[(Char,Int)]对于这个例子更好点)<br>如果字母的列表比较短，那么用’a’来补充，反之用1来补充。最后一个要介绍的zip函数是zipWithIndex。就像他的名字一样，元素的下标（从0开始）会被增加进去：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; list.zipWithIndex</div><div class=\"line\">res36: <span class=\"type\">Array</span>[(<span class=\"type\">Char</span>, <span class=\"type\">Int</span>)] = <span class=\"type\">Array</span>((<span class=\"type\">H</span>,<span class=\"number\">0</span>), (e,<span class=\"number\">1</span>), (l,<span class=\"number\">2</span>), (l,<span class=\"number\">3</span>), (o,<span class=\"number\">4</span>), (.,<span class=\"number\">5</span>), (<span class=\"type\">W</span>,<span class=\"number\">6</span>), (o,<span class=\"number\">7</span>), (r,<span class=\"number\">8</span>), (l,<span class=\"number\">9</span>), (d,<span class=\"number\">10</span>))</div></pre></td></tr></table></figure></p>\n<p>我们来看看最后一个函数：reduce。使用reduce我们可以处理列表的每个元素并返回一个值。通过使用reduceLeft和reduceRight我们可以强制处理元素的方向。（使用reduce方向是不被保证的）<br>译者注：reduce和fold很像，但reduce返回的值的类型必须和列表的元素类型相关（类型本身或其父类），但fold没有这种限制（但与此同时fold必须给定一个初始值），可以说reduce是fold的一种特殊情况。<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; list1</div><div class=\"line\">res51: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">9</span>, <span class=\"number\">10</span>, <span class=\"number\">11</span>, <span class=\"number\">12</span>, <span class=\"number\">13</span>, <span class=\"number\">14</span>, <span class=\"number\">15</span>, <span class=\"number\">16</span>, <span class=\"number\">17</span>, <span class=\"number\">18</span>, <span class=\"number\">19</span>, <span class=\"number\">20</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> sum = (x:<span class=\"type\">Int</span>, y:<span class=\"type\">Int</span>) =&gt; &#123;println(x,y) ; x + y&#125;</div><div class=\"line\">sum: (<span class=\"type\">Int</span>, <span class=\"type\">Int</span>) =&gt; <span class=\"type\">Int</span> = &lt;function2&gt;</div><div class=\"line\"></div><div class=\"line\">scala&gt; list1.reduce(sum)</div><div class=\"line\">(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</div><div class=\"line\">(<span class=\"number\">3</span>,<span class=\"number\">3</span>)</div><div class=\"line\">(<span class=\"number\">6</span>,<span class=\"number\">4</span>)</div><div class=\"line\">(<span class=\"number\">10</span>,<span class=\"number\">5</span>)</div><div class=\"line\">(<span class=\"number\">15</span>,<span class=\"number\">6</span>)</div><div class=\"line\">(<span class=\"number\">21</span>,<span class=\"number\">7</span>)</div><div class=\"line\">(<span class=\"number\">28</span>,<span class=\"number\">8</span>)</div><div class=\"line\">(<span class=\"number\">36</span>,<span class=\"number\">9</span>)</div><div class=\"line\">(<span class=\"number\">45</span>,<span class=\"number\">10</span>)</div><div class=\"line\">(<span class=\"number\">55</span>,<span class=\"number\">11</span>)</div><div class=\"line\">(<span class=\"number\">66</span>,<span class=\"number\">12</span>)</div><div class=\"line\">(<span class=\"number\">78</span>,<span class=\"number\">13</span>)</div><div class=\"line\">(<span class=\"number\">91</span>,<span class=\"number\">14</span>)</div><div class=\"line\">(<span class=\"number\">105</span>,<span class=\"number\">15</span>)</div><div class=\"line\">(<span class=\"number\">120</span>,<span class=\"number\">16</span>)</div><div class=\"line\">(<span class=\"number\">136</span>,<span class=\"number\">17</span>)</div><div class=\"line\">(<span class=\"number\">153</span>,<span class=\"number\">18</span>)</div><div class=\"line\">(<span class=\"number\">171</span>,<span class=\"number\">19</span>)</div><div class=\"line\">(<span class=\"number\">190</span>,<span class=\"number\">20</span>)</div><div class=\"line\">res52: <span class=\"type\">Int</span> = <span class=\"number\">210</span></div><div class=\"line\"></div><div class=\"line\">scala&gt; list1.reduceLeft(sum)</div><div class=\"line\">(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</div><div class=\"line\">(<span class=\"number\">3</span>,<span class=\"number\">3</span>)</div><div class=\"line\">(<span class=\"number\">6</span>,<span class=\"number\">4</span>)</div><div class=\"line\">(<span class=\"number\">10</span>,<span class=\"number\">5</span>)</div><div class=\"line\">(<span class=\"number\">15</span>,<span class=\"number\">6</span>)</div><div class=\"line\">(<span class=\"number\">21</span>,<span class=\"number\">7</span>)</div><div class=\"line\">(<span class=\"number\">28</span>,<span class=\"number\">8</span>)</div><div class=\"line\">(<span class=\"number\">36</span>,<span class=\"number\">9</span>)</div><div class=\"line\">(<span class=\"number\">45</span>,<span class=\"number\">10</span>)</div><div class=\"line\">(<span class=\"number\">55</span>,<span class=\"number\">11</span>)</div><div class=\"line\">(<span class=\"number\">66</span>,<span class=\"number\">12</span>)</div><div class=\"line\">(<span class=\"number\">78</span>,<span class=\"number\">13</span>)</div><div class=\"line\">(<span class=\"number\">91</span>,<span class=\"number\">14</span>)</div><div class=\"line\">(<span class=\"number\">105</span>,<span class=\"number\">15</span>)</div><div class=\"line\">(<span class=\"number\">120</span>,<span class=\"number\">16</span>)</div><div class=\"line\">(<span class=\"number\">136</span>,<span class=\"number\">17</span>)</div><div class=\"line\">(<span class=\"number\">153</span>,<span class=\"number\">18</span>)</div><div class=\"line\">(<span class=\"number\">171</span>,<span class=\"number\">19</span>)</div><div class=\"line\">(<span class=\"number\">190</span>,<span class=\"number\">20</span>)</div><div class=\"line\">res53: <span class=\"type\">Int</span> = <span class=\"number\">210</span></div><div class=\"line\"></div><div class=\"line\">scala&gt; list1.reduceRight(sum)</div><div class=\"line\">(<span class=\"number\">19</span>,<span class=\"number\">20</span>)</div><div class=\"line\">(<span class=\"number\">18</span>,<span class=\"number\">39</span>)</div><div class=\"line\">(<span class=\"number\">17</span>,<span class=\"number\">57</span>)</div><div class=\"line\">(<span class=\"number\">16</span>,<span class=\"number\">74</span>)</div><div class=\"line\">(<span class=\"number\">15</span>,<span class=\"number\">90</span>)</div><div class=\"line\">(<span class=\"number\">14</span>,<span class=\"number\">105</span>)</div><div class=\"line\">(<span class=\"number\">13</span>,<span class=\"number\">119</span>)</div><div class=\"line\">(<span class=\"number\">12</span>,<span class=\"number\">132</span>)</div><div class=\"line\">(<span class=\"number\">11</span>,<span class=\"number\">144</span>)</div><div class=\"line\">(<span class=\"number\">10</span>,<span class=\"number\">155</span>)</div><div class=\"line\">(<span class=\"number\">9</span>,<span class=\"number\">165</span>)</div><div class=\"line\">(<span class=\"number\">8</span>,<span class=\"number\">174</span>)</div><div class=\"line\">(<span class=\"number\">7</span>,<span class=\"number\">182</span>)</div><div class=\"line\">(<span class=\"number\">6</span>,<span class=\"number\">189</span>)</div><div class=\"line\">(<span class=\"number\">5</span>,<span class=\"number\">195</span>)</div><div class=\"line\">(<span class=\"number\">4</span>,<span class=\"number\">200</span>)</div><div class=\"line\">(<span class=\"number\">3</span>,<span class=\"number\">204</span>)</div><div class=\"line\">(<span class=\"number\">2</span>,<span class=\"number\">207</span>)</div><div class=\"line\">(<span class=\"number\">1</span>,<span class=\"number\">209</span>)</div><div class=\"line\">res54: <span class=\"type\">Int</span> = <span class=\"number\">210</span></div></pre></td></tr></table></figure></p>\n<p>对于这个片段来说这些就足够了，是时候你自己探索一下List/Collections的API了。在下一个片段中，我们将看一些Scalaz的东西，虽然因为复杂对于这个库有些负面的声音，但它的确提供了一些很棒的特性。</p>"},{"title":"linux 小工具使用技巧","date":"2017-02-22T15:57:54.000Z","_content":"工作中几乎每天都需要使用linux，刚开始接触linux觉得很高大上，适合装逼用，后面用着用着发现这东西不仅可以装逼还可以提高工作效率，开源界的各路高手为linux/unix写了非常多实用的小工具，这些工具配合起来使用可以极大的提升我们工作效率，今天主要记录一下join，cut,sort,paste,uniq,split 这几个小工具。\n<!--more-->\n首先跟大家说一下，linux下很多工作归根结底都是对文本的操作，比如我们要找到所有的文本文件，然后找到特定的内容保存。首先查找文本文件用到的是find,通过find我们可以找到很多路径，这些路径我们通过管道传递给grep awk join cut 这些小工具，作进一步的操作，最后将结果重定向到一个文件中保存。find awk grep这几个东西很强大我们后面会记录他的详细用法。\n# join的用法\njoin 普通文件关联\n首先看下join的文档\n```\njoin --help\nUsage: join [OPTION]... FILE1 FILE2\nFor each pair of input lines with identical join fields, write a line to\nstandard output.  The default join field is the first, delimited\nby whitespace.  When FILE1 or FILE2 (not both) is -, read standard input.\n\n  -a FILENUM        also print unpairable lines from file FILENUM, where\n                      FILENUM is 1 or 2, corresponding to FILE1 or FILE2\n  -e EMPTY          replace missing input fields with EMPTY\n  -i, --ignore-case  ignore differences in case when comparing fields\n  -j FIELD          equivalent to '-1 FIELD -2 FIELD'\n  -o FORMAT         obey FORMAT while constructing output line\n  -t CHAR           use CHAR as input and output field separator\n  -v FILENUM        like -a FILENUM, but suppress joined output lines\n  -1 FIELD          join on this FIELD of file 1\n  -2 FIELD          join on this FIELD of file 2\n  --check-order     check that the input is correctly sorted, even\n                      if all input lines are pairable\n  --nocheck-order   do not check that the input is correctly sorted\n  --header          treat the first line in each file as field headers,\n                      print them without trying to pair them\n      --help     display this help and exit\n      --version  output version information and exit\n```\n上面这个 -1 -2 可以指定以那个字段为KEY来关联，这个也可以用-j来指定\n比如我们有两个文件a.txt,b.txt\na.txt           \n1 22            \n2 33            \n3 44            \n\n\n b.txt\n 1   a\n 2   b\n 3   c\n\n\n我们将两个文件关联到一起只需要一下操作\n```\njoin  a.txt b.txt\n\n1 22 a\n2 33 b\n3 44 c\n```\n按列关联在一起哪个文件在前面那个列就在前面\n\n# paste 合并\n\n比如我们有上面两个文件用paste来合并结果如下\n```\npaste a.txt b.txt\n\n1 22     1   a\n2 33     2   b\n3 44     3   c\n```\npaste也可以通过-s将文件按行来合并\n```\npaste -s a.txt  b.txt \n1 22    2 33    3 44\n1 a     2 b     3 c\n```\npaste-选项可以格式化输出内容，即对每一个（-），从标准输入中读一次数据。\njoin和paste都有其他参数比如下面会将两个文件以冒号分连接在一起：\n\n```\npaste -d: a.txt b.txt\n\n1 22:1   a\n2 33:2   b\n3 44:3   c\n```\n\n# cut 切分\n```\ncut -d\" \" -f 1 a.txt \n1 \n2 \n3 \n```\nf可以指定输出切分的字段，d可以指定字段分隔符\n# sort 排序\nsort 可以对文件中的记录按照一定的规则排序\nc.txt\n1\n3\n5\n7\n11\n2\n4\n6\n10\n8\n9\n```\nsort c.txt \n1\n10\n11\n2\n3\n4\n5\n6\n7\n8\n9\n```\n以字母序排列\n```\nsort -n c.txt \n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n```\n按照数字顺序进行排序，sort默认的是生序排列通过 r选项可以逆序排列。\n其他选项如:\n-k可以指定排序的字段，\n-t可以指定字段分割符。\n-f会将小写字母都转换为大写字母来进行比较，亦即忽略大小写\n-c会检查文件是否已排好序，如果乱序，则输出第一个乱序的行的相关信息，最后返回1\n-C会检查文件是否已排好序，如果乱序，不输出内容，仅返回1\n-M会以月份来排序，比如JAN小于FEB等等\n-b会忽略每一行前面的所有空白部分，从第一个可见字符开始比较。\n\nuniq 可以去掉文件中重复的列 其实sort的u选项也可以完成同样的操作.\n# split 和 dd 切分文件\nsplit可以将大文件切割成小文件,但是使用起来不是很方便个人用的少不展开说了。\n\ndd 也可以完成切割我个人觉得他更优雅一些,下面是我对自己在打包docker镜像时的分割操作，当时是由于网站上传文件的大小限制太大了不能上传，所以用dd将其且分成小文件。\n```\n#dd if=ubunt.tar bs=1024 count=97000 skip=0  of=ubuntu.tar.1\n#dd if=ubunt.tar bs=1024 count=97000 skip=97000  of=ubuntu.tar.2\n#cat ubuntu.tar.1 ubuntu.tar.2 > ubuntu.tar\ndd if=f21.tar bs=1024 count=97000 skip=0  of=fedora.tar.1\ndd if=f21.tar bs=1024 count=97000 skip=97000  of=fedora.tar.2\ndd if=f21.tar bs=1024 count=97000 skip=194000  of=fedora.tar.3\ndd if=mysql.tar bs=1024 count=97000 skip=0  of=mysql.tar.1\ndd if=mysql.tar bs=1024 count=97000 skip=97000  of=mysql.tar.2\ndd if=mysql.tar bs=1024 count=97000 skip=194000  of=mysql.tar.\n```\n其中bs表示写出块的大小，count表示写出块的数量，skip表示跳过多少个块，of表示输出文件，if表示输入文件.\n这几个工具有很多种用法，恰当的使用可以提高效率。\n","source":"_posts/test.md","raw":"---\ntitle: linux 小工具使用技巧\ndate: 2017-02-22 23:57:54\ntags: linux\n---\n工作中几乎每天都需要使用linux，刚开始接触linux觉得很高大上，适合装逼用，后面用着用着发现这东西不仅可以装逼还可以提高工作效率，开源界的各路高手为linux/unix写了非常多实用的小工具，这些工具配合起来使用可以极大的提升我们工作效率，今天主要记录一下join，cut,sort,paste,uniq,split 这几个小工具。\n<!--more-->\n首先跟大家说一下，linux下很多工作归根结底都是对文本的操作，比如我们要找到所有的文本文件，然后找到特定的内容保存。首先查找文本文件用到的是find,通过find我们可以找到很多路径，这些路径我们通过管道传递给grep awk join cut 这些小工具，作进一步的操作，最后将结果重定向到一个文件中保存。find awk grep这几个东西很强大我们后面会记录他的详细用法。\n# join的用法\njoin 普通文件关联\n首先看下join的文档\n```\njoin --help\nUsage: join [OPTION]... FILE1 FILE2\nFor each pair of input lines with identical join fields, write a line to\nstandard output.  The default join field is the first, delimited\nby whitespace.  When FILE1 or FILE2 (not both) is -, read standard input.\n\n  -a FILENUM        also print unpairable lines from file FILENUM, where\n                      FILENUM is 1 or 2, corresponding to FILE1 or FILE2\n  -e EMPTY          replace missing input fields with EMPTY\n  -i, --ignore-case  ignore differences in case when comparing fields\n  -j FIELD          equivalent to '-1 FIELD -2 FIELD'\n  -o FORMAT         obey FORMAT while constructing output line\n  -t CHAR           use CHAR as input and output field separator\n  -v FILENUM        like -a FILENUM, but suppress joined output lines\n  -1 FIELD          join on this FIELD of file 1\n  -2 FIELD          join on this FIELD of file 2\n  --check-order     check that the input is correctly sorted, even\n                      if all input lines are pairable\n  --nocheck-order   do not check that the input is correctly sorted\n  --header          treat the first line in each file as field headers,\n                      print them without trying to pair them\n      --help     display this help and exit\n      --version  output version information and exit\n```\n上面这个 -1 -2 可以指定以那个字段为KEY来关联，这个也可以用-j来指定\n比如我们有两个文件a.txt,b.txt\na.txt           \n1 22            \n2 33            \n3 44            \n\n\n b.txt\n 1   a\n 2   b\n 3   c\n\n\n我们将两个文件关联到一起只需要一下操作\n```\njoin  a.txt b.txt\n\n1 22 a\n2 33 b\n3 44 c\n```\n按列关联在一起哪个文件在前面那个列就在前面\n\n# paste 合并\n\n比如我们有上面两个文件用paste来合并结果如下\n```\npaste a.txt b.txt\n\n1 22     1   a\n2 33     2   b\n3 44     3   c\n```\npaste也可以通过-s将文件按行来合并\n```\npaste -s a.txt  b.txt \n1 22    2 33    3 44\n1 a     2 b     3 c\n```\npaste-选项可以格式化输出内容，即对每一个（-），从标准输入中读一次数据。\njoin和paste都有其他参数比如下面会将两个文件以冒号分连接在一起：\n\n```\npaste -d: a.txt b.txt\n\n1 22:1   a\n2 33:2   b\n3 44:3   c\n```\n\n# cut 切分\n```\ncut -d\" \" -f 1 a.txt \n1 \n2 \n3 \n```\nf可以指定输出切分的字段，d可以指定字段分隔符\n# sort 排序\nsort 可以对文件中的记录按照一定的规则排序\nc.txt\n1\n3\n5\n7\n11\n2\n4\n6\n10\n8\n9\n```\nsort c.txt \n1\n10\n11\n2\n3\n4\n5\n6\n7\n8\n9\n```\n以字母序排列\n```\nsort -n c.txt \n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n```\n按照数字顺序进行排序，sort默认的是生序排列通过 r选项可以逆序排列。\n其他选项如:\n-k可以指定排序的字段，\n-t可以指定字段分割符。\n-f会将小写字母都转换为大写字母来进行比较，亦即忽略大小写\n-c会检查文件是否已排好序，如果乱序，则输出第一个乱序的行的相关信息，最后返回1\n-C会检查文件是否已排好序，如果乱序，不输出内容，仅返回1\n-M会以月份来排序，比如JAN小于FEB等等\n-b会忽略每一行前面的所有空白部分，从第一个可见字符开始比较。\n\nuniq 可以去掉文件中重复的列 其实sort的u选项也可以完成同样的操作.\n# split 和 dd 切分文件\nsplit可以将大文件切割成小文件,但是使用起来不是很方便个人用的少不展开说了。\n\ndd 也可以完成切割我个人觉得他更优雅一些,下面是我对自己在打包docker镜像时的分割操作，当时是由于网站上传文件的大小限制太大了不能上传，所以用dd将其且分成小文件。\n```\n#dd if=ubunt.tar bs=1024 count=97000 skip=0  of=ubuntu.tar.1\n#dd if=ubunt.tar bs=1024 count=97000 skip=97000  of=ubuntu.tar.2\n#cat ubuntu.tar.1 ubuntu.tar.2 > ubuntu.tar\ndd if=f21.tar bs=1024 count=97000 skip=0  of=fedora.tar.1\ndd if=f21.tar bs=1024 count=97000 skip=97000  of=fedora.tar.2\ndd if=f21.tar bs=1024 count=97000 skip=194000  of=fedora.tar.3\ndd if=mysql.tar bs=1024 count=97000 skip=0  of=mysql.tar.1\ndd if=mysql.tar bs=1024 count=97000 skip=97000  of=mysql.tar.2\ndd if=mysql.tar bs=1024 count=97000 skip=194000  of=mysql.tar.\n```\n其中bs表示写出块的大小，count表示写出块的数量，skip表示跳过多少个块，of表示输出文件，if表示输入文件.\n这几个工具有很多种用法，恰当的使用可以提高效率。\n","slug":"test","published":1,"updated":"2017-02-24T17:56:06.762Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj70mt0qo000y60tulzcbcag5","content":"<p>工作中几乎每天都需要使用linux，刚开始接触linux觉得很高大上，适合装逼用，后面用着用着发现这东西不仅可以装逼还可以提高工作效率，开源界的各路高手为linux/unix写了非常多实用的小工具，这些工具配合起来使用可以极大的提升我们工作效率，今天主要记录一下join，cut,sort,paste,uniq,split 这几个小工具。<br><a id=\"more\"></a><br>首先跟大家说一下，linux下很多工作归根结底都是对文本的操作，比如我们要找到所有的文本文件，然后找到特定的内容保存。首先查找文本文件用到的是find,通过find我们可以找到很多路径，这些路径我们通过管道传递给grep awk join cut 这些小工具，作进一步的操作，最后将结果重定向到一个文件中保存。find awk grep这几个东西很强大我们后面会记录他的详细用法。</p>\n<h1 id=\"join的用法\"><a href=\"#join的用法\" class=\"headerlink\" title=\"join的用法\"></a>join的用法</h1><p>join 普通文件关联<br>首先看下join的文档<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\">join --help</div><div class=\"line\">Usage: join [OPTION]... FILE1 FILE2</div><div class=\"line\">For each pair of input lines with identical join fields, write a line to</div><div class=\"line\">standard output.  The default join field is the first, delimited</div><div class=\"line\">by whitespace.  When FILE1 or FILE2 (not both) is -, read standard input.</div><div class=\"line\"></div><div class=\"line\">  -a FILENUM        also print unpairable lines from file FILENUM, where</div><div class=\"line\">                      FILENUM is 1 or 2, corresponding to FILE1 or FILE2</div><div class=\"line\">  -e EMPTY          replace missing input fields with EMPTY</div><div class=\"line\">  -i, --ignore-case  ignore differences in case when comparing fields</div><div class=\"line\">  -j FIELD          equivalent to &apos;-1 FIELD -2 FIELD&apos;</div><div class=\"line\">  -o FORMAT         obey FORMAT while constructing output line</div><div class=\"line\">  -t CHAR           use CHAR as input and output field separator</div><div class=\"line\">  -v FILENUM        like -a FILENUM, but suppress joined output lines</div><div class=\"line\">  -1 FIELD          join on this FIELD of file 1</div><div class=\"line\">  -2 FIELD          join on this FIELD of file 2</div><div class=\"line\">  --check-order     check that the input is correctly sorted, even</div><div class=\"line\">                      if all input lines are pairable</div><div class=\"line\">  --nocheck-order   do not check that the input is correctly sorted</div><div class=\"line\">  --header          treat the first line in each file as field headers,</div><div class=\"line\">                      print them without trying to pair them</div><div class=\"line\">      --help     display this help and exit</div><div class=\"line\">      --version  output version information and exit</div></pre></td></tr></table></figure></p>\n<p>上面这个 -1 -2 可以指定以那个字段为KEY来关联，这个也可以用-j来指定<br>比如我们有两个文件a.txt,b.txt<br>a.txt<br>1 22<br>2 33<br>3 44            </p>\n<p> b.txt<br> 1   a<br> 2   b<br> 3   c</p>\n<p>我们将两个文件关联到一起只需要一下操作<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">join  a.txt b.txt</div><div class=\"line\"></div><div class=\"line\">1 22 a</div><div class=\"line\">2 33 b</div><div class=\"line\">3 44 c</div></pre></td></tr></table></figure></p>\n<p>按列关联在一起哪个文件在前面那个列就在前面</p>\n<h1 id=\"paste-合并\"><a href=\"#paste-合并\" class=\"headerlink\" title=\"paste 合并\"></a>paste 合并</h1><p>比如我们有上面两个文件用paste来合并结果如下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">paste a.txt b.txt</div><div class=\"line\"></div><div class=\"line\">1 22     1   a</div><div class=\"line\">2 33     2   b</div><div class=\"line\">3 44     3   c</div></pre></td></tr></table></figure></p>\n<p>paste也可以通过-s将文件按行来合并<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">paste -s a.txt  b.txt </div><div class=\"line\">1 22    2 33    3 44</div><div class=\"line\">1 a     2 b     3 c</div></pre></td></tr></table></figure></p>\n<p>paste-选项可以格式化输出内容，即对每一个（-），从标准输入中读一次数据。<br>join和paste都有其他参数比如下面会将两个文件以冒号分连接在一起：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">paste -d: a.txt b.txt</div><div class=\"line\"></div><div class=\"line\">1 22:1   a</div><div class=\"line\">2 33:2   b</div><div class=\"line\">3 44:3   c</div></pre></td></tr></table></figure>\n<h1 id=\"cut-切分\"><a href=\"#cut-切分\" class=\"headerlink\" title=\"cut 切分\"></a>cut 切分</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">cut -d&quot; &quot; -f 1 a.txt </div><div class=\"line\">1 </div><div class=\"line\">2 </div><div class=\"line\">3</div></pre></td></tr></table></figure>\n<p>f可以指定输出切分的字段，d可以指定字段分隔符</p>\n<h1 id=\"sort-排序\"><a href=\"#sort-排序\" class=\"headerlink\" title=\"sort 排序\"></a>sort 排序</h1><p>sort 可以对文件中的记录按照一定的规则排序<br>c.txt<br>1<br>3<br>5<br>7<br>11<br>2<br>4<br>6<br>10<br>8<br>9<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">sort c.txt </div><div class=\"line\">1</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td></tr></table></figure></p>\n<p>以字母序排列<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">sort -n c.txt </div><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td></tr></table></figure></p>\n<p>按照数字顺序进行排序，sort默认的是生序排列通过 r选项可以逆序排列。<br>其他选项如:<br>-k可以指定排序的字段，<br>-t可以指定字段分割符。<br>-f会将小写字母都转换为大写字母来进行比较，亦即忽略大小写<br>-c会检查文件是否已排好序，如果乱序，则输出第一个乱序的行的相关信息，最后返回1<br>-C会检查文件是否已排好序，如果乱序，不输出内容，仅返回1<br>-M会以月份来排序，比如JAN小于FEB等等<br>-b会忽略每一行前面的所有空白部分，从第一个可见字符开始比较。</p>\n<p>uniq 可以去掉文件中重复的列 其实sort的u选项也可以完成同样的操作.</p>\n<h1 id=\"split-和-dd-切分文件\"><a href=\"#split-和-dd-切分文件\" class=\"headerlink\" title=\"split 和 dd 切分文件\"></a>split 和 dd 切分文件</h1><p>split可以将大文件切割成小文件,但是使用起来不是很方便个人用的少不展开说了。</p>\n<p>dd 也可以完成切割我个人觉得他更优雅一些,下面是我对自己在打包docker镜像时的分割操作，当时是由于网站上传文件的大小限制太大了不能上传，所以用dd将其且分成小文件。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">#dd if=ubunt.tar bs=1024 count=97000 skip=0  of=ubuntu.tar.1</div><div class=\"line\">#dd if=ubunt.tar bs=1024 count=97000 skip=97000  of=ubuntu.tar.2</div><div class=\"line\">#cat ubuntu.tar.1 ubuntu.tar.2 &gt; ubuntu.tar</div><div class=\"line\">dd if=f21.tar bs=1024 count=97000 skip=0  of=fedora.tar.1</div><div class=\"line\">dd if=f21.tar bs=1024 count=97000 skip=97000  of=fedora.tar.2</div><div class=\"line\">dd if=f21.tar bs=1024 count=97000 skip=194000  of=fedora.tar.3</div><div class=\"line\">dd if=mysql.tar bs=1024 count=97000 skip=0  of=mysql.tar.1</div><div class=\"line\">dd if=mysql.tar bs=1024 count=97000 skip=97000  of=mysql.tar.2</div><div class=\"line\">dd if=mysql.tar bs=1024 count=97000 skip=194000  of=mysql.tar.</div></pre></td></tr></table></figure></p>\n<p>其中bs表示写出块的大小，count表示写出块的数量，skip表示跳过多少个块，of表示输出文件，if表示输入文件.<br>这几个工具有很多种用法，恰当的使用可以提高效率。</p>\n","excerpt":"<p>工作中几乎每天都需要使用linux，刚开始接触linux觉得很高大上，适合装逼用，后面用着用着发现这东西不仅可以装逼还可以提高工作效率，开源界的各路高手为linux/unix写了非常多实用的小工具，这些工具配合起来使用可以极大的提升我们工作效率，今天主要记录一下join，cut,sort,paste,uniq,split 这几个小工具。<br>","more":"<br>首先跟大家说一下，linux下很多工作归根结底都是对文本的操作，比如我们要找到所有的文本文件，然后找到特定的内容保存。首先查找文本文件用到的是find,通过find我们可以找到很多路径，这些路径我们通过管道传递给grep awk join cut 这些小工具，作进一步的操作，最后将结果重定向到一个文件中保存。find awk grep这几个东西很强大我们后面会记录他的详细用法。</p>\n<h1 id=\"join的用法\"><a href=\"#join的用法\" class=\"headerlink\" title=\"join的用法\"></a>join的用法</h1><p>join 普通文件关联<br>首先看下join的文档<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\">join --help</div><div class=\"line\">Usage: join [OPTION]... FILE1 FILE2</div><div class=\"line\">For each pair of input lines with identical join fields, write a line to</div><div class=\"line\">standard output.  The default join field is the first, delimited</div><div class=\"line\">by whitespace.  When FILE1 or FILE2 (not both) is -, read standard input.</div><div class=\"line\"></div><div class=\"line\">  -a FILENUM        also print unpairable lines from file FILENUM, where</div><div class=\"line\">                      FILENUM is 1 or 2, corresponding to FILE1 or FILE2</div><div class=\"line\">  -e EMPTY          replace missing input fields with EMPTY</div><div class=\"line\">  -i, --ignore-case  ignore differences in case when comparing fields</div><div class=\"line\">  -j FIELD          equivalent to &apos;-1 FIELD -2 FIELD&apos;</div><div class=\"line\">  -o FORMAT         obey FORMAT while constructing output line</div><div class=\"line\">  -t CHAR           use CHAR as input and output field separator</div><div class=\"line\">  -v FILENUM        like -a FILENUM, but suppress joined output lines</div><div class=\"line\">  -1 FIELD          join on this FIELD of file 1</div><div class=\"line\">  -2 FIELD          join on this FIELD of file 2</div><div class=\"line\">  --check-order     check that the input is correctly sorted, even</div><div class=\"line\">                      if all input lines are pairable</div><div class=\"line\">  --nocheck-order   do not check that the input is correctly sorted</div><div class=\"line\">  --header          treat the first line in each file as field headers,</div><div class=\"line\">                      print them without trying to pair them</div><div class=\"line\">      --help     display this help and exit</div><div class=\"line\">      --version  output version information and exit</div></pre></td></tr></table></figure></p>\n<p>上面这个 -1 -2 可以指定以那个字段为KEY来关联，这个也可以用-j来指定<br>比如我们有两个文件a.txt,b.txt<br>a.txt<br>1 22<br>2 33<br>3 44            </p>\n<p> b.txt<br> 1   a<br> 2   b<br> 3   c</p>\n<p>我们将两个文件关联到一起只需要一下操作<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">join  a.txt b.txt</div><div class=\"line\"></div><div class=\"line\">1 22 a</div><div class=\"line\">2 33 b</div><div class=\"line\">3 44 c</div></pre></td></tr></table></figure></p>\n<p>按列关联在一起哪个文件在前面那个列就在前面</p>\n<h1 id=\"paste-合并\"><a href=\"#paste-合并\" class=\"headerlink\" title=\"paste 合并\"></a>paste 合并</h1><p>比如我们有上面两个文件用paste来合并结果如下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">paste a.txt b.txt</div><div class=\"line\"></div><div class=\"line\">1 22     1   a</div><div class=\"line\">2 33     2   b</div><div class=\"line\">3 44     3   c</div></pre></td></tr></table></figure></p>\n<p>paste也可以通过-s将文件按行来合并<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">paste -s a.txt  b.txt </div><div class=\"line\">1 22    2 33    3 44</div><div class=\"line\">1 a     2 b     3 c</div></pre></td></tr></table></figure></p>\n<p>paste-选项可以格式化输出内容，即对每一个（-），从标准输入中读一次数据。<br>join和paste都有其他参数比如下面会将两个文件以冒号分连接在一起：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">paste -d: a.txt b.txt</div><div class=\"line\"></div><div class=\"line\">1 22:1   a</div><div class=\"line\">2 33:2   b</div><div class=\"line\">3 44:3   c</div></pre></td></tr></table></figure>\n<h1 id=\"cut-切分\"><a href=\"#cut-切分\" class=\"headerlink\" title=\"cut 切分\"></a>cut 切分</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">cut -d&quot; &quot; -f 1 a.txt </div><div class=\"line\">1 </div><div class=\"line\">2 </div><div class=\"line\">3</div></pre></td></tr></table></figure>\n<p>f可以指定输出切分的字段，d可以指定字段分隔符</p>\n<h1 id=\"sort-排序\"><a href=\"#sort-排序\" class=\"headerlink\" title=\"sort 排序\"></a>sort 排序</h1><p>sort 可以对文件中的记录按照一定的规则排序<br>c.txt<br>1<br>3<br>5<br>7<br>11<br>2<br>4<br>6<br>10<br>8<br>9<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">sort c.txt </div><div class=\"line\">1</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td></tr></table></figure></p>\n<p>以字母序排列<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">sort -n c.txt </div><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td></tr></table></figure></p>\n<p>按照数字顺序进行排序，sort默认的是生序排列通过 r选项可以逆序排列。<br>其他选项如:<br>-k可以指定排序的字段，<br>-t可以指定字段分割符。<br>-f会将小写字母都转换为大写字母来进行比较，亦即忽略大小写<br>-c会检查文件是否已排好序，如果乱序，则输出第一个乱序的行的相关信息，最后返回1<br>-C会检查文件是否已排好序，如果乱序，不输出内容，仅返回1<br>-M会以月份来排序，比如JAN小于FEB等等<br>-b会忽略每一行前面的所有空白部分，从第一个可见字符开始比较。</p>\n<p>uniq 可以去掉文件中重复的列 其实sort的u选项也可以完成同样的操作.</p>\n<h1 id=\"split-和-dd-切分文件\"><a href=\"#split-和-dd-切分文件\" class=\"headerlink\" title=\"split 和 dd 切分文件\"></a>split 和 dd 切分文件</h1><p>split可以将大文件切割成小文件,但是使用起来不是很方便个人用的少不展开说了。</p>\n<p>dd 也可以完成切割我个人觉得他更优雅一些,下面是我对自己在打包docker镜像时的分割操作，当时是由于网站上传文件的大小限制太大了不能上传，所以用dd将其且分成小文件。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">#dd if=ubunt.tar bs=1024 count=97000 skip=0  of=ubuntu.tar.1</div><div class=\"line\">#dd if=ubunt.tar bs=1024 count=97000 skip=97000  of=ubuntu.tar.2</div><div class=\"line\">#cat ubuntu.tar.1 ubuntu.tar.2 &gt; ubuntu.tar</div><div class=\"line\">dd if=f21.tar bs=1024 count=97000 skip=0  of=fedora.tar.1</div><div class=\"line\">dd if=f21.tar bs=1024 count=97000 skip=97000  of=fedora.tar.2</div><div class=\"line\">dd if=f21.tar bs=1024 count=97000 skip=194000  of=fedora.tar.3</div><div class=\"line\">dd if=mysql.tar bs=1024 count=97000 skip=0  of=mysql.tar.1</div><div class=\"line\">dd if=mysql.tar bs=1024 count=97000 skip=97000  of=mysql.tar.2</div><div class=\"line\">dd if=mysql.tar bs=1024 count=97000 skip=194000  of=mysql.tar.</div></pre></td></tr></table></figure></p>\n<p>其中bs表示写出块的大小，count表示写出块的数量，skip表示跳过多少个块，of表示输出文件，if表示输入文件.<br>这几个工具有很多种用法，恰当的使用可以提高效率。</p>"},{"title":"spark 源码分析之 sparksql DataSet","date":"2017-03-10T16:00:00.000Z","_content":"记录一下sparksql的dataframe 中常用的操作，spark在大数据处理方面有很广泛的应供，每天都在研究spark的源码，简单记录一下以便后续查阅,今天先简单整理一下，后续逐步完善.\n版本:spark 2.0.1\n<!-- more -->\n## 数据显示\n这个showString 是spark内部的方法，我们实际是调用不到的，但是我们调用的show方法最终都是调用了这个showString\n```\n  /**\n   * Compose the string representing rows for output\n   *\n   * @param _numRows Number of rows to show\n   * @param truncate If set to more than 0, truncates strings to `truncate` characters and\n   *                   all cells will be aligned right.\n   */\n  private[sql] def showString(_numRows: Int, truncate: Int = 20): String \n```\n## 将dataSet转换成dataFrame\n   datafram其实是按列来存储的dataset\n```\n  /**\n   * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed.\n   * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with\n   * meaningful names. For example:\n   * {{{\n   *   val rdd: RDD[(Int, String)] = ...\n   *   rdd.toDF()  // this implicit conversion creates a DataFrame with column name `_1` and `_2`\n   *   rdd.toDF(\"id\", \"name\")  // this creates a DataFrame with column name \"id\" and \"name\"\n   * }}}\n   *\n   * @group basic\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def toDF(colNames: String*): DataFrame = {\n    require(schema.size == colNames.size,\n      \"The number of columns doesn't match.\\n\" +\n        s\"Old column names (${schema.size}): \" + schema.fields.map(_.name).mkString(\", \") + \"\\n\" +\n        s\"New column names (${colNames.size}): \" + colNames.mkString(\", \"))\n\n    val newCols = logicalPlan.output.zip(colNames).map { case (oldAttribute, newName) =>\n      Column(oldAttribute).as(newName)\n    }\n    select(newCols : _*)\n  }\n```\n输出当前dataset的结构信息\n```\n  /**\n   * Returns the schema of this Dataset.\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  def schema: StructType = queryExecution.analyzed.schema\n\n  /**\n   * Prints the schema to the console in a nice tree format.\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  // scalastyle:off println\n  def printSchema(): Unit = println(schema.treeString)\n  // scalastyle:on println\n```\n输出一些调试信息\n```\n  /**\n   * Prints the plans (logical and physical) to the console for debugging purposes.\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  def explain(extended: Boolean): Unit = {\n    val explain = ExplainCommand(queryExecution.logical, extended = extended)\n    sparkSession.sessionState.executePlan(explain).executedPlan.executeCollect().foreach {\n      // scalastyle:off println\n      r => println(r.getString(0))\n      // scalastyle:on println\n    }\n  }\n\n  /**\n   * Prints the physical plan to the console for debugging purposes.\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  def explain(): Unit = explain(extended = false)\n```\n输出列名以及每个列的类型\n```\n  /**\n   * Returns all column names and their data types as an array.\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  def dtypes: Array[(String, String)] = schema.fields.map { field =>\n    (field.name, field.dataType.toString)\n  }\n\n  /**\n   * Returns all column names as an array.\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  def columns: Array[String] = schema.fields.map(_.name)\n```\n是否能够获取数据\n```\n  /**\n   * Returns true if the `collect` and `take` methods can be run locally\n   * (without any Spark executors).\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation]\n\n  /**\n   * Returns true if this Dataset contains one or more sources that continuously\n   * return data as it arrives. A Dataset that reads data from a streaming source\n   * must be executed as a `StreamingQuery` using the `start()` method in\n   * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or\n   * `collect()`, will throw an [[AnalysisException]] when there is a streaming\n   * source present.\n   *\n   * @group streaming\n   * @since 2.0.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def isStreaming: Boolean = logicalPlan.isStreaming\n```\n**检查点，以前没有用过，需要在研究一下**\n```\n  /**\n   * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate\n   * the logical plan of this Dataset, which is especially useful in iterative algorithms where the\n   * plan may grow exponentially. It will be saved to files inside the checkpoint\n   * directory set with `SparkContext#setCheckpointDir`.\n   *\n   * @group basic\n   * @since 2.1.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def checkpoint(): Dataset[T] = checkpoint(eager = true)\n\n  /**\n   * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the\n   * logical plan of this Dataset, which is especially useful in iterative algorithms where the\n   * plan may grow exponentially. It will be saved to files inside the checkpoint\n   * directory set with `SparkContext#setCheckpointDir`.\n   *\n   * @group basic\n   * @since 2.1.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def checkpoint(eager: Boolean): Dataset[T] = {\n    val internalRdd = queryExecution.toRdd.map(_.copy())\n    internalRdd.checkpoint()\n\n    if (eager) {\n      internalRdd.count()\n    }\n\n    val physicalPlan = queryExecution.executedPlan\n```\n这个什么鬼需要在分析一下\n```\n    // Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the\n    // size of `PartitioningCollection` may grow exponentially for queries involving deep inner\n    // joins.\n    def firstLeafPartitioning(partitioning: Partitioning): Partitioning = {\n      partitioning match {\n        case p: PartitioningCollection => firstLeafPartitioning(p.partitionings.head)\n        case p => p\n      }\n    }\n\n    val outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning)\n\n    Dataset.ofRows(\n      sparkSession,\n      LogicalRDD(\n        logicalPlan.output,\n        internalRdd,\n        outputPartitioning,\n        physicalPlan.outputOrdering\n      )(sparkSession)).as[T]\n  }\n```\n水印？？？\n```\n  /**\n   * :: Experimental ::\n   * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time\n   * before which we assume no more late data is going to arrive.\n   *\n   * Spark will use this watermark for several purposes:\n   *  - To know when a given time window aggregation can be finalized and thus can be emitted when\n   *    using output modes that do not allow updates.\n   *  - To minimize the amount of state that we need to keep for on-going aggregations,\n   *    `mapGroupsWithState` and `dropDuplicates` operators.\n   *\n   *  The current watermark is computed by looking at the `MAX(eventTime)` seen across\n   *  all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\n   *  of coordinating this value across partitions, the actual watermark used is only guaranteed\n   *  to be at least `delayThreshold` behind the actual event time.  In some cases we may still\n   *  process records that arrive more than `delayThreshold` late.\n   *\n   * @param eventTime the name of the column that contains the event time of the row.\n   * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest\n   *                       record that has been processed in the form of an interval\n   *                       (e.g. \"1 minute\" or \"5 hours\").\n   *\n   * @group streaming\n   * @since 2.1.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  // We only accept an existing column name, not a derived column here as a watermark that is\n  // defined on a derived column cannot referenced elsewhere in the plan.\n  def withWatermark(eventTime: String, delayThreshold: String): Dataset[T] = withTypedPlan {\n    val parsedDelay =\n      Option(CalendarInterval.fromString(\"interval \" + delayThreshold))\n        .getOrElse(throw new AnalysisException(s\"Unable to parse time delay '$delayThreshold'\"))\n    EventTimeWatermark(UnresolvedAttribute(eventTime), parsedDelay, logicalPlan)\n  }\n```\n## 数据展示\n```\n  /**\n   * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated,\n   * and all cells will be aligned right. For example:\n   * {{{\n   *   year  month AVG('Adj Close) MAX('Adj Close)\n   *   1980  12    0.503218        0.595103\n   *   1981  01    0.523289        0.570307\n   *   1982  02    0.436504        0.475256\n   *   1983  03    0.410516        0.442194\n   *   1984  04    0.450090        0.483521\n   * }}}\n   *\n   * @param numRows Number of rows to show\n   *\n   * @group action\n   * @since 1.6.0\n   */\n显示datafram中的指定数量的数据，默认字段长度超过20位则截断。\n  def show(numRows: Int): Unit = show(numRows, truncate = true)\n\n显示datafram的数据，默认取前面20条记录显示，默认字段长度超过20位则截断。\n  \n  def show(): Unit = show(20)\n\n显示datafram的数据，默认取前面20条记录显示，通过truncate选择是否需要全部显示每一列的信息。\n  def show(truncate: Boolean): Unit = show(20, truncate)\n\n显示datafram的数据，numRows为显示数量，通过truncate选择是否需要全部显示每一列的信息。\n  def show(numRows: Int, truncate: Boolean): Unit = if (truncate) {\n  def show(numRows: Int, truncate: Int): Unit = println(showString(numRows, truncate))\n```\n## 数据的关联\n```\n\n返回dataset中空值操作算子\n  def na: DataFrameNaFunctions = new DataFrameNaFunctions(toDF())\n\n返回dataset中统计操作算子\n  def stat: DataFrameStatFunctions = new DataFrameStatFunctions(toDF())\n关联dataframe\n  def join(right: Dataset[_]): DataFrame = withPlan {\n    Join(logicalPlan, right.logicalPlan, joinType = Inner, None)\n  }\n\n  /**\n   * Inner equi-join with another `DataFrame` using the given column.\n   *\n   * Different from other join functions, the join column will only appear once in the output,\n   * i.e. similar to SQL's `JOIN USING` syntax.\n   *\n   * {{{\n   *   // Joining df1 and df2 using the column \"user_id\"\n   *   df1.join(df2, \"user_id\")\n   * }}}\n   *\n   * @param right Right side of the join operation.\n   * @param usingColumn Name of the column to join on. This column must exist on both sides.\n   *\n   * @note If you perform a self-join using this function without aliasing the input\n   * `DataFrame`s, you will NOT be able to reference any columns after the join, since\n   * there is no way to disambiguate which side of the join you would like to reference.\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n指定字段关联\n  def join(right: Dataset[_], usingColumn: String): DataFrame = {\n    join(right, Seq(usingColumn))\n  }\n\n  /**\n   * Inner equi-join with another `DataFrame` using the given columns.\n   *\n   * Different from other join functions, the join columns will only appear once in the output,\n   * i.e. similar to SQL's `JOIN USING` syntax.\n   *\n   * {{{\n   *   // Joining df1 and df2 using the columns \"user_id\" and \"user_name\"\n   *   df1.join(df2, Seq(\"user_id\", \"user_name\"))\n   * }}}\n   *\n   * @param right Right side of the join operation.\n   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.\n   *\n   * @note If you perform a self-join using this function without aliasing the input\n   * `DataFrame`s, you will NOT be able to reference any columns after the join, since\n   * there is no way to disambiguate which side of the join you would like to reference.\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n指定关联字段，不同dataframe中同名字段的key不重复出现\n  def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame = {\n    join(right, usingColumns, \"inner\")\n  }\n\n  /**\n   * Equi-join with another `DataFrame` using the given columns. A cross join with a predicate\n   * is specified as an inner join. If you would explicitly like to perform a cross join use the\n   * `crossJoin` method.\n   *\n   * Different from other join functions, the join columns will only appear once in the output,\n   * i.e. similar to SQL's `JOIN USING` syntax.\n   *\n   * @param right Right side of the join operation.\n   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.\n   * @param joinType Type of join to perform. Default `inner`. Must be one of:\n   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,\n   *                 `right`, `right_outer`, `left_semi`, `left_anti`.\n   *\n   * @note If you perform a self-join using this function without aliasing the input\n   * `DataFrame`s, you will NOT be able to reference any columns after the join, since\n   * there is no way to disambiguate which side of the join you would like to reference.\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n指定关联的类型\n  def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame = {\n    // Analyze the self join. The assumption is that the analyzer will disambiguate left vs right\n    // by creating a new instance for one of the branch.\n    val joined = sparkSession.sessionState.executePlan(\n      Join(logicalPlan, right.logicalPlan, joinType = JoinType(joinType), None))\n      .analyzed.asInstanceOf[Join]\n\n    withPlan {\n      Join(\n        joined.left,\n        joined.right,\n        UsingJoin(JoinType(joinType), usingColumns),\n        None)\n    }\n  }\n\n  /**\n   * Inner join with another `DataFrame`, using the given join expression.\n   *\n   * {{{\n   *   // The following two are equivalent:\n   *   df1.join(df2, $\"df1Key\" === $\"df2Key\")\n   *   df1.join(df2).where($\"df1Key\" === $\"df2Key\")\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n通过表达式进行关联\n  def join(right: Dataset[_], joinExprs: Column): DataFrame = join(right, joinExprs, \"inner\")\n\n  /**\n   * Join with another `DataFrame`, using the given join expression. The following performs\n   * a full outer join between `df1` and `df2`.\n   *\n   * {{{\n   *   // Scala:\n   *   import org.apache.spark.sql.functions._\n   *   df1.join(df2, $\"df1Key\" === $\"df2Key\", \"outer\")\n   *\n   *   // Java:\n   *   import static org.apache.spark.sql.functions.*;\n   *   df1.join(df2, col(\"df1Key\").equalTo(col(\"df2Key\")), \"outer\");\n   * }}}\n   *\n   * @param right Right side of the join.\n   * @param joinExprs Join expression.\n   * @param joinType Type of join to perform. Default `inner`. Must be one of:\n   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,\n   *                 `right`, `right_outer`, `left_semi`, `left_anti`.\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n通过列名表达式进行关联\n  def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame = {\n    // Note that in this function, we introduce a hack in the case of self-join to automatically\n    // resolve ambiguous join conditions into ones that might make sense [SPARK-6231].\n    // Consider this case: df.join(df, df(\"key\") === df(\"key\"))\n    // Since df(\"key\") === df(\"key\") is a trivially true condition, this actually becomes a\n    // cartesian join. However, most likely users expect to perform a self join using \"key\".\n    // With that assumption, this hack turns the trivially true condition into equality on join\n    // keys that are resolved to both sides.\n\n    // Trigger analysis so in the case of self-join, the analyzer will clone the plan.\n    // After the cloning, left and right side will have distinct expression ids.\n    val plan = withPlan(\n      Join(logicalPlan, right.logicalPlan, JoinType(joinType), Some(joinExprs.expr)))\n      .queryExecution.analyzed.asInstanceOf[Join]\n\n    // If auto self join alias is disabled, return the plan.\n    if (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) {\n      return withPlan(plan)\n    }\n\n    // If left/right have no output set intersection, return the plan.\n    val lanalyzed = withPlan(this.logicalPlan).queryExecution.analyzed\n    val ranalyzed = withPlan(right.logicalPlan).queryExecution.analyzed\n    if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) {\n      return withPlan(plan)\n    }\n\n    // Otherwise, find the trivially true predicates and automatically resolves them to both sides.\n    // By the time we get here, since we have already run analysis, all attributes should've been\n    // resolved and become AttributeReference.\n    val cond = plan.condition.map { _.transform {\n      case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference)\n          if a.sameRef(b) =>\n        catalyst.expressions.EqualTo(\n          withPlan(plan.left).resolve(a.name),\n          withPlan(plan.right).resolve(b.name))\n    }}\n\n    withPlan {\n      plan.copy(condition = cond)\n    }\n  }\n\n  /**\n   * Explicit cartesian join with another `DataFrame`.\n   *\n   * @param right Right side of the join operation.\n   *\n   * @note Cartesian joins are very expensive without an extra filter that can be pushed down.\n   *\n   * @group untypedrel\n   * @since 2.1.0\n   */\n全表关联\n  def crossJoin(right: Dataset[_]): DataFrame = withPlan {\n    Join(logicalPlan, right.logicalPlan, joinType = Cross, None)\n  }\n\n  /**\n   * :: Experimental ::\n   * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to\n   * true.\n   *\n   * This is similar to the relation `join` function with one important difference in the\n   * result schema. Since `joinWith` preserves objects present on either side of the join, the\n   * result schema is similarly nested into a tuple under the column names `_1` and `_2`.\n   *\n   * This type of join can be useful both for preserving type-safety with the original object\n   * types as well as working with relational data where either side of the join has column\n   * names in common.\n   *\n   * @param other Right side of the join.\n   * @param condition Join expression.\n   * @param joinType Type of join to perform. Default `inner`. Must be one of:\n   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,\n   *                 `right`, `right_outer`, `left_semi`, `left_anti`.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n一种特殊的关联，得到的结果集的结构不同于普通的关联结果\n  @Experimental\n  @InterfaceStability.Evolving\n  def joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = {\n    // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved,\n    // etc.\n    val joined = sparkSession.sessionState.executePlan(\n      Join(\n        this.logicalPlan,\n        other.logicalPlan,\n        JoinType(joinType),\n        Some(condition.expr))).analyzed.asInstanceOf[Join]\n\n    // For both join side, combine all outputs into a single column and alias it with \"_1\" or \"_2\",\n    // to match the schema for the encoder of the join result.\n    // Note that we do this before joining them, to enable the join operator to return null for one\n    // side, in cases like outer-join.\n    val left = {\n      val combined = if (this.exprEnc.flat) {\n        assert(joined.left.output.length == 1)\n        Alias(joined.left.output.head, \"_1\")()\n      } else {\n        Alias(CreateStruct(joined.left.output), \"_1\")()\n      }\n      Project(combined :: Nil, joined.left)\n    }\n\n    val right = {\n      val combined = if (other.exprEnc.flat) {\n        assert(joined.right.output.length == 1)\n        Alias(joined.right.output.head, \"_2\")()\n      } else {\n        Alias(CreateStruct(joined.right.output), \"_2\")()\n      }\n      Project(combined :: Nil, joined.right)\n    }\n\n    // Rewrites the join condition to make the attribute point to correct column/field, after we\n    // combine the outputs of each join side.\n    val conditionExpr = joined.condition.get transformUp {\n      case a: Attribute if joined.left.outputSet.contains(a) =>\n        if (this.exprEnc.flat) {\n          left.output.head\n        } else {\n          val index = joined.left.output.indexWhere(_.exprId == a.exprId)\n          GetStructField(left.output.head, index)\n        }\n      case a: Attribute if joined.right.outputSet.contains(a) =>\n        if (other.exprEnc.flat) {\n          right.output.head\n        } else {\n          val index = joined.right.output.indexWhere(_.exprId == a.exprId)\n          GetStructField(right.output.head, index)\n        }\n    }\n\n    implicit val tuple2Encoder: Encoder[(T, U)] =\n      ExpressionEncoder.tuple(this.exprEnc, other.exprEnc)\n\n    withTypedPlan(Join(left, right, joined.joinType, Some(conditionExpr)))\n  }\n\n  /**\n   * :: Experimental ::\n   * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair\n   * where `condition` evaluates to true.\n   *\n   * @param other Right side of the join.\n   * @param condition Join expression.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = {\n    joinWith(other, condition, \"inner\")\n  }\n```\n## 排序分组\n```\n  /**\n   * Returns a new Dataset with each partition sorted by the given expressions.\n   *\n   * This is the same operation as \"SORT BY\" in SQL (Hive QL).\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n根据指定字段对每个分区进行排序\n  @scala.annotation.varargs\n  def sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T] = {\n    sortWithinPartitions((sortCol +: sortCols).map(Column(_)) : _*)\n  }\n\n  /**\n   * Returns a new Dataset with each partition sorted by the given expressions.\n   *\n   * This is the same operation as \"SORT BY\" in SQL (Hive QL).\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n\n根据指定列对每个分区进行排序\n  @scala.annotation.varargs\n  def sortWithinPartitions(sortExprs: Column*): Dataset[T] = {\n    sortInternal(global = false, sortExprs)\n  }\n\n  /**\n   * Returns a new Dataset sorted by the specified column, all in ascending order.\n   * {{{\n   *   // The following 3 are equivalent\n   *   ds.sort(\"sortcol\")\n   *   ds.sort($\"sortcol\")\n   *   ds.sort($\"sortcol\".asc)\n   * }}}\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def sort(sortCol: String, sortCols: String*): Dataset[T] = {\n    sort((sortCol +: sortCols).map(apply) : _*)\n  }\n\n  /**\n   * Returns a new Dataset sorted by the given expressions. For example:\n   * {{{\n   *   ds.sort($\"col1\", $\"col2\".desc)\n   * }}}\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def sort(sortExprs: Column*): Dataset[T] = {\n    sortInternal(global = true, sortExprs)\n  }\n\n  /**\n   * Returns a new Dataset sorted by the given expressions.\n   * This is an alias of the `sort` function.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def orderBy(sortCol: String, sortCols: String*): Dataset[T] = sort(sortCol, sortCols : _*)\n\n  /**\n   * Returns a new Dataset sorted by the given expressions.\n   * This is an alias of the `sort` function.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def orderBy(sortExprs: Column*): Dataset[T] = sort(sortExprs : _*)\n```\n提取指定的列\n```\n  /**\n   * Selects column based on the column name and return it as a [[Column]].\n   *\n   * @note The column name can also reference to a nested column like `a.b`.\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  def apply(colName: String): Column = col(colName)\n  /**\n   * Selects column based on the column name and return it as a [[Column]].\n   *\n   * @note The column name can also reference to a nested column like `a.b`.\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  def col(colName: String): Column = colName match {\n    case \"*\" =>\n      Column(ResolvedStar(queryExecution.analyzed.output))\n    case _ =>\n      val expr = resolve(colName)\n      Column(expr)\n  }\n\n```\n## 别名\n别名有给列取别名的也有给dataset取别名的这里是给当前dataset取别名\n ```\n\n  /**\n   * Returns a new Dataset with an alias set.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  def as(alias: String): Dataset[T] = withTypedPlan {\n    SubqueryAlias(alias, logicalPlan, None)\n  }\n\n  /**\n   * (Scala-specific) Returns a new Dataset with an alias set.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  def as(alias: Symbol): Dataset[T] = as(alias.name)\n\n  /**\n   * Returns a new Dataset with an alias set. Same as `as`.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  def alias(alias: String): Dataset[T] = as(alias)\n\n  /**\n   * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  def alias(alias: Symbol): Dataset[T] = as(alias)\n```\n## 查询\n查询有很多种接口使用的方式不太一样\n```\n  /**\n   * Selects a set of column based expressions.\n   * {{{\n   *   ds.select($\"colA\", $\"colB\" + 1)\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def select(cols: Column*): DataFrame = withPlan {\n    Project(cols.map(_.named), logicalPlan)\n  }\n\n  /**\n   * Selects a set of columns. This is a variant of `select` that can only select\n   * existing columns using column names (i.e. cannot construct expressions).\n   *\n   * {{{\n   *   // The following two are equivalent:\n   *   ds.select(\"colA\", \"colB\")\n   *   ds.select($\"colA\", $\"colB\")\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def select(col: String, cols: String*): DataFrame = select((col +: cols).map(Column(_)) : _*)\n\n  /**\n   * Selects a set of SQL expressions. This is a variant of `select` that accepts\n   * SQL expressions.\n   *\n   * {{{\n   *   // The following are equivalent:\n   *   ds.selectExpr(\"colA\", \"colB as newName\", \"abs(colC)\")\n   *   ds.select(expr(\"colA\"), expr(\"colB as newName\"), expr(\"abs(colC)\"))\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n按照表达式来查询\n  @scala.annotation.varargs\n  def selectExpr(exprs: String*): DataFrame = {\n    select(exprs.map { expr =>\n      Column(sparkSession.sessionState.sqlParser.parseExpression(expr))\n    }: _*)\n  }\n\n  /**\n   * :: Experimental ::\n   * Returns a new Dataset by computing the given [[Column]] expression for each element.\n   *\n   * {{{\n   *   val ds = Seq(1, 2, 3).toDS()\n   *   val newDS = ds.select(expr(\"value + 1\").as[Int])\n   * }}}\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def select[U1](c1: TypedColumn[T, U1]): Dataset[U1] = {\n    implicit val encoder = c1.encoder\n    val project = Project(c1.withInputType(exprEnc, logicalPlan.output).named :: Nil,\n      logicalPlan)\n\n    if (encoder.flat) {\n      new Dataset[U1](sparkSession, project, encoder)\n    } else {\n      // Flattens inner fields of U1\n      new Dataset[Tuple1[U1]](sparkSession, project, ExpressionEncoder.tuple(encoder)).map(_._1)\n    }\n  }\n\n  /**\n   * Internal helper function for building typed selects that return tuples. For simplicity and\n   * code reuse, we do this without the help of the type system and then use helper functions\n   * that cast appropriately for the user facing interface.\n   */\n  ???这个查询怎么用\n  protected def selectUntyped(columns: TypedColumn[_, _]*): Dataset[_] = {\n    val encoders = columns.map(_.encoder)\n    val namedColumns =\n      columns.map(_.withInputType(exprEnc, logicalPlan.output).named)\n    val execution = new QueryExecution(sparkSession, Project(namedColumns, logicalPlan))\n    new Dataset(sparkSession, execution, ExpressionEncoder.tuple(encoders))\n  }\n\n  /**\n   * :: Experimental ::\n   * Returns a new Dataset by computing the given [[Column]] expressions for each element.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)] =\n    selectUntyped(c1, c2).asInstanceOf[Dataset[(U1, U2)]]\n\n  /**\n   * :: Experimental ::\n   * Returns a new Dataset by computing the given [[Column]] expressions for each element.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def select[U1, U2, U3](\n      c1: TypedColumn[T, U1],\n      c2: TypedColumn[T, U2],\n      c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)] =\n    selectUntyped(c1, c2, c3).asInstanceOf[Dataset[(U1, U2, U3)]]\n\n  /**\n   * :: Experimental ::\n   * Returns a new Dataset by computing the given [[Column]] expressions for each element.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def select[U1, U2, U3, U4](\n      c1: TypedColumn[T, U1],\n      c2: TypedColumn[T, U2],\n      c3: TypedColumn[T, U3],\n      c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)] =\n    selectUntyped(c1, c2, c3, c4).asInstanceOf[Dataset[(U1, U2, U3, U4)]]\n\n  /**\n   * :: Experimental ::\n   * Returns a new Dataset by computing the given [[Column]] expressions for each element.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def select[U1, U2, U3, U4, U5](\n      c1: TypedColumn[T, U1],\n      c2: TypedColumn[T, U2],\n      c3: TypedColumn[T, U3],\n      c4: TypedColumn[T, U4],\n      c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)] =\n    selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[Dataset[(U1, U2, U3, U4, U5)]]\n```\n## 过滤\n过滤，这里的过滤和sql里面的where条件是相同的，查询满足一定条件的记录。\n```\n  /**\n   * Filters rows using the given condition.\n   * {{{\n   *   // The following are equivalent:\n   *   peopleDs.filter($\"age\" > 15)\n   *   peopleDs.where($\"age\" > 15)\n   * }}}\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  def filter(condition: Column): Dataset[T] = withTypedPlan {\n    Filter(condition.expr, logicalPlan)\n  }\n\n  /**\n   * Filters rows using the given SQL expression.\n   * {{{\n   *   peopleDs.filter(\"age > 15\")\n   * }}}\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  def filter(conditionExpr: String): Dataset[T] = {\n    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))\n  }\n\n  /**\n   * Filters rows using the given condition. This is an alias for `filter`.\n   * {{{\n   *   // The following are equivalent:\n   *   peopleDs.filter($\"age\" > 15)\n   *   peopleDs.where($\"age\" > 15)\n   * }}}\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  def where(condition: Column): Dataset[T] = filter(condition)\n\n  /**\n   * Filters rows using the given SQL expression.\n   * {{{\n   *   peopleDs.where(\"age > 15\")\n   * }}}\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  def where(conditionExpr: String): Dataset[T] = {\n    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))\n  }\n```\n## 分组查询\n\n```\n  /**\n   * Groups the Dataset using the specified columns, so we can run aggregation on them. See\n   * [[RelationalGroupedDataset]] for all the available aggregate functions.\n   *\n   * {{{\n   *   // Compute the average for all numeric columns grouped by department.\n   *   ds.groupBy($\"department\").avg()\n   *\n   *   // Compute the max age and average salary, grouped by department and gender.\n   *   ds.groupBy($\"department\", $\"gender\").agg(Map(\n   *     \"salary\" -> \"avg\",\n   *     \"age\" -> \"max\"\n   *   ))\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def groupBy(cols: Column*): RelationalGroupedDataset = {\n    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.GroupByType)\n  }\n\n  /**\n   * Create a multi-dimensional rollup for the current Dataset using the specified columns,\n   * so we can run aggregation on them.\n   * See [[RelationalGroupedDataset]] for all the available aggregate functions.\n   *\n   * {{{\n   *   // Compute the average for all numeric columns rolluped by department and group.\n   *   ds.rollup($\"department\", $\"group\").avg()\n   *\n   *   // Compute the max age and average salary, rolluped by department and gender.\n   *   ds.rollup($\"department\", $\"gender\").agg(Map(\n   *     \"salary\" -> \"avg\",\n   *     \"age\" -> \"max\"\n   *   ))\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  数据归纳\n  @scala.annotation.varargs\n  def rollup(cols: Column*): RelationalGroupedDataset = {\n    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.RollupType)\n  }\n\n  /**\n   * Create a multi-dimensional cube for the current Dataset using the specified columns,\n   * so we can run aggregation on them.\n   * See [[RelationalGroupedDataset]] for all the available aggregate functions.\n   *\n   * {{{\n   *   // Compute the average for all numeric columns cubed by department and group.\n   *   ds.cube($\"department\", $\"group\").avg()\n   *\n   *   // Compute the max age and average salary, cubed by department and gender.\n   *   ds.cube($\"department\", $\"gender\").agg(Map(\n   *     \"salary\" -> \"avg\",\n   *     \"age\" -> \"max\"\n   *   ))\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def cube(cols: Column*): RelationalGroupedDataset = {\n    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.CubeType)\n  }\n\n  /**\n   * Groups the Dataset using the specified columns, so that we can run aggregation on them.\n   * See [[RelationalGroupedDataset]] for all the available aggregate functions.\n   *\n   * This is a variant of groupBy that can only group by existing columns using column names\n   * (i.e. cannot construct expressions).\n   *\n   * {{{\n   *   // Compute the average for all numeric columns grouped by department.\n   *   ds.groupBy(\"department\").avg()\n   *\n   *   // Compute the max age and average salary, grouped by department and gender.\n   *   ds.groupBy($\"department\", $\"gender\").agg(Map(\n   *     \"salary\" -> \"avg\",\n   *     \"age\" -> \"max\"\n   *   ))\n   * }}}\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def groupBy(col1: String, cols: String*): RelationalGroupedDataset = {\n    val colNames: Seq[String] = col1 +: cols\n    RelationalGroupedDataset(\n      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.GroupByType)\n  }\n```\n## reduce操作\n\n  ```\n  /**\n   * :: Experimental ::\n   * (Scala-specific)\n   * Reduces the elements of this Dataset using the specified binary function. The given `func`\n   * must be commutative and associative or the result may be non-deterministic.\n   *\n   * @group action\n   * @since 1.6.0\n   */\n  合并操作\n  @Experimental\n  @InterfaceStability.Evolving\n  def reduce(func: (T, T) => T): T = rdd.reduce(func)\n\n  /**\n   * :: Experimental ::\n   * (Java-specific)\n   * Reduces the elements of this Dataset using the specified binary function. The given `func`\n   * must be commutative and associative or the result may be non-deterministic.\n   *\n   * @group action\n   * @since 1.6.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def reduce(func: ReduceFunction[T]): T = reduce(func.call(_, _))\n\n  /**\n   * :: Experimental ::\n   * (Scala-specific)\n   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  分组                                  \n  @Experimental\n  @InterfaceStability.Evolving\n  def groupByKey[K: Encoder](func: T => K): KeyValueGroupedDataset[K, T] = {\n    val inputPlan = logicalPlan\n    val withGroupingKey = AppendColumns(func, inputPlan)\n    val executed = sparkSession.sessionState.executePlan(withGroupingKey)\n\n    new KeyValueGroupedDataset(\n      encoderFor[K],\n      encoderFor[T],\n      executed,\n      inputPlan.output,\n      withGroupingKey.newColumns)\n  }\n\n  /**\n   * :: Experimental ::\n   * (Java-specific)\n   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T] =\n    groupByKey(func.call(_))(encoder)\n```\n## 数据钻取与聚合操作\n```\n  /**\n   * Create a multi-dimensional rollup for the current Dataset using the specified columns,\n   * so we can run aggregation on them.\n   * See [[RelationalGroupedDataset]] for all the available aggregate functions.\n   *\n   * This is a variant of rollup that can only group by existing columns using column names\n   * (i.e. cannot construct expressions).\n   *\n   * {{{\n   *   // Compute the average for all numeric columns rolluped by department and group.\n   *   ds.rollup(\"department\", \"group\").avg()\n   *\n   *   // Compute the max age and average salary, rolluped by department and gender.\n   *   ds.rollup($\"department\", $\"gender\").agg(Map(\n   *     \"salary\" -> \"avg\",\n   *     \"age\" -> \"max\"\n   *   ))\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def rollup(col1: String, cols: String*): RelationalGroupedDataset = {\n    val colNames: Seq[String] = col1 +: cols\n    RelationalGroupedDataset(\n      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.RollupType)\n  }\n\n  /**\n   * Create a multi-dimensional cube for the current Dataset using the specified columns,\n   * so we can run aggregation on them.\n   * See [[RelationalGroupedDataset]] for all the available aggregate functions.\n   *\n   * This is a variant of cube that can only group by existing columns using column names\n   * (i.e. cannot construct expressions).\n   *\n   * {{{\n   *   // Compute the average for all numeric columns cubed by department and group.\n   *   ds.cube(\"department\", \"group\").avg()\n   *\n   *   // Compute the max age and average salary, cubed by department and gender.\n   *   ds.cube($\"department\", $\"gender\").agg(Map(\n   *     \"salary\" -> \"avg\",\n   *     \"age\" -> \"max\"\n   *   ))\n   * }}}\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def cube(col1: String, cols: String*): RelationalGroupedDataset = {\n    val colNames: Seq[String] = col1 +: cols\n    RelationalGroupedDataset(\n      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.CubeType)\n  }\n\n  /**\n   * (Scala-specific) Aggregates on the entire Dataset without groups.\n   * {{{\n   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)\n   *   ds.agg(\"age\" -> \"max\", \"salary\" -> \"avg\")\n   *   ds.groupBy().agg(\"age\" -> \"max\", \"salary\" -> \"avg\")\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = {\n    groupBy().agg(aggExpr, aggExprs : _*)\n  }\n\n  /**\n   * (Scala-specific) Aggregates on the entire Dataset without groups.\n   * {{{\n   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)\n   *   ds.agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\"))\n   *   ds.groupBy().agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\"))\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  def agg(exprs: Map[String, String]): DataFrame = groupBy().agg(exprs)\n\n  /**\n   * (Java-specific) Aggregates on the entire Dataset without groups.\n   * {{{\n   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)\n   *   ds.agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\"))\n   *   ds.groupBy().agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\"))\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  def agg(exprs: java.util.Map[String, String]): DataFrame = groupBy().agg(exprs)\n\n  /**\n   * Aggregates on the entire Dataset without groups.\n   * {{{\n   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)\n   *   ds.agg(max($\"age\"), avg($\"salary\"))\n   *   ds.groupBy().agg(max($\"age\"), avg($\"salary\"))\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def agg(expr: Column, exprs: Column*): DataFrame = groupBy().agg(expr, exprs : _*)\n\n  /**\n   * Returns a new Dataset by taking the first `n` rows. The difference between this function\n   * and `head` is that `head` is an action and returns an array (by triggering query execution)\n   * while `limit` returns a new Dataset.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  def limit(n: Int): Dataset[T] = withTypedPlan {\n    Limit(Literal(n), logicalPlan)\n  }\n```\n## 集合的交并补接口\n```\n  /**\n   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.\n   * This is equivalent to `UNION ALL` in SQL.\n   *\n   * To do a SQL-style set union (that does deduplication of elements), use this function followed\n   * by a [[distinct]].\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  @deprecated(\"use union()\", \"2.0.0\")\n  def unionAll(other: Dataset[T]): Dataset[T] = union(other)\n\n  /**\n   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.\n   * This is equivalent to `UNION ALL` in SQL.\n   *\n   * To do a SQL-style set union (that does deduplication of elements), use this function followed\n   * by a [[distinct]].\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  def union(other: Dataset[T]): Dataset[T] = withSetOperator {\n    // This breaks caching, but it's usually ok because it addresses a very specific use case:\n    // using union to union many files or partitions.\n    CombineUnions(Union(logicalPlan, other.logicalPlan))\n  }\n\n  /**\n   * Returns a new Dataset containing rows only in both this Dataset and another Dataset.\n   * This is equivalent to `INTERSECT` in SQL.\n   *\n   * @note Equality checking is performed directly on the encoded representation of the data\n   * and thus is not affected by a custom `equals` function defined on `T`.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  def intersect(other: Dataset[T]): Dataset[T] = withSetOperator {\n    Intersect(logicalPlan, other.logicalPlan)\n  }\n\n  /**\n   * Returns a new Dataset containing rows in this Dataset but not in another Dataset.\n   * This is equivalent to `EXCEPT` in SQL.\n   *\n   * @note Equality checking is performed directly on the encoded representation of the data\n   * and thus is not affected by a custom `equals` function defined on `T`.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  取补集\n  def except(other: Dataset[T]): Dataset[T] = withSetOperator {\n    Except(logicalPlan, other.logicalPlan)\n  }\n```\n## 取样与切分\n```\n  /**\n   * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed.\n   *\n   * @param withReplacement Sample with replacement or not.\n   * @param fraction Fraction of rows to generate.\n   * @param seed Seed for sampling.\n   *\n   * @note This is NOT guaranteed to provide exactly the fraction of the count\n   * of the given [[Dataset]].\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = {\n    require(fraction >= 0,\n      s\"Fraction must be nonnegative, but got ${fraction}\")\n\n    withTypedPlan {\n      Sample(0.0, fraction, withReplacement, seed, logicalPlan)()\n    }\n  }\n\n  /**\n   * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed.\n   *\n   * @param withReplacement Sample with replacement or not.\n   * @param fraction Fraction of rows to generate.\n   *\n   * @note This is NOT guaranteed to provide exactly the fraction of the total count\n   * of the given [[Dataset]].\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  随即取样\n  def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = {\n    sample(withReplacement, fraction, Utils.random.nextLong)\n  }\n\n  /**\n   * Randomly splits this Dataset with the provided weights.\n   *\n   * @param weights weights for splits, will be normalized if they don't sum to 1.\n   * @param seed Seed for sampling.\n   *\n   * For Java API, use [[randomSplitAsList]].\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  随即切分???\n  def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]] = {\n    require(weights.forall(_ >= 0),\n      s\"Weights must be nonnegative, but got ${weights.mkString(\"[\", \",\", \"]\")}\")\n    require(weights.sum > 0,\n      s\"Sum of weights must be positive, but got ${weights.mkString(\"[\", \",\", \"]\")}\")\n\n    // It is possible that the underlying dataframe doesn't guarantee the ordering of rows in its\n    // constituent partitions each time a split is materialized which could result in\n    // overlapping splits. To prevent this, we explicitly sort each input partition to make the\n    // ordering deterministic.\n    // MapType cannot be sorted.\n    val sorted = Sort(logicalPlan.output.filterNot(_.dataType.isInstanceOf[MapType])\n      .map(SortOrder(_, Ascending)), global = false, logicalPlan)\n    val sum = weights.sum\n    val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _)\n    normalizedCumWeights.sliding(2).map { x =>\n      new Dataset[T](\n        sparkSession, Sample(x(0), x(1), withReplacement = false, seed, sorted)(), encoder)\n    }.toArray\n  }\n\n  /**\n   * Returns a Java list that contains randomly split Dataset with the provided weights.\n   *\n   * @param weights weights for splits, will be normalized if they don't sum to 1.\n   * @param seed Seed for sampling.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  def randomSplitAsList(weights: Array[Double], seed: Long): java.util.List[Dataset[T]] = {\n    val values = randomSplit(weights, seed)\n    java.util.Arrays.asList(values : _*)\n  }\n\n  /**\n   * Randomly splits this Dataset with the provided weights.\n   *\n   * @param weights weights for splits, will be normalized if they don't sum to 1.\n   * @group typedrel\n   * @since 2.0.0\n   */\n  def randomSplit(weights: Array[Double]): Array[Dataset[T]] = {\n    randomSplit(weights, Utils.random.nextLong)\n  }\n\n  /**\n   * Randomly splits this Dataset with the provided weights. Provided for the Python Api.\n   *\n   * @param weights weights for splits, will be normalized if they don't sum to 1.\n   * @param seed Seed for sampling.\n   */\n  private[spark] def randomSplit(weights: List[Double], seed: Long): Array[Dataset[T]] = {\n    randomSplit(weights.toArray, seed)\n  }\n\n  /**\n   * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more\n   * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of\n   * the input row are implicitly joined with each row that is output by the function.\n   *\n   * Given that this is deprecated, as an alternative, you can explode columns either using\n   * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count\n   * the number of books that contain a given word:\n   *\n   * {{{\n   *   case class Book(title: String, words: String)\n   *   val ds: Dataset[Book]\n   *\n   *   val allWords = ds.select('title, explode(split('words, \" \")).as(\"word\"))\n   *\n   *   val bookCountPerWord = allWords.groupBy(\"word\").agg(countDistinct(\"title\"))\n   * }}}\n   *\n   * Using `flatMap()` this can similarly be exploded as:\n   *\n   * {{{\n   *   ds.flatMap(_.words.split(\" \"))\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  将字段再处理\n  @deprecated(\"use flatMap() or select() with functions.explode() instead\", \"2.0.0\")\n  def explode[A <: Product : TypeTag](input: Column*)(f: Row => TraversableOnce[A]): DataFrame = {\n    val elementSchema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]\n\n    val convert = CatalystTypeConverters.createToCatalystConverter(elementSchema)\n\n    val rowFunction =\n      f.andThen(_.map(convert(_).asInstanceOf[InternalRow]))\n    val generator = UserDefinedGenerator(elementSchema, rowFunction, input.map(_.expr))\n\n    withPlan {\n      Generate(generator, join = true, outer = false,\n        qualifier = None, generatorOutput = Nil, logicalPlan)\n    }\n  }\n\n  /**\n   * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero\n   * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All\n   * columns of the input row are implicitly joined with each value that is output by the function.\n   *\n   * Given that this is deprecated, as an alternative, you can explode columns either using\n   * `functions.explode()`:\n   *\n   * {{{\n   *   ds.select(explode(split('words, \" \")).as(\"word\"))\n   * }}}\n   *\n   * or `flatMap()`:\n   *\n   * {{{\n   *   ds.flatMap(_.words.split(\" \"))\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  @deprecated(\"use flatMap() or select() with functions.explode() instead\", \"2.0.0\")\n  def explode[A, B : TypeTag](inputColumn: String, outputColumn: String)(f: A => TraversableOnce[B])\n    : DataFrame = {\n    val dataType = ScalaReflection.schemaFor[B].dataType\n    val attributes = AttributeReference(outputColumn, dataType)() :: Nil\n    // TODO handle the metadata?\n    val elementSchema = attributes.toStructType\n\n    def rowFunction(row: Row): TraversableOnce[InternalRow] = {\n      val convert = CatalystTypeConverters.createToCatalystConverter(dataType)\n      f(row(0).asInstanceOf[A]).map(o => InternalRow(convert(o)))\n    }\n    val generator = UserDefinedGenerator(elementSchema, rowFunction, apply(inputColumn).expr :: Nil)\n\n    withPlan {\n      Generate(generator, join = true, outer = false,\n        qualifier = None, generatorOutput = Nil, logicalPlan)\n    }\n  }\n## 列操作\n  /**\n   * Returns a new Dataset by adding a column or replacing the existing column that has\n   * the same name.\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  列操作\n  def withColumn(colName: String, col: Column): DataFrame = {\n    val resolver = sparkSession.sessionState.analyzer.resolver\n    val output = queryExecution.analyzed.output\n    val shouldReplace = output.exists(f => resolver(f.name, colName))\n    if (shouldReplace) {\n      val columns = output.map { field =>\n        if (resolver(field.name, colName)) {\n          col.as(colName)\n        } else {\n          Column(field)\n        }\n      }\n      select(columns : _*)\n    } else {\n      select(Column(\"*\"), col.as(colName))\n    }\n  }\n\n  /**\n   * Returns a new Dataset by adding a column with metadata.\n   */\n  private[spark] def withColumn(colName: String, col: Column, metadata: Metadata): DataFrame = {\n    withColumn(colName, col.as(colName, metadata))\n  }\n\n  /**\n   * Returns a new Dataset with a column renamed.\n   * This is a no-op if schema doesn't contain existingName.\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  def withColumnRenamed(existingName: String, newName: String): DataFrame = {\n    val resolver = sparkSession.sessionState.analyzer.resolver\n    val output = queryExecution.analyzed.output\n    val shouldRename = output.exists(f => resolver(f.name, existingName))\n    if (shouldRename) {\n      val columns = output.map { col =>\n        if (resolver(col.name, existingName)) {\n          Column(col).as(newName)\n        } else {\n          Column(col)\n        }\n      }\n      select(columns : _*)\n    } else {\n      toDF()\n    }\n  }\n\n  /**\n   * Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain\n   * column name.\n   *\n   * This method can only be used to drop top level columns. the colName string is treated\n   * literally without further interpretation.\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  删除指定列\n  def drop(colName: String): DataFrame = {\n    drop(Seq(colName) : _*)\n  }\n\n  /**\n   * Returns a new Dataset with columns dropped.\n   * This is a no-op if schema doesn't contain column name(s).\n   *\n   * This method can only be used to drop top level columns. the colName string is treated literally\n   * without further interpretation.\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def drop(colNames: String*): DataFrame = {\n    val resolver = sparkSession.sessionState.analyzer.resolver\n    val allColumns = queryExecution.analyzed.output\n    val remainingCols = allColumns.filter { attribute =>\n      colNames.forall(n => !resolver(attribute.name, n))\n    }.map(attribute => Column(attribute))\n    if (remainingCols.size == allColumns.size) {\n      toDF()\n    } else {\n      this.select(remainingCols: _*)\n    }\n  }\n\n  /**\n   * Returns a new Dataset with a column dropped.\n   * This version of drop accepts a [[Column]] rather than a name.\n   * This is a no-op if the Dataset doesn't have a column\n   * with an equivalent expression.\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  def drop(col: Column): DataFrame = {\n    val expression = col match {\n      case Column(u: UnresolvedAttribute) =>\n        queryExecution.analyzed.resolveQuoted(\n          u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u)\n      case Column(expr: Expression) => expr\n    }\n    val attrs = this.logicalPlan.output\n    val colsAfterDrop = attrs.filter { attr =>\n      attr != expression\n    }.map(attr => Column(attr))\n    select(colsAfterDrop : _*)\n  }\n```\n## 去重\n```\n  /**\n   * Returns a new Dataset that contains only the unique rows from this Dataset.\n   * This is an alias for `distinct`.\n   *\n   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it\n   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use\n   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit\n   * the state. In addition, too late data older than watermark will be dropped to avoid any\n   * possibility of duplicates.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  删除重复的行\n  def dropDuplicates(): Dataset[T] = dropDuplicates(this.columns)\n\n  /**\n   * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only\n   * the subset of columns.\n   *\n   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it\n   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use\n   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit\n   * the state. In addition, too late data older than watermark will be dropped to avoid any\n   * possibility of duplicates.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  def dropDuplicates(colNames: Seq[String]): Dataset[T] = withTypedPlan {\n    val resolver = sparkSession.sessionState.analyzer.resolver\n    val allColumns = queryExecution.analyzed.output\n    val groupCols = colNames.toSet.toSeq.flatMap { (colName: String) =>\n      // It is possibly there are more than one columns with the same name,\n      // so we call filter instead of find.\n      val cols = allColumns.filter(col => resolver(col.name, colName))\n      if (cols.isEmpty) {\n        throw new AnalysisException(\n          s\"\"\"Cannot resolve column name \"$colName\" among (${schema.fieldNames.mkString(\", \")})\"\"\")\n      }\n      cols\n    }\n    Deduplicate(groupCols, logicalPlan, isStreaming)\n  }\n\n  /**\n   * Returns a new Dataset with duplicate rows removed, considering only\n   * the subset of columns.\n   *\n   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it\n   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use\n   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit\n   * the state. In addition, too late data older than watermark will be dropped to avoid any\n   * possibility of duplicates.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  def dropDuplicates(colNames: Array[String]): Dataset[T] = dropDuplicates(colNames.toSeq)\n\n  /**\n   * Returns a new [[Dataset]] with duplicate rows removed, considering only\n   * the subset of columns.\n   *\n   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it\n   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use\n   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit\n   * the state. In addition, too late data older than watermark will be dropped to avoid any\n   * possibility of duplicates.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def dropDuplicates(col1: String, cols: String*): Dataset[T] = {\n    val colNames: Seq[String] = col1 +: cols\n    dropDuplicates(colNames)\n  }\n  ```\n## 统计指定的列\n    ```\n  /**\n   * Computes statistics for numeric and string columns, including count, mean, stddev, min, and\n   * max. If no columns are given, this function computes statistics for all numerical or string\n   * columns.\n   *\n   * This function is meant for exploratory data analysis, as we make no guarantee about the\n   * backward compatibility of the schema of the resulting Dataset. If you want to\n   * programmatically compute summary statistics, use the `agg` function instead.\n   *\n   * {{{\n   *   ds.describe(\"age\", \"height\").show()\n   *\n   *   // output:\n   *   // summary age   height\n   *   // count   10.0  10.0\n   *   // mean    53.3  178.05\n   *   // stddev  11.6  15.7\n   *   // min     18.0  163.0\n   *   // max     92.0  192.0\n   * }}}\n   *\n   * @group action\n   * @since 1.6.0\n   */\n  获得指定列的描述性统计量\n  @scala.annotation.varargs\n  def describe(cols: String*): DataFrame = withPlan {\n\n    // The list of summary statistics to compute, in the form of expressions.\n    val statistics = List[(String, Expression => Expression)](\n      \"count\" -> ((child: Expression) => Count(child).toAggregateExpression()),\n      \"mean\" -> ((child: Expression) => Average(child).toAggregateExpression()),\n      \"stddev\" -> ((child: Expression) => StddevSamp(child).toAggregateExpression()),\n      \"min\" -> ((child: Expression) => Min(child).toAggregateExpression()),\n      \"max\" -> ((child: Expression) => Max(child).toAggregateExpression()))\n\n    val outputCols =\n      (if (cols.isEmpty) aggregatableColumns.map(usePrettyExpression(_).sql) else cols).toList\n\n    val ret: Seq[Row] = if (outputCols.nonEmpty) {\n      val aggExprs = statistics.flatMap { case (_, colToAgg) =>\n        outputCols.map(c => Column(Cast(colToAgg(Column(c).expr), StringType)).as(c))\n      }\n\n      val row = groupBy().agg(aggExprs.head, aggExprs.tail: _*).head().toSeq\n\n      // Pivot the data so each summary is one row\n      row.grouped(outputCols.size).toSeq.zip(statistics).map { case (aggregation, (statistic, _)) =>\n        Row(statistic :: aggregation.toList: _*)\n      }\n    } else {\n      // If there are no output columns, just output a single column that contains the stats.\n      statistics.map { case (name, _) => Row(name) }\n    }\n\n    // All columns are string type\n    val schema = StructType(\n      StructField(\"summary\", StringType) :: outputCols.map(StructField(_, StringType))).toAttributes\n    // `toArray` forces materialization to make the seq serializable\n    LocalRelation.fromExternalRows(schema, ret.toArray.toSeq)\n  }\n\n  /**\n   * Returns the first `n` rows.\n   *\n   * @note this method should only be used if the resulting array is expected to be small, as\n   * all the data is loaded into the driver's memory.\n   *\n   * @group action\n   * @since 1.6.0\n   */\n  取得前n行数据\n  def head(n: Int): Array[T] = withAction(\"head\", limit(n).queryExecution)(collectFromPlan)\n\n  /**\n   * Returns the first row.\n   * @group action\n   * @since 1.6.0\n   */\n  def head(): T = head(1).head\n\n  /**\n   * Returns the first row. Alias for head().\n   * @group action\n   * @since 1.6.0\n   */\n  def first(): T = head()\n\n  /**\n   * Concise syntax for chaining custom transformations.\n   * {{{\n   *   def featurize(ds: Dataset[T]): Dataset[U] = ...\n   *\n   *   ds\n   *     .transform(featurize)\n   *     .transform(...)\n   * }}}\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  转换？？\n  def transform[U](t: Dataset[T] => Dataset[U]): Dataset[U] = t(this)\n\n  /**\n   * :: Experimental ::\n   * (Scala-specific)\n   * Returns a new Dataset that only contains elements where `func` returns `true`.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  过滤\n  @Experimental\n  @InterfaceStability.Evolving\n  def filter(func: T => Boolean): Dataset[T] = {\n    withTypedPlan(TypedFilter(func, logicalPlan))\n  }\n\n  /**\n   * :: Experimental ::\n   * (Java-specific)\n   * Returns a new Dataset that only contains elements where `func` returns `true`.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def filter(func: FilterFunction[T]): Dataset[T] = {\n    withTypedPlan(TypedFilter(func, logicalPlan))\n  }\n```\n## 数据的转换\n  ```\n  /**\n   * :: Experimental ::\n   * (Scala-specific)\n   * Returns a new Dataset that contains the result of applying `func` to each element.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  映射操作\n  @Experimental\n  @InterfaceStability.Evolving\n  def map[U : Encoder](func: T => U): Dataset[U] = withTypedPlan {\n    MapElements[T, U](func, logicalPlan)\n  }\n\n  /**\n   * :: Experimental ::\n   * (Java-specific)\n   * Returns a new Dataset that contains the result of applying `func` to each element.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {\n    implicit val uEnc = encoder\n    withTypedPlan(MapElements[T, U](func, logicalPlan))\n  }\n\n  /**\n   * :: Experimental ::\n   * (Scala-specific)\n   * Returns a new Dataset that contains the result of applying `func` to each partition.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def mapPartitions[U : Encoder](func: Iterator[T] => Iterator[U]): Dataset[U] = {\n    new Dataset[U](\n      sparkSession,\n      MapPartitions[T, U](func, logicalPlan),\n      implicitly[Encoder[U]])\n  }\n\n  /**\n   * :: Experimental ::\n   * (Java-specific)\n   * Returns a new Dataset that contains the result of applying `f` to each partition.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U] = {\n    val func: (Iterator[T]) => Iterator[U] = x => f.call(x.asJava).asScala\n    mapPartitions(func)(encoder)\n  }\n\n  /**\n   * Returns a new `DataFrame` that contains the result of applying a serialized R function\n   * `func` to each partition.\n   */\n  private[sql] def mapPartitionsInR(\n      func: Array[Byte],\n      packageNames: Array[Byte],\n      broadcastVars: Array[Broadcast[Object]],\n      schema: StructType): DataFrame = {\n    val rowEncoder = encoder.asInstanceOf[ExpressionEncoder[Row]]\n    Dataset.ofRows(\n      sparkSession,\n      MapPartitionsInR(func, packageNames, broadcastVars, schema, rowEncoder, logicalPlan))\n  }\n\n  /**\n   * :: Experimental ::\n   * (Scala-specific)\n   * Returns a new Dataset by first applying a function to all elements of this Dataset,\n   * and then flattening the results.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def flatMap[U : Encoder](func: T => TraversableOnce[U]): Dataset[U] =\n    mapPartitions(_.flatMap(func))\n\n  /**\n   * :: Experimental ::\n   * (Java-specific)\n   * Returns a new Dataset by first applying a function to all elements of this Dataset,\n   * and then flattening the results.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {\n    val func: (T) => Iterator[U] = x => f.call(x).asScala\n    flatMap(func)(encoder)\n  }\n\n  /**\n   * Applies a function `f` to all rows.\n   *\n   * @group action\n   * @since 1.6.0\n   */\n  def foreach(f: T => Unit): Unit = withNewExecutionId {\n    rdd.foreach(f)\n  }\n\n  /**\n   * (Java-specific)\n   * Runs `func` on each element of this Dataset.\n   *\n   * @group action\n   * @since 1.6.0\n   */\n  def foreach(func: ForeachFunction[T]): Unit = foreach(func.call(_))\n\n  /**\n   * Applies a function `f` to each partition of this Dataset.\n   *\n   * @group action\n   * @since 1.6.0\n   */\n  def foreachPartition(f: Iterator[T] => Unit): Unit = withNewExecutionId {\n    rdd.foreachPartition(f)\n  }\n```\n## 数据的提取与聚合\n  ```\n  /**\n   * (Java-specific)\n   * Runs `func` on each partition of this Dataset.\n   *\n   * @group action\n   * @since 1.6.0\n   */\n  def foreachPartition(func: ForeachPartitionFunction[T]): Unit =\n    foreachPartition(it => func.call(it.asJava))\n\n  /**\n   * Returns the first `n` rows in the Dataset.\n   *\n   * Running take requires moving data into the application's driver process, and doing so with\n   * a very large `n` can crash the driver process with OutOfMemoryError.\n   *\n   * @group action\n   * @since 1.6.0\n   */\n  def take(n: Int): Array[T] = head(n)\n\n  /**\n   * Returns the first `n` rows in the Dataset as a list.\n   *\n   * Running take requires moving data into the application's driver process, and doing so with\n   * a very large `n` can crash the driver process with OutOfMemoryError.\n   *\n   * @group action\n   * @since 1.6.0\n   */\n    获取数据成一个列表\n  def takeAsList(n: Int): java.util.List[T] = java.util.Arrays.asList(take(n) : _*)\n\n  /**\n   * Returns an array that contains all rows in this Dataset.\n   *\n   * Running collect requires moving all the data into the application's driver process, and\n   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.\n   *\n   * For Java API, use [[collectAsList]].\n   *\n   * @group action\n   * @since 1.6.0\n   */\n\n统计数据\n  def collect(): Array[T] = withAction(\"collect\", queryExecution)(collectFromPlan)\n\n  /**\n   * Returns a Java list that contains all rows in this Dataset.\n   *\n   * Running collect requires moving all the data into the application's driver process, and\n   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.\n   *\n   * @group action\n   * @since 1.6.0\n   */\n  def collectAsList(): java.util.List[T] = withAction(\"collectAsList\", queryExecution) { plan =>\n    val values = collectFromPlan(plan)\n    java.util.Arrays.asList(values : _*)\n  }\n\n  /**\n   * Return an iterator that contains all rows in this Dataset.\n   *\n   * The iterator will consume as much memory as the largest partition in this Dataset.\n   *\n   * @note this results in multiple Spark jobs, and if the input Dataset is the result\n   * of a wide transformation (e.g. join with different partitioners), to avoid\n   * recomputing the input Dataset should be cached first.\n   *\n   * @group action\n   * @since 2.0.0\n   */\n  def toLocalIterator(): java.util.Iterator[T] = {\n    withAction(\"toLocalIterator\", queryExecution) { plan =>\n      plan.executeToIterator().map(boundEnc.fromRow).asJava\n    }\n  }\n\n  /**\n   * Returns the number of rows in the Dataset.\n   * @group action\n   * @since 1.6.0\n   */\n  统计行\n  def count(): Long = withAction(\"count\", groupBy().count().queryExecution) { plan =>\n    plan.executeCollect().head.getLong(0)\n  }\n  ```\n## 分区\n    ```\n  /**\n   * Returns a new Dataset that has exactly `numPartitions` partitions.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  分区\n  def repartition(numPartitions: Int): Dataset[T] = withTypedPlan {\n    Repartition(numPartitions, shuffle = true, logicalPlan)\n  }\n\n  /**\n   * Returns a new Dataset partitioned by the given partitioning expressions into\n   * `numPartitions`. The resulting Dataset is hash partitioned.\n   *\n   * This is the same operation as \"DISTRIBUTE BY\" in SQL (Hive QL).\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T] = withTypedPlan {\n    RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, numPartitions)\n  }\n\n  /**\n   * Returns a new Dataset partitioned by the given partitioning expressions, using\n   * `spark.sql.shuffle.partitions` as number of partitions.\n   * The resulting Dataset is hash partitioned.\n   *\n   * This is the same operation as \"DISTRIBUTE BY\" in SQL (Hive QL).\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def repartition(partitionExprs: Column*): Dataset[T] = withTypedPlan {\n    RepartitionByExpression(\n      partitionExprs.map(_.expr), logicalPlan, sparkSession.sessionState.conf.numShufflePartitions)\n  }\n\n  /**\n   * Returns a new Dataset that has exactly `numPartitions` partitions.\n   * Similar to coalesce defined on an `RDD`, this operation results in a narrow dependency, e.g.\n   * if you go from 1000 partitions to 100 partitions, there will not be a shuffle, instead each of\n   * the 100 new partitions will claim 10 of the current partitions.  If a larger number of\n   * partitions is requested, it will stay at the current number of partitions.\n   *\n   * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,\n   * this may result in your computation taking place on fewer nodes than\n   * you like (e.g. one node in the case of numPartitions = 1). To avoid this,\n   * you can call repartition. This will add a shuffle step, but means the\n   * current upstream partitions will be executed in parallel (per whatever\n   * the current partitioning is).\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  def coalesce(numPartitions: Int): Dataset[T] = withTypedPlan {\n    Repartition(numPartitions, shuffle = false, logicalPlan)\n  }\n\n  /**\n   * Returns a new Dataset that contains only the unique rows from this Dataset.\n   * This is an alias for `dropDuplicates`.\n   *\n   * @note Equality checking is performed directly on the encoded representation of the data\n   * and thus is not affected by a custom `equals` function defined on `T`.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  def distinct(): Dataset[T] = dropDuplicates()\n\n```\n## 数据的持久化\n  数据的持久化和缓存策略，一般我们操作rdd都是延迟计算，但是当我们多次重复使用一个rdd的时候可以选择将其缓存而不是每次进行一个计算，可以提高效率。\n\n```\n  /**\n   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n                  持久化\n  def persist(): this.type = {\n    sparkSession.sharedState.cacheManager.cacheQuery(this)\n    this\n  }\n\n  /**\n   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  def cache(): this.type = persist()\n\n  /**\n   * Persist this Dataset with the given storage level.\n   * @param newLevel One of: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`,\n   *                 `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`,\n   *                 `MEMORY_AND_DISK_2`, etc.\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  def persist(newLevel: StorageLevel): this.type = {\n    sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel)\n    this\n  }\n\n  /**\n   * Get the Dataset's current storage level, or StorageLevel.NONE if not persisted.\n   *\n   * @group basic\n   * @since 2.1.0\n   */\n  def storageLevel: StorageLevel = {\n    sparkSession.sharedState.cacheManager.lookupCachedData(this).map { cachedData =>\n      cachedData.cachedRepresentation.storageLevel\n    }.getOrElse(StorageLevel.NONE)\n  }\n\n  /**\n   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.\n   *\n   * @param blocking Whether to block until all blocks are deleted.\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  def unpersist(blocking: Boolean): this.type = {\n    sparkSession.sharedState.cacheManager.uncacheQuery(this, blocking)\n    this\n  }\n\n  /**\n   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  def unpersist(): this.type = unpersist(blocking = false)\n\n  /**\n   * Represents the content of the Dataset as an `RDD` of `T`.\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  lazy val rdd: RDD[T] = {\n    val objectType = exprEnc.deserializer.dataType\n    val deserialized = CatalystSerde.deserialize[T](logicalPlan)\n    sparkSession.sessionState.executePlan(deserialized).toRdd.mapPartitions { rows =>\n      rows.map(_.get(0, objectType).asInstanceOf[T])\n    }\n  }\n\n  /**\n   * Returns the content of the Dataset as a `JavaRDD` of `T`s.\n   * @group basic\n   * @since 1.6.0\n   */\n  def toJavaRDD: JavaRDD[T] = rdd.toJavaRDD()\n\n  /**\n   * Returns the content of the Dataset as a `JavaRDD` of `T`s.\n   * @group basic\n   * @since 1.6.0\n   */\n  def javaRDD: JavaRDD[T] = toJavaRDD\n```\n## 注册临时表\n通过注册可以将一个dataset直接当作一个表来操作，这样就可以直接通过sql来执行了，不过返回的结果又是一个dataset\n```\n  /**\n   * Registers this Dataset as a temporary table using the given name. The lifetime of this\n   * temporary table is tied to the [[SparkSession]] that was used to create this Dataset.\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  注册\n  @deprecated(\"Use createOrReplaceTempView(viewName) instead.\", \"2.0.0\")\n  def registerTempTable(tableName: String): Unit = {\n    createOrReplaceTempView(tableName)\n  }\n\n  /**\n   * Creates a local temporary view using the given name. The lifetime of this\n   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.\n   *\n   * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that\n   * created it, i.e. it will be automatically dropped when the session terminates. It's not\n   * tied to any databases, i.e. we can't use `db1.view1` to reference a local temporary view.\n   *\n   * @throws AnalysisException if the view name is invalid or already exists\n   *\n   * @group basic\n   * @since 2.0.0\n   */\n  创建表\n  @throws[AnalysisException]\n  def createTempView(viewName: String): Unit = withPlan {\n    createTempViewCommand(viewName, replace = false, global = false)\n  }\n\n\n\n  /**\n   * Creates a local temporary view using the given name. The lifetime of this\n   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.\n   *\n   * @group basic\n   * @since 2.0.0\n   */\n  def createOrReplaceTempView(viewName: String): Unit = withPlan {\n    createTempViewCommand(viewName, replace = true, global = false)\n  }\n\n  /**\n   * Creates a global temporary view using the given name. The lifetime of this\n   * temporary view is tied to this Spark application.\n   *\n   * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,\n   * i.e. it will be automatically dropped when the application terminates. It's tied to a system\n   * preserved database `global_temp`, and we must use the qualified name to refer a global temp\n   * view, e.g. `SELECT * FROM global_temp.view1`.\n   *\n   * @throws AnalysisException if the view name is invalid or already exists\n   *\n   * @group basic\n   * @since 2.1.0\n   */\n  @throws[AnalysisException]\n  def createGlobalTempView(viewName: String): Unit = withPlan {\n    createTempViewCommand(viewName, replace = false, global = true)\n  }\n\n  private def createTempViewCommand(\n      viewName: String,\n      replace: Boolean,\n      global: Boolean): CreateViewCommand = {\n    val viewType = if (global) GlobalTempView else LocalTempView\n\n    val tableIdentifier = try {\n      sparkSession.sessionState.sqlParser.parseTableIdentifier(viewName)\n    } catch {\n      case _: ParseException => throw new AnalysisException(s\"Invalid view name: $viewName\")\n    }\n    CreateViewCommand(\n      name = tableIdentifier,\n      userSpecifiedColumns = Nil,\n      comment = None,\n      properties = Map.empty,\n      originalText = None,\n      child = logicalPlan,\n      allowExisting = false,\n      replace = replace,\n      viewType = viewType)\n  }\n  ```\n## 数据保存\n    数据保存有一个专门的write类来处理，这里就是调用write方法返回一个write对象来实现的\n```\n  /**\n   * Interface for saving the content of the non-streaming Dataset out into external storage.\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  def write: DataFrameWriter[T] = {\n    if (isStreaming) {\n      logicalPlan.failAnalysis(\n        \"'write' can not be called on streaming Dataset/DataFrame\")\n    }\n    new DataFrameWriter[T](this)\n  }\n\n  /**\n   * :: Experimental ::\n   * Interface for saving the content of the streaming Dataset out into external storage.\n   *\n   * @group basic\n   * @since 2.0.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def writeStream: DataStreamWriter[T] = {\n    if (!isStreaming) {\n      logicalPlan.failAnalysis(\n        \"'writeStream' can be called only on streaming Dataset/DataFrame\")\n    }\n    new DataStreamWriter[T](this)\n  }\n\n```\n## 转换成json格式\n```\n  /**\n   * Returns the content of the Dataset as a Dataset of JSON strings.\n   * @since 2.0.0\n   */\n  def toJSON: Dataset[String] = {\n    val rowSchema = this.schema\n    val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone\n    val rdd: RDD[String] = queryExecution.toRdd.mapPartitions { iter =>\n      val writer = new CharArrayWriter()\n      // create the Generator without separator inserted between 2 records\n      val gen = new JacksonGenerator(rowSchema, writer,\n        new JSONOptions(Map.empty[String, String], sessionLocalTimeZone))\n\n      new Iterator[String] {\n        override def hasNext: Boolean = iter.hasNext\n        override def next(): String = {\n          gen.write(iter.next())\n          gen.flush()\n\n          val json = writer.toString\n          if (hasNext) {\n            writer.reset()\n          } else {\n            gen.close()\n          }\n\n          json\n        }\n      }\n    }\n    import sparkSession.implicits.newStringEncoder\n    sparkSession.createDataset(rdd)\n  }\n  ```\n## 获取文件列表 \n  可以获取当前dataSet都加载了那些文件\n  ```\n  /**\n   * Returns a best-effort snapshot of the files that compose this Dataset. This method simply\n   * asks each constituent BaseRelation for its respective files and takes the union of all results.\n   * Depending on the source relations, this may not find all input files. Duplicates are removed.\n   *\n   * @group basic\n   * @since 2.0.0\n   */\n  def inputFiles: Array[String] = {\n    val files: Seq[String] = queryExecution.optimizedPlan.collect {\n      case LogicalRelation(fsBasedRelation: FileRelation, _, _) =>\n        fsBasedRelation.inputFiles\n      case fr: FileRelation =>\n        fr.inputFiles\n    }.flatten\n    files.toSet.toArray\n  }\n```\n","source":"_posts/spark1.md","raw":"---\ntitle: spark 源码分析之 sparksql DataSet\ndate: 2017-03-11\ncategory: spark\ntags: \n  - spark\n  - 大数据\n---\n记录一下sparksql的dataframe 中常用的操作，spark在大数据处理方面有很广泛的应供，每天都在研究spark的源码，简单记录一下以便后续查阅,今天先简单整理一下，后续逐步完善.\n版本:spark 2.0.1\n<!-- more -->\n## 数据显示\n这个showString 是spark内部的方法，我们实际是调用不到的，但是我们调用的show方法最终都是调用了这个showString\n```\n  /**\n   * Compose the string representing rows for output\n   *\n   * @param _numRows Number of rows to show\n   * @param truncate If set to more than 0, truncates strings to `truncate` characters and\n   *                   all cells will be aligned right.\n   */\n  private[sql] def showString(_numRows: Int, truncate: Int = 20): String \n```\n## 将dataSet转换成dataFrame\n   datafram其实是按列来存储的dataset\n```\n  /**\n   * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed.\n   * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with\n   * meaningful names. For example:\n   * {{{\n   *   val rdd: RDD[(Int, String)] = ...\n   *   rdd.toDF()  // this implicit conversion creates a DataFrame with column name `_1` and `_2`\n   *   rdd.toDF(\"id\", \"name\")  // this creates a DataFrame with column name \"id\" and \"name\"\n   * }}}\n   *\n   * @group basic\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def toDF(colNames: String*): DataFrame = {\n    require(schema.size == colNames.size,\n      \"The number of columns doesn't match.\\n\" +\n        s\"Old column names (${schema.size}): \" + schema.fields.map(_.name).mkString(\", \") + \"\\n\" +\n        s\"New column names (${colNames.size}): \" + colNames.mkString(\", \"))\n\n    val newCols = logicalPlan.output.zip(colNames).map { case (oldAttribute, newName) =>\n      Column(oldAttribute).as(newName)\n    }\n    select(newCols : _*)\n  }\n```\n输出当前dataset的结构信息\n```\n  /**\n   * Returns the schema of this Dataset.\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  def schema: StructType = queryExecution.analyzed.schema\n\n  /**\n   * Prints the schema to the console in a nice tree format.\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  // scalastyle:off println\n  def printSchema(): Unit = println(schema.treeString)\n  // scalastyle:on println\n```\n输出一些调试信息\n```\n  /**\n   * Prints the plans (logical and physical) to the console for debugging purposes.\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  def explain(extended: Boolean): Unit = {\n    val explain = ExplainCommand(queryExecution.logical, extended = extended)\n    sparkSession.sessionState.executePlan(explain).executedPlan.executeCollect().foreach {\n      // scalastyle:off println\n      r => println(r.getString(0))\n      // scalastyle:on println\n    }\n  }\n\n  /**\n   * Prints the physical plan to the console for debugging purposes.\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  def explain(): Unit = explain(extended = false)\n```\n输出列名以及每个列的类型\n```\n  /**\n   * Returns all column names and their data types as an array.\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  def dtypes: Array[(String, String)] = schema.fields.map { field =>\n    (field.name, field.dataType.toString)\n  }\n\n  /**\n   * Returns all column names as an array.\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  def columns: Array[String] = schema.fields.map(_.name)\n```\n是否能够获取数据\n```\n  /**\n   * Returns true if the `collect` and `take` methods can be run locally\n   * (without any Spark executors).\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation]\n\n  /**\n   * Returns true if this Dataset contains one or more sources that continuously\n   * return data as it arrives. A Dataset that reads data from a streaming source\n   * must be executed as a `StreamingQuery` using the `start()` method in\n   * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or\n   * `collect()`, will throw an [[AnalysisException]] when there is a streaming\n   * source present.\n   *\n   * @group streaming\n   * @since 2.0.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def isStreaming: Boolean = logicalPlan.isStreaming\n```\n**检查点，以前没有用过，需要在研究一下**\n```\n  /**\n   * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate\n   * the logical plan of this Dataset, which is especially useful in iterative algorithms where the\n   * plan may grow exponentially. It will be saved to files inside the checkpoint\n   * directory set with `SparkContext#setCheckpointDir`.\n   *\n   * @group basic\n   * @since 2.1.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def checkpoint(): Dataset[T] = checkpoint(eager = true)\n\n  /**\n   * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the\n   * logical plan of this Dataset, which is especially useful in iterative algorithms where the\n   * plan may grow exponentially. It will be saved to files inside the checkpoint\n   * directory set with `SparkContext#setCheckpointDir`.\n   *\n   * @group basic\n   * @since 2.1.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def checkpoint(eager: Boolean): Dataset[T] = {\n    val internalRdd = queryExecution.toRdd.map(_.copy())\n    internalRdd.checkpoint()\n\n    if (eager) {\n      internalRdd.count()\n    }\n\n    val physicalPlan = queryExecution.executedPlan\n```\n这个什么鬼需要在分析一下\n```\n    // Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the\n    // size of `PartitioningCollection` may grow exponentially for queries involving deep inner\n    // joins.\n    def firstLeafPartitioning(partitioning: Partitioning): Partitioning = {\n      partitioning match {\n        case p: PartitioningCollection => firstLeafPartitioning(p.partitionings.head)\n        case p => p\n      }\n    }\n\n    val outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning)\n\n    Dataset.ofRows(\n      sparkSession,\n      LogicalRDD(\n        logicalPlan.output,\n        internalRdd,\n        outputPartitioning,\n        physicalPlan.outputOrdering\n      )(sparkSession)).as[T]\n  }\n```\n水印？？？\n```\n  /**\n   * :: Experimental ::\n   * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time\n   * before which we assume no more late data is going to arrive.\n   *\n   * Spark will use this watermark for several purposes:\n   *  - To know when a given time window aggregation can be finalized and thus can be emitted when\n   *    using output modes that do not allow updates.\n   *  - To minimize the amount of state that we need to keep for on-going aggregations,\n   *    `mapGroupsWithState` and `dropDuplicates` operators.\n   *\n   *  The current watermark is computed by looking at the `MAX(eventTime)` seen across\n   *  all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\n   *  of coordinating this value across partitions, the actual watermark used is only guaranteed\n   *  to be at least `delayThreshold` behind the actual event time.  In some cases we may still\n   *  process records that arrive more than `delayThreshold` late.\n   *\n   * @param eventTime the name of the column that contains the event time of the row.\n   * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest\n   *                       record that has been processed in the form of an interval\n   *                       (e.g. \"1 minute\" or \"5 hours\").\n   *\n   * @group streaming\n   * @since 2.1.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  // We only accept an existing column name, not a derived column here as a watermark that is\n  // defined on a derived column cannot referenced elsewhere in the plan.\n  def withWatermark(eventTime: String, delayThreshold: String): Dataset[T] = withTypedPlan {\n    val parsedDelay =\n      Option(CalendarInterval.fromString(\"interval \" + delayThreshold))\n        .getOrElse(throw new AnalysisException(s\"Unable to parse time delay '$delayThreshold'\"))\n    EventTimeWatermark(UnresolvedAttribute(eventTime), parsedDelay, logicalPlan)\n  }\n```\n## 数据展示\n```\n  /**\n   * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated,\n   * and all cells will be aligned right. For example:\n   * {{{\n   *   year  month AVG('Adj Close) MAX('Adj Close)\n   *   1980  12    0.503218        0.595103\n   *   1981  01    0.523289        0.570307\n   *   1982  02    0.436504        0.475256\n   *   1983  03    0.410516        0.442194\n   *   1984  04    0.450090        0.483521\n   * }}}\n   *\n   * @param numRows Number of rows to show\n   *\n   * @group action\n   * @since 1.6.0\n   */\n显示datafram中的指定数量的数据，默认字段长度超过20位则截断。\n  def show(numRows: Int): Unit = show(numRows, truncate = true)\n\n显示datafram的数据，默认取前面20条记录显示，默认字段长度超过20位则截断。\n  \n  def show(): Unit = show(20)\n\n显示datafram的数据，默认取前面20条记录显示，通过truncate选择是否需要全部显示每一列的信息。\n  def show(truncate: Boolean): Unit = show(20, truncate)\n\n显示datafram的数据，numRows为显示数量，通过truncate选择是否需要全部显示每一列的信息。\n  def show(numRows: Int, truncate: Boolean): Unit = if (truncate) {\n  def show(numRows: Int, truncate: Int): Unit = println(showString(numRows, truncate))\n```\n## 数据的关联\n```\n\n返回dataset中空值操作算子\n  def na: DataFrameNaFunctions = new DataFrameNaFunctions(toDF())\n\n返回dataset中统计操作算子\n  def stat: DataFrameStatFunctions = new DataFrameStatFunctions(toDF())\n关联dataframe\n  def join(right: Dataset[_]): DataFrame = withPlan {\n    Join(logicalPlan, right.logicalPlan, joinType = Inner, None)\n  }\n\n  /**\n   * Inner equi-join with another `DataFrame` using the given column.\n   *\n   * Different from other join functions, the join column will only appear once in the output,\n   * i.e. similar to SQL's `JOIN USING` syntax.\n   *\n   * {{{\n   *   // Joining df1 and df2 using the column \"user_id\"\n   *   df1.join(df2, \"user_id\")\n   * }}}\n   *\n   * @param right Right side of the join operation.\n   * @param usingColumn Name of the column to join on. This column must exist on both sides.\n   *\n   * @note If you perform a self-join using this function without aliasing the input\n   * `DataFrame`s, you will NOT be able to reference any columns after the join, since\n   * there is no way to disambiguate which side of the join you would like to reference.\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n指定字段关联\n  def join(right: Dataset[_], usingColumn: String): DataFrame = {\n    join(right, Seq(usingColumn))\n  }\n\n  /**\n   * Inner equi-join with another `DataFrame` using the given columns.\n   *\n   * Different from other join functions, the join columns will only appear once in the output,\n   * i.e. similar to SQL's `JOIN USING` syntax.\n   *\n   * {{{\n   *   // Joining df1 and df2 using the columns \"user_id\" and \"user_name\"\n   *   df1.join(df2, Seq(\"user_id\", \"user_name\"))\n   * }}}\n   *\n   * @param right Right side of the join operation.\n   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.\n   *\n   * @note If you perform a self-join using this function without aliasing the input\n   * `DataFrame`s, you will NOT be able to reference any columns after the join, since\n   * there is no way to disambiguate which side of the join you would like to reference.\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n指定关联字段，不同dataframe中同名字段的key不重复出现\n  def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame = {\n    join(right, usingColumns, \"inner\")\n  }\n\n  /**\n   * Equi-join with another `DataFrame` using the given columns. A cross join with a predicate\n   * is specified as an inner join. If you would explicitly like to perform a cross join use the\n   * `crossJoin` method.\n   *\n   * Different from other join functions, the join columns will only appear once in the output,\n   * i.e. similar to SQL's `JOIN USING` syntax.\n   *\n   * @param right Right side of the join operation.\n   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.\n   * @param joinType Type of join to perform. Default `inner`. Must be one of:\n   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,\n   *                 `right`, `right_outer`, `left_semi`, `left_anti`.\n   *\n   * @note If you perform a self-join using this function without aliasing the input\n   * `DataFrame`s, you will NOT be able to reference any columns after the join, since\n   * there is no way to disambiguate which side of the join you would like to reference.\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n指定关联的类型\n  def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame = {\n    // Analyze the self join. The assumption is that the analyzer will disambiguate left vs right\n    // by creating a new instance for one of the branch.\n    val joined = sparkSession.sessionState.executePlan(\n      Join(logicalPlan, right.logicalPlan, joinType = JoinType(joinType), None))\n      .analyzed.asInstanceOf[Join]\n\n    withPlan {\n      Join(\n        joined.left,\n        joined.right,\n        UsingJoin(JoinType(joinType), usingColumns),\n        None)\n    }\n  }\n\n  /**\n   * Inner join with another `DataFrame`, using the given join expression.\n   *\n   * {{{\n   *   // The following two are equivalent:\n   *   df1.join(df2, $\"df1Key\" === $\"df2Key\")\n   *   df1.join(df2).where($\"df1Key\" === $\"df2Key\")\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n通过表达式进行关联\n  def join(right: Dataset[_], joinExprs: Column): DataFrame = join(right, joinExprs, \"inner\")\n\n  /**\n   * Join with another `DataFrame`, using the given join expression. The following performs\n   * a full outer join between `df1` and `df2`.\n   *\n   * {{{\n   *   // Scala:\n   *   import org.apache.spark.sql.functions._\n   *   df1.join(df2, $\"df1Key\" === $\"df2Key\", \"outer\")\n   *\n   *   // Java:\n   *   import static org.apache.spark.sql.functions.*;\n   *   df1.join(df2, col(\"df1Key\").equalTo(col(\"df2Key\")), \"outer\");\n   * }}}\n   *\n   * @param right Right side of the join.\n   * @param joinExprs Join expression.\n   * @param joinType Type of join to perform. Default `inner`. Must be one of:\n   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,\n   *                 `right`, `right_outer`, `left_semi`, `left_anti`.\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n通过列名表达式进行关联\n  def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame = {\n    // Note that in this function, we introduce a hack in the case of self-join to automatically\n    // resolve ambiguous join conditions into ones that might make sense [SPARK-6231].\n    // Consider this case: df.join(df, df(\"key\") === df(\"key\"))\n    // Since df(\"key\") === df(\"key\") is a trivially true condition, this actually becomes a\n    // cartesian join. However, most likely users expect to perform a self join using \"key\".\n    // With that assumption, this hack turns the trivially true condition into equality on join\n    // keys that are resolved to both sides.\n\n    // Trigger analysis so in the case of self-join, the analyzer will clone the plan.\n    // After the cloning, left and right side will have distinct expression ids.\n    val plan = withPlan(\n      Join(logicalPlan, right.logicalPlan, JoinType(joinType), Some(joinExprs.expr)))\n      .queryExecution.analyzed.asInstanceOf[Join]\n\n    // If auto self join alias is disabled, return the plan.\n    if (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) {\n      return withPlan(plan)\n    }\n\n    // If left/right have no output set intersection, return the plan.\n    val lanalyzed = withPlan(this.logicalPlan).queryExecution.analyzed\n    val ranalyzed = withPlan(right.logicalPlan).queryExecution.analyzed\n    if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) {\n      return withPlan(plan)\n    }\n\n    // Otherwise, find the trivially true predicates and automatically resolves them to both sides.\n    // By the time we get here, since we have already run analysis, all attributes should've been\n    // resolved and become AttributeReference.\n    val cond = plan.condition.map { _.transform {\n      case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference)\n          if a.sameRef(b) =>\n        catalyst.expressions.EqualTo(\n          withPlan(plan.left).resolve(a.name),\n          withPlan(plan.right).resolve(b.name))\n    }}\n\n    withPlan {\n      plan.copy(condition = cond)\n    }\n  }\n\n  /**\n   * Explicit cartesian join with another `DataFrame`.\n   *\n   * @param right Right side of the join operation.\n   *\n   * @note Cartesian joins are very expensive without an extra filter that can be pushed down.\n   *\n   * @group untypedrel\n   * @since 2.1.0\n   */\n全表关联\n  def crossJoin(right: Dataset[_]): DataFrame = withPlan {\n    Join(logicalPlan, right.logicalPlan, joinType = Cross, None)\n  }\n\n  /**\n   * :: Experimental ::\n   * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to\n   * true.\n   *\n   * This is similar to the relation `join` function with one important difference in the\n   * result schema. Since `joinWith` preserves objects present on either side of the join, the\n   * result schema is similarly nested into a tuple under the column names `_1` and `_2`.\n   *\n   * This type of join can be useful both for preserving type-safety with the original object\n   * types as well as working with relational data where either side of the join has column\n   * names in common.\n   *\n   * @param other Right side of the join.\n   * @param condition Join expression.\n   * @param joinType Type of join to perform. Default `inner`. Must be one of:\n   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,\n   *                 `right`, `right_outer`, `left_semi`, `left_anti`.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n一种特殊的关联，得到的结果集的结构不同于普通的关联结果\n  @Experimental\n  @InterfaceStability.Evolving\n  def joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = {\n    // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved,\n    // etc.\n    val joined = sparkSession.sessionState.executePlan(\n      Join(\n        this.logicalPlan,\n        other.logicalPlan,\n        JoinType(joinType),\n        Some(condition.expr))).analyzed.asInstanceOf[Join]\n\n    // For both join side, combine all outputs into a single column and alias it with \"_1\" or \"_2\",\n    // to match the schema for the encoder of the join result.\n    // Note that we do this before joining them, to enable the join operator to return null for one\n    // side, in cases like outer-join.\n    val left = {\n      val combined = if (this.exprEnc.flat) {\n        assert(joined.left.output.length == 1)\n        Alias(joined.left.output.head, \"_1\")()\n      } else {\n        Alias(CreateStruct(joined.left.output), \"_1\")()\n      }\n      Project(combined :: Nil, joined.left)\n    }\n\n    val right = {\n      val combined = if (other.exprEnc.flat) {\n        assert(joined.right.output.length == 1)\n        Alias(joined.right.output.head, \"_2\")()\n      } else {\n        Alias(CreateStruct(joined.right.output), \"_2\")()\n      }\n      Project(combined :: Nil, joined.right)\n    }\n\n    // Rewrites the join condition to make the attribute point to correct column/field, after we\n    // combine the outputs of each join side.\n    val conditionExpr = joined.condition.get transformUp {\n      case a: Attribute if joined.left.outputSet.contains(a) =>\n        if (this.exprEnc.flat) {\n          left.output.head\n        } else {\n          val index = joined.left.output.indexWhere(_.exprId == a.exprId)\n          GetStructField(left.output.head, index)\n        }\n      case a: Attribute if joined.right.outputSet.contains(a) =>\n        if (other.exprEnc.flat) {\n          right.output.head\n        } else {\n          val index = joined.right.output.indexWhere(_.exprId == a.exprId)\n          GetStructField(right.output.head, index)\n        }\n    }\n\n    implicit val tuple2Encoder: Encoder[(T, U)] =\n      ExpressionEncoder.tuple(this.exprEnc, other.exprEnc)\n\n    withTypedPlan(Join(left, right, joined.joinType, Some(conditionExpr)))\n  }\n\n  /**\n   * :: Experimental ::\n   * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair\n   * where `condition` evaluates to true.\n   *\n   * @param other Right side of the join.\n   * @param condition Join expression.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = {\n    joinWith(other, condition, \"inner\")\n  }\n```\n## 排序分组\n```\n  /**\n   * Returns a new Dataset with each partition sorted by the given expressions.\n   *\n   * This is the same operation as \"SORT BY\" in SQL (Hive QL).\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n根据指定字段对每个分区进行排序\n  @scala.annotation.varargs\n  def sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T] = {\n    sortWithinPartitions((sortCol +: sortCols).map(Column(_)) : _*)\n  }\n\n  /**\n   * Returns a new Dataset with each partition sorted by the given expressions.\n   *\n   * This is the same operation as \"SORT BY\" in SQL (Hive QL).\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n\n根据指定列对每个分区进行排序\n  @scala.annotation.varargs\n  def sortWithinPartitions(sortExprs: Column*): Dataset[T] = {\n    sortInternal(global = false, sortExprs)\n  }\n\n  /**\n   * Returns a new Dataset sorted by the specified column, all in ascending order.\n   * {{{\n   *   // The following 3 are equivalent\n   *   ds.sort(\"sortcol\")\n   *   ds.sort($\"sortcol\")\n   *   ds.sort($\"sortcol\".asc)\n   * }}}\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def sort(sortCol: String, sortCols: String*): Dataset[T] = {\n    sort((sortCol +: sortCols).map(apply) : _*)\n  }\n\n  /**\n   * Returns a new Dataset sorted by the given expressions. For example:\n   * {{{\n   *   ds.sort($\"col1\", $\"col2\".desc)\n   * }}}\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def sort(sortExprs: Column*): Dataset[T] = {\n    sortInternal(global = true, sortExprs)\n  }\n\n  /**\n   * Returns a new Dataset sorted by the given expressions.\n   * This is an alias of the `sort` function.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def orderBy(sortCol: String, sortCols: String*): Dataset[T] = sort(sortCol, sortCols : _*)\n\n  /**\n   * Returns a new Dataset sorted by the given expressions.\n   * This is an alias of the `sort` function.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def orderBy(sortExprs: Column*): Dataset[T] = sort(sortExprs : _*)\n```\n提取指定的列\n```\n  /**\n   * Selects column based on the column name and return it as a [[Column]].\n   *\n   * @note The column name can also reference to a nested column like `a.b`.\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  def apply(colName: String): Column = col(colName)\n  /**\n   * Selects column based on the column name and return it as a [[Column]].\n   *\n   * @note The column name can also reference to a nested column like `a.b`.\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  def col(colName: String): Column = colName match {\n    case \"*\" =>\n      Column(ResolvedStar(queryExecution.analyzed.output))\n    case _ =>\n      val expr = resolve(colName)\n      Column(expr)\n  }\n\n```\n## 别名\n别名有给列取别名的也有给dataset取别名的这里是给当前dataset取别名\n ```\n\n  /**\n   * Returns a new Dataset with an alias set.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  def as(alias: String): Dataset[T] = withTypedPlan {\n    SubqueryAlias(alias, logicalPlan, None)\n  }\n\n  /**\n   * (Scala-specific) Returns a new Dataset with an alias set.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  def as(alias: Symbol): Dataset[T] = as(alias.name)\n\n  /**\n   * Returns a new Dataset with an alias set. Same as `as`.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  def alias(alias: String): Dataset[T] = as(alias)\n\n  /**\n   * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  def alias(alias: Symbol): Dataset[T] = as(alias)\n```\n## 查询\n查询有很多种接口使用的方式不太一样\n```\n  /**\n   * Selects a set of column based expressions.\n   * {{{\n   *   ds.select($\"colA\", $\"colB\" + 1)\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def select(cols: Column*): DataFrame = withPlan {\n    Project(cols.map(_.named), logicalPlan)\n  }\n\n  /**\n   * Selects a set of columns. This is a variant of `select` that can only select\n   * existing columns using column names (i.e. cannot construct expressions).\n   *\n   * {{{\n   *   // The following two are equivalent:\n   *   ds.select(\"colA\", \"colB\")\n   *   ds.select($\"colA\", $\"colB\")\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def select(col: String, cols: String*): DataFrame = select((col +: cols).map(Column(_)) : _*)\n\n  /**\n   * Selects a set of SQL expressions. This is a variant of `select` that accepts\n   * SQL expressions.\n   *\n   * {{{\n   *   // The following are equivalent:\n   *   ds.selectExpr(\"colA\", \"colB as newName\", \"abs(colC)\")\n   *   ds.select(expr(\"colA\"), expr(\"colB as newName\"), expr(\"abs(colC)\"))\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n按照表达式来查询\n  @scala.annotation.varargs\n  def selectExpr(exprs: String*): DataFrame = {\n    select(exprs.map { expr =>\n      Column(sparkSession.sessionState.sqlParser.parseExpression(expr))\n    }: _*)\n  }\n\n  /**\n   * :: Experimental ::\n   * Returns a new Dataset by computing the given [[Column]] expression for each element.\n   *\n   * {{{\n   *   val ds = Seq(1, 2, 3).toDS()\n   *   val newDS = ds.select(expr(\"value + 1\").as[Int])\n   * }}}\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def select[U1](c1: TypedColumn[T, U1]): Dataset[U1] = {\n    implicit val encoder = c1.encoder\n    val project = Project(c1.withInputType(exprEnc, logicalPlan.output).named :: Nil,\n      logicalPlan)\n\n    if (encoder.flat) {\n      new Dataset[U1](sparkSession, project, encoder)\n    } else {\n      // Flattens inner fields of U1\n      new Dataset[Tuple1[U1]](sparkSession, project, ExpressionEncoder.tuple(encoder)).map(_._1)\n    }\n  }\n\n  /**\n   * Internal helper function for building typed selects that return tuples. For simplicity and\n   * code reuse, we do this without the help of the type system and then use helper functions\n   * that cast appropriately for the user facing interface.\n   */\n  ???这个查询怎么用\n  protected def selectUntyped(columns: TypedColumn[_, _]*): Dataset[_] = {\n    val encoders = columns.map(_.encoder)\n    val namedColumns =\n      columns.map(_.withInputType(exprEnc, logicalPlan.output).named)\n    val execution = new QueryExecution(sparkSession, Project(namedColumns, logicalPlan))\n    new Dataset(sparkSession, execution, ExpressionEncoder.tuple(encoders))\n  }\n\n  /**\n   * :: Experimental ::\n   * Returns a new Dataset by computing the given [[Column]] expressions for each element.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)] =\n    selectUntyped(c1, c2).asInstanceOf[Dataset[(U1, U2)]]\n\n  /**\n   * :: Experimental ::\n   * Returns a new Dataset by computing the given [[Column]] expressions for each element.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def select[U1, U2, U3](\n      c1: TypedColumn[T, U1],\n      c2: TypedColumn[T, U2],\n      c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)] =\n    selectUntyped(c1, c2, c3).asInstanceOf[Dataset[(U1, U2, U3)]]\n\n  /**\n   * :: Experimental ::\n   * Returns a new Dataset by computing the given [[Column]] expressions for each element.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def select[U1, U2, U3, U4](\n      c1: TypedColumn[T, U1],\n      c2: TypedColumn[T, U2],\n      c3: TypedColumn[T, U3],\n      c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)] =\n    selectUntyped(c1, c2, c3, c4).asInstanceOf[Dataset[(U1, U2, U3, U4)]]\n\n  /**\n   * :: Experimental ::\n   * Returns a new Dataset by computing the given [[Column]] expressions for each element.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def select[U1, U2, U3, U4, U5](\n      c1: TypedColumn[T, U1],\n      c2: TypedColumn[T, U2],\n      c3: TypedColumn[T, U3],\n      c4: TypedColumn[T, U4],\n      c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)] =\n    selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[Dataset[(U1, U2, U3, U4, U5)]]\n```\n## 过滤\n过滤，这里的过滤和sql里面的where条件是相同的，查询满足一定条件的记录。\n```\n  /**\n   * Filters rows using the given condition.\n   * {{{\n   *   // The following are equivalent:\n   *   peopleDs.filter($\"age\" > 15)\n   *   peopleDs.where($\"age\" > 15)\n   * }}}\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  def filter(condition: Column): Dataset[T] = withTypedPlan {\n    Filter(condition.expr, logicalPlan)\n  }\n\n  /**\n   * Filters rows using the given SQL expression.\n   * {{{\n   *   peopleDs.filter(\"age > 15\")\n   * }}}\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  def filter(conditionExpr: String): Dataset[T] = {\n    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))\n  }\n\n  /**\n   * Filters rows using the given condition. This is an alias for `filter`.\n   * {{{\n   *   // The following are equivalent:\n   *   peopleDs.filter($\"age\" > 15)\n   *   peopleDs.where($\"age\" > 15)\n   * }}}\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  def where(condition: Column): Dataset[T] = filter(condition)\n\n  /**\n   * Filters rows using the given SQL expression.\n   * {{{\n   *   peopleDs.where(\"age > 15\")\n   * }}}\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  def where(conditionExpr: String): Dataset[T] = {\n    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))\n  }\n```\n## 分组查询\n\n```\n  /**\n   * Groups the Dataset using the specified columns, so we can run aggregation on them. See\n   * [[RelationalGroupedDataset]] for all the available aggregate functions.\n   *\n   * {{{\n   *   // Compute the average for all numeric columns grouped by department.\n   *   ds.groupBy($\"department\").avg()\n   *\n   *   // Compute the max age and average salary, grouped by department and gender.\n   *   ds.groupBy($\"department\", $\"gender\").agg(Map(\n   *     \"salary\" -> \"avg\",\n   *     \"age\" -> \"max\"\n   *   ))\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def groupBy(cols: Column*): RelationalGroupedDataset = {\n    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.GroupByType)\n  }\n\n  /**\n   * Create a multi-dimensional rollup for the current Dataset using the specified columns,\n   * so we can run aggregation on them.\n   * See [[RelationalGroupedDataset]] for all the available aggregate functions.\n   *\n   * {{{\n   *   // Compute the average for all numeric columns rolluped by department and group.\n   *   ds.rollup($\"department\", $\"group\").avg()\n   *\n   *   // Compute the max age and average salary, rolluped by department and gender.\n   *   ds.rollup($\"department\", $\"gender\").agg(Map(\n   *     \"salary\" -> \"avg\",\n   *     \"age\" -> \"max\"\n   *   ))\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  数据归纳\n  @scala.annotation.varargs\n  def rollup(cols: Column*): RelationalGroupedDataset = {\n    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.RollupType)\n  }\n\n  /**\n   * Create a multi-dimensional cube for the current Dataset using the specified columns,\n   * so we can run aggregation on them.\n   * See [[RelationalGroupedDataset]] for all the available aggregate functions.\n   *\n   * {{{\n   *   // Compute the average for all numeric columns cubed by department and group.\n   *   ds.cube($\"department\", $\"group\").avg()\n   *\n   *   // Compute the max age and average salary, cubed by department and gender.\n   *   ds.cube($\"department\", $\"gender\").agg(Map(\n   *     \"salary\" -> \"avg\",\n   *     \"age\" -> \"max\"\n   *   ))\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def cube(cols: Column*): RelationalGroupedDataset = {\n    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.CubeType)\n  }\n\n  /**\n   * Groups the Dataset using the specified columns, so that we can run aggregation on them.\n   * See [[RelationalGroupedDataset]] for all the available aggregate functions.\n   *\n   * This is a variant of groupBy that can only group by existing columns using column names\n   * (i.e. cannot construct expressions).\n   *\n   * {{{\n   *   // Compute the average for all numeric columns grouped by department.\n   *   ds.groupBy(\"department\").avg()\n   *\n   *   // Compute the max age and average salary, grouped by department and gender.\n   *   ds.groupBy($\"department\", $\"gender\").agg(Map(\n   *     \"salary\" -> \"avg\",\n   *     \"age\" -> \"max\"\n   *   ))\n   * }}}\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def groupBy(col1: String, cols: String*): RelationalGroupedDataset = {\n    val colNames: Seq[String] = col1 +: cols\n    RelationalGroupedDataset(\n      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.GroupByType)\n  }\n```\n## reduce操作\n\n  ```\n  /**\n   * :: Experimental ::\n   * (Scala-specific)\n   * Reduces the elements of this Dataset using the specified binary function. The given `func`\n   * must be commutative and associative or the result may be non-deterministic.\n   *\n   * @group action\n   * @since 1.6.0\n   */\n  合并操作\n  @Experimental\n  @InterfaceStability.Evolving\n  def reduce(func: (T, T) => T): T = rdd.reduce(func)\n\n  /**\n   * :: Experimental ::\n   * (Java-specific)\n   * Reduces the elements of this Dataset using the specified binary function. The given `func`\n   * must be commutative and associative or the result may be non-deterministic.\n   *\n   * @group action\n   * @since 1.6.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def reduce(func: ReduceFunction[T]): T = reduce(func.call(_, _))\n\n  /**\n   * :: Experimental ::\n   * (Scala-specific)\n   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  分组                                  \n  @Experimental\n  @InterfaceStability.Evolving\n  def groupByKey[K: Encoder](func: T => K): KeyValueGroupedDataset[K, T] = {\n    val inputPlan = logicalPlan\n    val withGroupingKey = AppendColumns(func, inputPlan)\n    val executed = sparkSession.sessionState.executePlan(withGroupingKey)\n\n    new KeyValueGroupedDataset(\n      encoderFor[K],\n      encoderFor[T],\n      executed,\n      inputPlan.output,\n      withGroupingKey.newColumns)\n  }\n\n  /**\n   * :: Experimental ::\n   * (Java-specific)\n   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T] =\n    groupByKey(func.call(_))(encoder)\n```\n## 数据钻取与聚合操作\n```\n  /**\n   * Create a multi-dimensional rollup for the current Dataset using the specified columns,\n   * so we can run aggregation on them.\n   * See [[RelationalGroupedDataset]] for all the available aggregate functions.\n   *\n   * This is a variant of rollup that can only group by existing columns using column names\n   * (i.e. cannot construct expressions).\n   *\n   * {{{\n   *   // Compute the average for all numeric columns rolluped by department and group.\n   *   ds.rollup(\"department\", \"group\").avg()\n   *\n   *   // Compute the max age and average salary, rolluped by department and gender.\n   *   ds.rollup($\"department\", $\"gender\").agg(Map(\n   *     \"salary\" -> \"avg\",\n   *     \"age\" -> \"max\"\n   *   ))\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def rollup(col1: String, cols: String*): RelationalGroupedDataset = {\n    val colNames: Seq[String] = col1 +: cols\n    RelationalGroupedDataset(\n      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.RollupType)\n  }\n\n  /**\n   * Create a multi-dimensional cube for the current Dataset using the specified columns,\n   * so we can run aggregation on them.\n   * See [[RelationalGroupedDataset]] for all the available aggregate functions.\n   *\n   * This is a variant of cube that can only group by existing columns using column names\n   * (i.e. cannot construct expressions).\n   *\n   * {{{\n   *   // Compute the average for all numeric columns cubed by department and group.\n   *   ds.cube(\"department\", \"group\").avg()\n   *\n   *   // Compute the max age and average salary, cubed by department and gender.\n   *   ds.cube($\"department\", $\"gender\").agg(Map(\n   *     \"salary\" -> \"avg\",\n   *     \"age\" -> \"max\"\n   *   ))\n   * }}}\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def cube(col1: String, cols: String*): RelationalGroupedDataset = {\n    val colNames: Seq[String] = col1 +: cols\n    RelationalGroupedDataset(\n      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.CubeType)\n  }\n\n  /**\n   * (Scala-specific) Aggregates on the entire Dataset without groups.\n   * {{{\n   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)\n   *   ds.agg(\"age\" -> \"max\", \"salary\" -> \"avg\")\n   *   ds.groupBy().agg(\"age\" -> \"max\", \"salary\" -> \"avg\")\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = {\n    groupBy().agg(aggExpr, aggExprs : _*)\n  }\n\n  /**\n   * (Scala-specific) Aggregates on the entire Dataset without groups.\n   * {{{\n   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)\n   *   ds.agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\"))\n   *   ds.groupBy().agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\"))\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  def agg(exprs: Map[String, String]): DataFrame = groupBy().agg(exprs)\n\n  /**\n   * (Java-specific) Aggregates on the entire Dataset without groups.\n   * {{{\n   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)\n   *   ds.agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\"))\n   *   ds.groupBy().agg(Map(\"age\" -> \"max\", \"salary\" -> \"avg\"))\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  def agg(exprs: java.util.Map[String, String]): DataFrame = groupBy().agg(exprs)\n\n  /**\n   * Aggregates on the entire Dataset without groups.\n   * {{{\n   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)\n   *   ds.agg(max($\"age\"), avg($\"salary\"))\n   *   ds.groupBy().agg(max($\"age\"), avg($\"salary\"))\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def agg(expr: Column, exprs: Column*): DataFrame = groupBy().agg(expr, exprs : _*)\n\n  /**\n   * Returns a new Dataset by taking the first `n` rows. The difference between this function\n   * and `head` is that `head` is an action and returns an array (by triggering query execution)\n   * while `limit` returns a new Dataset.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  def limit(n: Int): Dataset[T] = withTypedPlan {\n    Limit(Literal(n), logicalPlan)\n  }\n```\n## 集合的交并补接口\n```\n  /**\n   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.\n   * This is equivalent to `UNION ALL` in SQL.\n   *\n   * To do a SQL-style set union (that does deduplication of elements), use this function followed\n   * by a [[distinct]].\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  @deprecated(\"use union()\", \"2.0.0\")\n  def unionAll(other: Dataset[T]): Dataset[T] = union(other)\n\n  /**\n   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.\n   * This is equivalent to `UNION ALL` in SQL.\n   *\n   * To do a SQL-style set union (that does deduplication of elements), use this function followed\n   * by a [[distinct]].\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  def union(other: Dataset[T]): Dataset[T] = withSetOperator {\n    // This breaks caching, but it's usually ok because it addresses a very specific use case:\n    // using union to union many files or partitions.\n    CombineUnions(Union(logicalPlan, other.logicalPlan))\n  }\n\n  /**\n   * Returns a new Dataset containing rows only in both this Dataset and another Dataset.\n   * This is equivalent to `INTERSECT` in SQL.\n   *\n   * @note Equality checking is performed directly on the encoded representation of the data\n   * and thus is not affected by a custom `equals` function defined on `T`.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  def intersect(other: Dataset[T]): Dataset[T] = withSetOperator {\n    Intersect(logicalPlan, other.logicalPlan)\n  }\n\n  /**\n   * Returns a new Dataset containing rows in this Dataset but not in another Dataset.\n   * This is equivalent to `EXCEPT` in SQL.\n   *\n   * @note Equality checking is performed directly on the encoded representation of the data\n   * and thus is not affected by a custom `equals` function defined on `T`.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  取补集\n  def except(other: Dataset[T]): Dataset[T] = withSetOperator {\n    Except(logicalPlan, other.logicalPlan)\n  }\n```\n## 取样与切分\n```\n  /**\n   * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed.\n   *\n   * @param withReplacement Sample with replacement or not.\n   * @param fraction Fraction of rows to generate.\n   * @param seed Seed for sampling.\n   *\n   * @note This is NOT guaranteed to provide exactly the fraction of the count\n   * of the given [[Dataset]].\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = {\n    require(fraction >= 0,\n      s\"Fraction must be nonnegative, but got ${fraction}\")\n\n    withTypedPlan {\n      Sample(0.0, fraction, withReplacement, seed, logicalPlan)()\n    }\n  }\n\n  /**\n   * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed.\n   *\n   * @param withReplacement Sample with replacement or not.\n   * @param fraction Fraction of rows to generate.\n   *\n   * @note This is NOT guaranteed to provide exactly the fraction of the total count\n   * of the given [[Dataset]].\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  随即取样\n  def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = {\n    sample(withReplacement, fraction, Utils.random.nextLong)\n  }\n\n  /**\n   * Randomly splits this Dataset with the provided weights.\n   *\n   * @param weights weights for splits, will be normalized if they don't sum to 1.\n   * @param seed Seed for sampling.\n   *\n   * For Java API, use [[randomSplitAsList]].\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  随即切分???\n  def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]] = {\n    require(weights.forall(_ >= 0),\n      s\"Weights must be nonnegative, but got ${weights.mkString(\"[\", \",\", \"]\")}\")\n    require(weights.sum > 0,\n      s\"Sum of weights must be positive, but got ${weights.mkString(\"[\", \",\", \"]\")}\")\n\n    // It is possible that the underlying dataframe doesn't guarantee the ordering of rows in its\n    // constituent partitions each time a split is materialized which could result in\n    // overlapping splits. To prevent this, we explicitly sort each input partition to make the\n    // ordering deterministic.\n    // MapType cannot be sorted.\n    val sorted = Sort(logicalPlan.output.filterNot(_.dataType.isInstanceOf[MapType])\n      .map(SortOrder(_, Ascending)), global = false, logicalPlan)\n    val sum = weights.sum\n    val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _)\n    normalizedCumWeights.sliding(2).map { x =>\n      new Dataset[T](\n        sparkSession, Sample(x(0), x(1), withReplacement = false, seed, sorted)(), encoder)\n    }.toArray\n  }\n\n  /**\n   * Returns a Java list that contains randomly split Dataset with the provided weights.\n   *\n   * @param weights weights for splits, will be normalized if they don't sum to 1.\n   * @param seed Seed for sampling.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  def randomSplitAsList(weights: Array[Double], seed: Long): java.util.List[Dataset[T]] = {\n    val values = randomSplit(weights, seed)\n    java.util.Arrays.asList(values : _*)\n  }\n\n  /**\n   * Randomly splits this Dataset with the provided weights.\n   *\n   * @param weights weights for splits, will be normalized if they don't sum to 1.\n   * @group typedrel\n   * @since 2.0.0\n   */\n  def randomSplit(weights: Array[Double]): Array[Dataset[T]] = {\n    randomSplit(weights, Utils.random.nextLong)\n  }\n\n  /**\n   * Randomly splits this Dataset with the provided weights. Provided for the Python Api.\n   *\n   * @param weights weights for splits, will be normalized if they don't sum to 1.\n   * @param seed Seed for sampling.\n   */\n  private[spark] def randomSplit(weights: List[Double], seed: Long): Array[Dataset[T]] = {\n    randomSplit(weights.toArray, seed)\n  }\n\n  /**\n   * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more\n   * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of\n   * the input row are implicitly joined with each row that is output by the function.\n   *\n   * Given that this is deprecated, as an alternative, you can explode columns either using\n   * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count\n   * the number of books that contain a given word:\n   *\n   * {{{\n   *   case class Book(title: String, words: String)\n   *   val ds: Dataset[Book]\n   *\n   *   val allWords = ds.select('title, explode(split('words, \" \")).as(\"word\"))\n   *\n   *   val bookCountPerWord = allWords.groupBy(\"word\").agg(countDistinct(\"title\"))\n   * }}}\n   *\n   * Using `flatMap()` this can similarly be exploded as:\n   *\n   * {{{\n   *   ds.flatMap(_.words.split(\" \"))\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  将字段再处理\n  @deprecated(\"use flatMap() or select() with functions.explode() instead\", \"2.0.0\")\n  def explode[A <: Product : TypeTag](input: Column*)(f: Row => TraversableOnce[A]): DataFrame = {\n    val elementSchema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]\n\n    val convert = CatalystTypeConverters.createToCatalystConverter(elementSchema)\n\n    val rowFunction =\n      f.andThen(_.map(convert(_).asInstanceOf[InternalRow]))\n    val generator = UserDefinedGenerator(elementSchema, rowFunction, input.map(_.expr))\n\n    withPlan {\n      Generate(generator, join = true, outer = false,\n        qualifier = None, generatorOutput = Nil, logicalPlan)\n    }\n  }\n\n  /**\n   * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero\n   * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All\n   * columns of the input row are implicitly joined with each value that is output by the function.\n   *\n   * Given that this is deprecated, as an alternative, you can explode columns either using\n   * `functions.explode()`:\n   *\n   * {{{\n   *   ds.select(explode(split('words, \" \")).as(\"word\"))\n   * }}}\n   *\n   * or `flatMap()`:\n   *\n   * {{{\n   *   ds.flatMap(_.words.split(\" \"))\n   * }}}\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  @deprecated(\"use flatMap() or select() with functions.explode() instead\", \"2.0.0\")\n  def explode[A, B : TypeTag](inputColumn: String, outputColumn: String)(f: A => TraversableOnce[B])\n    : DataFrame = {\n    val dataType = ScalaReflection.schemaFor[B].dataType\n    val attributes = AttributeReference(outputColumn, dataType)() :: Nil\n    // TODO handle the metadata?\n    val elementSchema = attributes.toStructType\n\n    def rowFunction(row: Row): TraversableOnce[InternalRow] = {\n      val convert = CatalystTypeConverters.createToCatalystConverter(dataType)\n      f(row(0).asInstanceOf[A]).map(o => InternalRow(convert(o)))\n    }\n    val generator = UserDefinedGenerator(elementSchema, rowFunction, apply(inputColumn).expr :: Nil)\n\n    withPlan {\n      Generate(generator, join = true, outer = false,\n        qualifier = None, generatorOutput = Nil, logicalPlan)\n    }\n  }\n## 列操作\n  /**\n   * Returns a new Dataset by adding a column or replacing the existing column that has\n   * the same name.\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  列操作\n  def withColumn(colName: String, col: Column): DataFrame = {\n    val resolver = sparkSession.sessionState.analyzer.resolver\n    val output = queryExecution.analyzed.output\n    val shouldReplace = output.exists(f => resolver(f.name, colName))\n    if (shouldReplace) {\n      val columns = output.map { field =>\n        if (resolver(field.name, colName)) {\n          col.as(colName)\n        } else {\n          Column(field)\n        }\n      }\n      select(columns : _*)\n    } else {\n      select(Column(\"*\"), col.as(colName))\n    }\n  }\n\n  /**\n   * Returns a new Dataset by adding a column with metadata.\n   */\n  private[spark] def withColumn(colName: String, col: Column, metadata: Metadata): DataFrame = {\n    withColumn(colName, col.as(colName, metadata))\n  }\n\n  /**\n   * Returns a new Dataset with a column renamed.\n   * This is a no-op if schema doesn't contain existingName.\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  def withColumnRenamed(existingName: String, newName: String): DataFrame = {\n    val resolver = sparkSession.sessionState.analyzer.resolver\n    val output = queryExecution.analyzed.output\n    val shouldRename = output.exists(f => resolver(f.name, existingName))\n    if (shouldRename) {\n      val columns = output.map { col =>\n        if (resolver(col.name, existingName)) {\n          Column(col).as(newName)\n        } else {\n          Column(col)\n        }\n      }\n      select(columns : _*)\n    } else {\n      toDF()\n    }\n  }\n\n  /**\n   * Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain\n   * column name.\n   *\n   * This method can only be used to drop top level columns. the colName string is treated\n   * literally without further interpretation.\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  删除指定列\n  def drop(colName: String): DataFrame = {\n    drop(Seq(colName) : _*)\n  }\n\n  /**\n   * Returns a new Dataset with columns dropped.\n   * This is a no-op if schema doesn't contain column name(s).\n   *\n   * This method can only be used to drop top level columns. the colName string is treated literally\n   * without further interpretation.\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def drop(colNames: String*): DataFrame = {\n    val resolver = sparkSession.sessionState.analyzer.resolver\n    val allColumns = queryExecution.analyzed.output\n    val remainingCols = allColumns.filter { attribute =>\n      colNames.forall(n => !resolver(attribute.name, n))\n    }.map(attribute => Column(attribute))\n    if (remainingCols.size == allColumns.size) {\n      toDF()\n    } else {\n      this.select(remainingCols: _*)\n    }\n  }\n\n  /**\n   * Returns a new Dataset with a column dropped.\n   * This version of drop accepts a [[Column]] rather than a name.\n   * This is a no-op if the Dataset doesn't have a column\n   * with an equivalent expression.\n   *\n   * @group untypedrel\n   * @since 2.0.0\n   */\n  def drop(col: Column): DataFrame = {\n    val expression = col match {\n      case Column(u: UnresolvedAttribute) =>\n        queryExecution.analyzed.resolveQuoted(\n          u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u)\n      case Column(expr: Expression) => expr\n    }\n    val attrs = this.logicalPlan.output\n    val colsAfterDrop = attrs.filter { attr =>\n      attr != expression\n    }.map(attr => Column(attr))\n    select(colsAfterDrop : _*)\n  }\n```\n## 去重\n```\n  /**\n   * Returns a new Dataset that contains only the unique rows from this Dataset.\n   * This is an alias for `distinct`.\n   *\n   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it\n   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use\n   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit\n   * the state. In addition, too late data older than watermark will be dropped to avoid any\n   * possibility of duplicates.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  删除重复的行\n  def dropDuplicates(): Dataset[T] = dropDuplicates(this.columns)\n\n  /**\n   * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only\n   * the subset of columns.\n   *\n   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it\n   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use\n   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit\n   * the state. In addition, too late data older than watermark will be dropped to avoid any\n   * possibility of duplicates.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  def dropDuplicates(colNames: Seq[String]): Dataset[T] = withTypedPlan {\n    val resolver = sparkSession.sessionState.analyzer.resolver\n    val allColumns = queryExecution.analyzed.output\n    val groupCols = colNames.toSet.toSeq.flatMap { (colName: String) =>\n      // It is possibly there are more than one columns with the same name,\n      // so we call filter instead of find.\n      val cols = allColumns.filter(col => resolver(col.name, colName))\n      if (cols.isEmpty) {\n        throw new AnalysisException(\n          s\"\"\"Cannot resolve column name \"$colName\" among (${schema.fieldNames.mkString(\", \")})\"\"\")\n      }\n      cols\n    }\n    Deduplicate(groupCols, logicalPlan, isStreaming)\n  }\n\n  /**\n   * Returns a new Dataset with duplicate rows removed, considering only\n   * the subset of columns.\n   *\n   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it\n   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use\n   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit\n   * the state. In addition, too late data older than watermark will be dropped to avoid any\n   * possibility of duplicates.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  def dropDuplicates(colNames: Array[String]): Dataset[T] = dropDuplicates(colNames.toSeq)\n\n  /**\n   * Returns a new [[Dataset]] with duplicate rows removed, considering only\n   * the subset of columns.\n   *\n   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it\n   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use\n   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit\n   * the state. In addition, too late data older than watermark will be dropped to avoid any\n   * possibility of duplicates.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def dropDuplicates(col1: String, cols: String*): Dataset[T] = {\n    val colNames: Seq[String] = col1 +: cols\n    dropDuplicates(colNames)\n  }\n  ```\n## 统计指定的列\n    ```\n  /**\n   * Computes statistics for numeric and string columns, including count, mean, stddev, min, and\n   * max. If no columns are given, this function computes statistics for all numerical or string\n   * columns.\n   *\n   * This function is meant for exploratory data analysis, as we make no guarantee about the\n   * backward compatibility of the schema of the resulting Dataset. If you want to\n   * programmatically compute summary statistics, use the `agg` function instead.\n   *\n   * {{{\n   *   ds.describe(\"age\", \"height\").show()\n   *\n   *   // output:\n   *   // summary age   height\n   *   // count   10.0  10.0\n   *   // mean    53.3  178.05\n   *   // stddev  11.6  15.7\n   *   // min     18.0  163.0\n   *   // max     92.0  192.0\n   * }}}\n   *\n   * @group action\n   * @since 1.6.0\n   */\n  获得指定列的描述性统计量\n  @scala.annotation.varargs\n  def describe(cols: String*): DataFrame = withPlan {\n\n    // The list of summary statistics to compute, in the form of expressions.\n    val statistics = List[(String, Expression => Expression)](\n      \"count\" -> ((child: Expression) => Count(child).toAggregateExpression()),\n      \"mean\" -> ((child: Expression) => Average(child).toAggregateExpression()),\n      \"stddev\" -> ((child: Expression) => StddevSamp(child).toAggregateExpression()),\n      \"min\" -> ((child: Expression) => Min(child).toAggregateExpression()),\n      \"max\" -> ((child: Expression) => Max(child).toAggregateExpression()))\n\n    val outputCols =\n      (if (cols.isEmpty) aggregatableColumns.map(usePrettyExpression(_).sql) else cols).toList\n\n    val ret: Seq[Row] = if (outputCols.nonEmpty) {\n      val aggExprs = statistics.flatMap { case (_, colToAgg) =>\n        outputCols.map(c => Column(Cast(colToAgg(Column(c).expr), StringType)).as(c))\n      }\n\n      val row = groupBy().agg(aggExprs.head, aggExprs.tail: _*).head().toSeq\n\n      // Pivot the data so each summary is one row\n      row.grouped(outputCols.size).toSeq.zip(statistics).map { case (aggregation, (statistic, _)) =>\n        Row(statistic :: aggregation.toList: _*)\n      }\n    } else {\n      // If there are no output columns, just output a single column that contains the stats.\n      statistics.map { case (name, _) => Row(name) }\n    }\n\n    // All columns are string type\n    val schema = StructType(\n      StructField(\"summary\", StringType) :: outputCols.map(StructField(_, StringType))).toAttributes\n    // `toArray` forces materialization to make the seq serializable\n    LocalRelation.fromExternalRows(schema, ret.toArray.toSeq)\n  }\n\n  /**\n   * Returns the first `n` rows.\n   *\n   * @note this method should only be used if the resulting array is expected to be small, as\n   * all the data is loaded into the driver's memory.\n   *\n   * @group action\n   * @since 1.6.0\n   */\n  取得前n行数据\n  def head(n: Int): Array[T] = withAction(\"head\", limit(n).queryExecution)(collectFromPlan)\n\n  /**\n   * Returns the first row.\n   * @group action\n   * @since 1.6.0\n   */\n  def head(): T = head(1).head\n\n  /**\n   * Returns the first row. Alias for head().\n   * @group action\n   * @since 1.6.0\n   */\n  def first(): T = head()\n\n  /**\n   * Concise syntax for chaining custom transformations.\n   * {{{\n   *   def featurize(ds: Dataset[T]): Dataset[U] = ...\n   *\n   *   ds\n   *     .transform(featurize)\n   *     .transform(...)\n   * }}}\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  转换？？\n  def transform[U](t: Dataset[T] => Dataset[U]): Dataset[U] = t(this)\n\n  /**\n   * :: Experimental ::\n   * (Scala-specific)\n   * Returns a new Dataset that only contains elements where `func` returns `true`.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  过滤\n  @Experimental\n  @InterfaceStability.Evolving\n  def filter(func: T => Boolean): Dataset[T] = {\n    withTypedPlan(TypedFilter(func, logicalPlan))\n  }\n\n  /**\n   * :: Experimental ::\n   * (Java-specific)\n   * Returns a new Dataset that only contains elements where `func` returns `true`.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def filter(func: FilterFunction[T]): Dataset[T] = {\n    withTypedPlan(TypedFilter(func, logicalPlan))\n  }\n```\n## 数据的转换\n  ```\n  /**\n   * :: Experimental ::\n   * (Scala-specific)\n   * Returns a new Dataset that contains the result of applying `func` to each element.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  映射操作\n  @Experimental\n  @InterfaceStability.Evolving\n  def map[U : Encoder](func: T => U): Dataset[U] = withTypedPlan {\n    MapElements[T, U](func, logicalPlan)\n  }\n\n  /**\n   * :: Experimental ::\n   * (Java-specific)\n   * Returns a new Dataset that contains the result of applying `func` to each element.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {\n    implicit val uEnc = encoder\n    withTypedPlan(MapElements[T, U](func, logicalPlan))\n  }\n\n  /**\n   * :: Experimental ::\n   * (Scala-specific)\n   * Returns a new Dataset that contains the result of applying `func` to each partition.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def mapPartitions[U : Encoder](func: Iterator[T] => Iterator[U]): Dataset[U] = {\n    new Dataset[U](\n      sparkSession,\n      MapPartitions[T, U](func, logicalPlan),\n      implicitly[Encoder[U]])\n  }\n\n  /**\n   * :: Experimental ::\n   * (Java-specific)\n   * Returns a new Dataset that contains the result of applying `f` to each partition.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U] = {\n    val func: (Iterator[T]) => Iterator[U] = x => f.call(x.asJava).asScala\n    mapPartitions(func)(encoder)\n  }\n\n  /**\n   * Returns a new `DataFrame` that contains the result of applying a serialized R function\n   * `func` to each partition.\n   */\n  private[sql] def mapPartitionsInR(\n      func: Array[Byte],\n      packageNames: Array[Byte],\n      broadcastVars: Array[Broadcast[Object]],\n      schema: StructType): DataFrame = {\n    val rowEncoder = encoder.asInstanceOf[ExpressionEncoder[Row]]\n    Dataset.ofRows(\n      sparkSession,\n      MapPartitionsInR(func, packageNames, broadcastVars, schema, rowEncoder, logicalPlan))\n  }\n\n  /**\n   * :: Experimental ::\n   * (Scala-specific)\n   * Returns a new Dataset by first applying a function to all elements of this Dataset,\n   * and then flattening the results.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def flatMap[U : Encoder](func: T => TraversableOnce[U]): Dataset[U] =\n    mapPartitions(_.flatMap(func))\n\n  /**\n   * :: Experimental ::\n   * (Java-specific)\n   * Returns a new Dataset by first applying a function to all elements of this Dataset,\n   * and then flattening the results.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {\n    val func: (T) => Iterator[U] = x => f.call(x).asScala\n    flatMap(func)(encoder)\n  }\n\n  /**\n   * Applies a function `f` to all rows.\n   *\n   * @group action\n   * @since 1.6.0\n   */\n  def foreach(f: T => Unit): Unit = withNewExecutionId {\n    rdd.foreach(f)\n  }\n\n  /**\n   * (Java-specific)\n   * Runs `func` on each element of this Dataset.\n   *\n   * @group action\n   * @since 1.6.0\n   */\n  def foreach(func: ForeachFunction[T]): Unit = foreach(func.call(_))\n\n  /**\n   * Applies a function `f` to each partition of this Dataset.\n   *\n   * @group action\n   * @since 1.6.0\n   */\n  def foreachPartition(f: Iterator[T] => Unit): Unit = withNewExecutionId {\n    rdd.foreachPartition(f)\n  }\n```\n## 数据的提取与聚合\n  ```\n  /**\n   * (Java-specific)\n   * Runs `func` on each partition of this Dataset.\n   *\n   * @group action\n   * @since 1.6.0\n   */\n  def foreachPartition(func: ForeachPartitionFunction[T]): Unit =\n    foreachPartition(it => func.call(it.asJava))\n\n  /**\n   * Returns the first `n` rows in the Dataset.\n   *\n   * Running take requires moving data into the application's driver process, and doing so with\n   * a very large `n` can crash the driver process with OutOfMemoryError.\n   *\n   * @group action\n   * @since 1.6.0\n   */\n  def take(n: Int): Array[T] = head(n)\n\n  /**\n   * Returns the first `n` rows in the Dataset as a list.\n   *\n   * Running take requires moving data into the application's driver process, and doing so with\n   * a very large `n` can crash the driver process with OutOfMemoryError.\n   *\n   * @group action\n   * @since 1.6.0\n   */\n    获取数据成一个列表\n  def takeAsList(n: Int): java.util.List[T] = java.util.Arrays.asList(take(n) : _*)\n\n  /**\n   * Returns an array that contains all rows in this Dataset.\n   *\n   * Running collect requires moving all the data into the application's driver process, and\n   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.\n   *\n   * For Java API, use [[collectAsList]].\n   *\n   * @group action\n   * @since 1.6.0\n   */\n\n统计数据\n  def collect(): Array[T] = withAction(\"collect\", queryExecution)(collectFromPlan)\n\n  /**\n   * Returns a Java list that contains all rows in this Dataset.\n   *\n   * Running collect requires moving all the data into the application's driver process, and\n   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.\n   *\n   * @group action\n   * @since 1.6.0\n   */\n  def collectAsList(): java.util.List[T] = withAction(\"collectAsList\", queryExecution) { plan =>\n    val values = collectFromPlan(plan)\n    java.util.Arrays.asList(values : _*)\n  }\n\n  /**\n   * Return an iterator that contains all rows in this Dataset.\n   *\n   * The iterator will consume as much memory as the largest partition in this Dataset.\n   *\n   * @note this results in multiple Spark jobs, and if the input Dataset is the result\n   * of a wide transformation (e.g. join with different partitioners), to avoid\n   * recomputing the input Dataset should be cached first.\n   *\n   * @group action\n   * @since 2.0.0\n   */\n  def toLocalIterator(): java.util.Iterator[T] = {\n    withAction(\"toLocalIterator\", queryExecution) { plan =>\n      plan.executeToIterator().map(boundEnc.fromRow).asJava\n    }\n  }\n\n  /**\n   * Returns the number of rows in the Dataset.\n   * @group action\n   * @since 1.6.0\n   */\n  统计行\n  def count(): Long = withAction(\"count\", groupBy().count().queryExecution) { plan =>\n    plan.executeCollect().head.getLong(0)\n  }\n  ```\n## 分区\n    ```\n  /**\n   * Returns a new Dataset that has exactly `numPartitions` partitions.\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  分区\n  def repartition(numPartitions: Int): Dataset[T] = withTypedPlan {\n    Repartition(numPartitions, shuffle = true, logicalPlan)\n  }\n\n  /**\n   * Returns a new Dataset partitioned by the given partitioning expressions into\n   * `numPartitions`. The resulting Dataset is hash partitioned.\n   *\n   * This is the same operation as \"DISTRIBUTE BY\" in SQL (Hive QL).\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T] = withTypedPlan {\n    RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, numPartitions)\n  }\n\n  /**\n   * Returns a new Dataset partitioned by the given partitioning expressions, using\n   * `spark.sql.shuffle.partitions` as number of partitions.\n   * The resulting Dataset is hash partitioned.\n   *\n   * This is the same operation as \"DISTRIBUTE BY\" in SQL (Hive QL).\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  @scala.annotation.varargs\n  def repartition(partitionExprs: Column*): Dataset[T] = withTypedPlan {\n    RepartitionByExpression(\n      partitionExprs.map(_.expr), logicalPlan, sparkSession.sessionState.conf.numShufflePartitions)\n  }\n\n  /**\n   * Returns a new Dataset that has exactly `numPartitions` partitions.\n   * Similar to coalesce defined on an `RDD`, this operation results in a narrow dependency, e.g.\n   * if you go from 1000 partitions to 100 partitions, there will not be a shuffle, instead each of\n   * the 100 new partitions will claim 10 of the current partitions.  If a larger number of\n   * partitions is requested, it will stay at the current number of partitions.\n   *\n   * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,\n   * this may result in your computation taking place on fewer nodes than\n   * you like (e.g. one node in the case of numPartitions = 1). To avoid this,\n   * you can call repartition. This will add a shuffle step, but means the\n   * current upstream partitions will be executed in parallel (per whatever\n   * the current partitioning is).\n   *\n   * @group typedrel\n   * @since 1.6.0\n   */\n  def coalesce(numPartitions: Int): Dataset[T] = withTypedPlan {\n    Repartition(numPartitions, shuffle = false, logicalPlan)\n  }\n\n  /**\n   * Returns a new Dataset that contains only the unique rows from this Dataset.\n   * This is an alias for `dropDuplicates`.\n   *\n   * @note Equality checking is performed directly on the encoded representation of the data\n   * and thus is not affected by a custom `equals` function defined on `T`.\n   *\n   * @group typedrel\n   * @since 2.0.0\n   */\n  def distinct(): Dataset[T] = dropDuplicates()\n\n```\n## 数据的持久化\n  数据的持久化和缓存策略，一般我们操作rdd都是延迟计算，但是当我们多次重复使用一个rdd的时候可以选择将其缓存而不是每次进行一个计算，可以提高效率。\n\n```\n  /**\n   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n                  持久化\n  def persist(): this.type = {\n    sparkSession.sharedState.cacheManager.cacheQuery(this)\n    this\n  }\n\n  /**\n   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  def cache(): this.type = persist()\n\n  /**\n   * Persist this Dataset with the given storage level.\n   * @param newLevel One of: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`,\n   *                 `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`,\n   *                 `MEMORY_AND_DISK_2`, etc.\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  def persist(newLevel: StorageLevel): this.type = {\n    sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel)\n    this\n  }\n\n  /**\n   * Get the Dataset's current storage level, or StorageLevel.NONE if not persisted.\n   *\n   * @group basic\n   * @since 2.1.0\n   */\n  def storageLevel: StorageLevel = {\n    sparkSession.sharedState.cacheManager.lookupCachedData(this).map { cachedData =>\n      cachedData.cachedRepresentation.storageLevel\n    }.getOrElse(StorageLevel.NONE)\n  }\n\n  /**\n   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.\n   *\n   * @param blocking Whether to block until all blocks are deleted.\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  def unpersist(blocking: Boolean): this.type = {\n    sparkSession.sharedState.cacheManager.uncacheQuery(this, blocking)\n    this\n  }\n\n  /**\n   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  def unpersist(): this.type = unpersist(blocking = false)\n\n  /**\n   * Represents the content of the Dataset as an `RDD` of `T`.\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  lazy val rdd: RDD[T] = {\n    val objectType = exprEnc.deserializer.dataType\n    val deserialized = CatalystSerde.deserialize[T](logicalPlan)\n    sparkSession.sessionState.executePlan(deserialized).toRdd.mapPartitions { rows =>\n      rows.map(_.get(0, objectType).asInstanceOf[T])\n    }\n  }\n\n  /**\n   * Returns the content of the Dataset as a `JavaRDD` of `T`s.\n   * @group basic\n   * @since 1.6.0\n   */\n  def toJavaRDD: JavaRDD[T] = rdd.toJavaRDD()\n\n  /**\n   * Returns the content of the Dataset as a `JavaRDD` of `T`s.\n   * @group basic\n   * @since 1.6.0\n   */\n  def javaRDD: JavaRDD[T] = toJavaRDD\n```\n## 注册临时表\n通过注册可以将一个dataset直接当作一个表来操作，这样就可以直接通过sql来执行了，不过返回的结果又是一个dataset\n```\n  /**\n   * Registers this Dataset as a temporary table using the given name. The lifetime of this\n   * temporary table is tied to the [[SparkSession]] that was used to create this Dataset.\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  注册\n  @deprecated(\"Use createOrReplaceTempView(viewName) instead.\", \"2.0.0\")\n  def registerTempTable(tableName: String): Unit = {\n    createOrReplaceTempView(tableName)\n  }\n\n  /**\n   * Creates a local temporary view using the given name. The lifetime of this\n   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.\n   *\n   * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that\n   * created it, i.e. it will be automatically dropped when the session terminates. It's not\n   * tied to any databases, i.e. we can't use `db1.view1` to reference a local temporary view.\n   *\n   * @throws AnalysisException if the view name is invalid or already exists\n   *\n   * @group basic\n   * @since 2.0.0\n   */\n  创建表\n  @throws[AnalysisException]\n  def createTempView(viewName: String): Unit = withPlan {\n    createTempViewCommand(viewName, replace = false, global = false)\n  }\n\n\n\n  /**\n   * Creates a local temporary view using the given name. The lifetime of this\n   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.\n   *\n   * @group basic\n   * @since 2.0.0\n   */\n  def createOrReplaceTempView(viewName: String): Unit = withPlan {\n    createTempViewCommand(viewName, replace = true, global = false)\n  }\n\n  /**\n   * Creates a global temporary view using the given name. The lifetime of this\n   * temporary view is tied to this Spark application.\n   *\n   * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,\n   * i.e. it will be automatically dropped when the application terminates. It's tied to a system\n   * preserved database `global_temp`, and we must use the qualified name to refer a global temp\n   * view, e.g. `SELECT * FROM global_temp.view1`.\n   *\n   * @throws AnalysisException if the view name is invalid or already exists\n   *\n   * @group basic\n   * @since 2.1.0\n   */\n  @throws[AnalysisException]\n  def createGlobalTempView(viewName: String): Unit = withPlan {\n    createTempViewCommand(viewName, replace = false, global = true)\n  }\n\n  private def createTempViewCommand(\n      viewName: String,\n      replace: Boolean,\n      global: Boolean): CreateViewCommand = {\n    val viewType = if (global) GlobalTempView else LocalTempView\n\n    val tableIdentifier = try {\n      sparkSession.sessionState.sqlParser.parseTableIdentifier(viewName)\n    } catch {\n      case _: ParseException => throw new AnalysisException(s\"Invalid view name: $viewName\")\n    }\n    CreateViewCommand(\n      name = tableIdentifier,\n      userSpecifiedColumns = Nil,\n      comment = None,\n      properties = Map.empty,\n      originalText = None,\n      child = logicalPlan,\n      allowExisting = false,\n      replace = replace,\n      viewType = viewType)\n  }\n  ```\n## 数据保存\n    数据保存有一个专门的write类来处理，这里就是调用write方法返回一个write对象来实现的\n```\n  /**\n   * Interface for saving the content of the non-streaming Dataset out into external storage.\n   *\n   * @group basic\n   * @since 1.6.0\n   */\n  def write: DataFrameWriter[T] = {\n    if (isStreaming) {\n      logicalPlan.failAnalysis(\n        \"'write' can not be called on streaming Dataset/DataFrame\")\n    }\n    new DataFrameWriter[T](this)\n  }\n\n  /**\n   * :: Experimental ::\n   * Interface for saving the content of the streaming Dataset out into external storage.\n   *\n   * @group basic\n   * @since 2.0.0\n   */\n  @Experimental\n  @InterfaceStability.Evolving\n  def writeStream: DataStreamWriter[T] = {\n    if (!isStreaming) {\n      logicalPlan.failAnalysis(\n        \"'writeStream' can be called only on streaming Dataset/DataFrame\")\n    }\n    new DataStreamWriter[T](this)\n  }\n\n```\n## 转换成json格式\n```\n  /**\n   * Returns the content of the Dataset as a Dataset of JSON strings.\n   * @since 2.0.0\n   */\n  def toJSON: Dataset[String] = {\n    val rowSchema = this.schema\n    val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone\n    val rdd: RDD[String] = queryExecution.toRdd.mapPartitions { iter =>\n      val writer = new CharArrayWriter()\n      // create the Generator without separator inserted between 2 records\n      val gen = new JacksonGenerator(rowSchema, writer,\n        new JSONOptions(Map.empty[String, String], sessionLocalTimeZone))\n\n      new Iterator[String] {\n        override def hasNext: Boolean = iter.hasNext\n        override def next(): String = {\n          gen.write(iter.next())\n          gen.flush()\n\n          val json = writer.toString\n          if (hasNext) {\n            writer.reset()\n          } else {\n            gen.close()\n          }\n\n          json\n        }\n      }\n    }\n    import sparkSession.implicits.newStringEncoder\n    sparkSession.createDataset(rdd)\n  }\n  ```\n## 获取文件列表 \n  可以获取当前dataSet都加载了那些文件\n  ```\n  /**\n   * Returns a best-effort snapshot of the files that compose this Dataset. This method simply\n   * asks each constituent BaseRelation for its respective files and takes the union of all results.\n   * Depending on the source relations, this may not find all input files. Duplicates are removed.\n   *\n   * @group basic\n   * @since 2.0.0\n   */\n  def inputFiles: Array[String] = {\n    val files: Seq[String] = queryExecution.optimizedPlan.collect {\n      case LogicalRelation(fsBasedRelation: FileRelation, _, _) =>\n        fsBasedRelation.inputFiles\n      case fr: FileRelation =>\n        fr.inputFiles\n    }.flatten\n    files.toSet.toArray\n  }\n```\n","slug":"spark1","published":1,"updated":"2017-03-11T23:38:34.813Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj70mt11o001g60tusrxx6308","content":"<p>记录一下sparksql的dataframe 中常用的操作，spark在大数据处理方面有很广泛的应供，每天都在研究spark的源码，简单记录一下以便后续查阅,今天先简单整理一下，后续逐步完善.<br>版本:spark 2.0.1<br><a id=\"more\"></a></p>\n<h2 id=\"数据显示\"><a href=\"#数据显示\" class=\"headerlink\" title=\"数据显示\"></a>数据显示</h2><p>这个showString 是spark内部的方法，我们实际是调用不到的，但是我们调用的show方法最终都是调用了这个showString<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Compose the string representing rows for output</div><div class=\"line\"> *</div><div class=\"line\"> * @param _numRows Number of rows to show</div><div class=\"line\"> * @param truncate If set to more than 0, truncates strings to `truncate` characters and</div><div class=\"line\"> *                   all cells will be aligned right.</div><div class=\"line\"> */</div><div class=\"line\">private[sql] def showString(_numRows: Int, truncate: Int = 20): String</div></pre></td></tr></table></figure></p>\n<h2 id=\"将dataSet转换成dataFrame\"><a href=\"#将dataSet转换成dataFrame\" class=\"headerlink\" title=\"将dataSet转换成dataFrame\"></a>将dataSet转换成dataFrame</h2><p>   datafram其实是按列来存储的dataset<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed.</div><div class=\"line\"> * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with</div><div class=\"line\"> * meaningful names. For example:</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   val rdd: RDD[(Int, String)] = ...</div><div class=\"line\"> *   rdd.toDF()  // this implicit conversion creates a DataFrame with column name `_1` and `_2`</div><div class=\"line\"> *   rdd.toDF(&quot;id&quot;, &quot;name&quot;)  // this creates a DataFrame with column name &quot;id&quot; and &quot;name&quot;</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@scala.annotation.varargs</div><div class=\"line\">def toDF(colNames: String*): DataFrame = &#123;</div><div class=\"line\">  require(schema.size == colNames.size,</div><div class=\"line\">    &quot;The number of columns doesn&apos;t match.\\n&quot; +</div><div class=\"line\">      s&quot;Old column names ($&#123;schema.size&#125;): &quot; + schema.fields.map(_.name).mkString(&quot;, &quot;) + &quot;\\n&quot; +</div><div class=\"line\">      s&quot;New column names ($&#123;colNames.size&#125;): &quot; + colNames.mkString(&quot;, &quot;))</div><div class=\"line\"></div><div class=\"line\">  val newCols = logicalPlan.output.zip(colNames).map &#123; case (oldAttribute, newName) =&gt;</div><div class=\"line\">    Column(oldAttribute).as(newName)</div><div class=\"line\">  &#125;</div><div class=\"line\">  select(newCols : _*)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>输出当前dataset的结构信息<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Returns the schema of this Dataset.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def schema: StructType = queryExecution.analyzed.schema</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Prints the schema to the console in a nice tree format.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">// scalastyle:off println</div><div class=\"line\">def printSchema(): Unit = println(schema.treeString)</div><div class=\"line\">// scalastyle:on println</div></pre></td></tr></table></figure></p>\n<p>输出一些调试信息<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Prints the plans (logical and physical) to the console for debugging purposes.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def explain(extended: Boolean): Unit = &#123;</div><div class=\"line\">  val explain = ExplainCommand(queryExecution.logical, extended = extended)</div><div class=\"line\">  sparkSession.sessionState.executePlan(explain).executedPlan.executeCollect().foreach &#123;</div><div class=\"line\">    // scalastyle:off println</div><div class=\"line\">    r =&gt; println(r.getString(0))</div><div class=\"line\">    // scalastyle:on println</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Prints the physical plan to the console for debugging purposes.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def explain(): Unit = explain(extended = false)</div></pre></td></tr></table></figure></p>\n<p>输出列名以及每个列的类型<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Returns all column names and their data types as an array.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def dtypes: Array[(String, String)] = schema.fields.map &#123; field =&gt;</div><div class=\"line\">  (field.name, field.dataType.toString)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns all column names as an array.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def columns: Array[String] = schema.fields.map(_.name)</div></pre></td></tr></table></figure></p>\n<p>是否能够获取数据<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Returns true if the `collect` and `take` methods can be run locally</div><div class=\"line\"> * (without any Spark executors).</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation]</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns true if this Dataset contains one or more sources that continuously</div><div class=\"line\"> * return data as it arrives. A Dataset that reads data from a streaming source</div><div class=\"line\"> * must be executed as a `StreamingQuery` using the `start()` method in</div><div class=\"line\"> * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or</div><div class=\"line\"> * `collect()`, will throw an [[AnalysisException]] when there is a streaming</div><div class=\"line\"> * source present.</div><div class=\"line\"> *</div><div class=\"line\"> * @group streaming</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def isStreaming: Boolean = logicalPlan.isStreaming</div></pre></td></tr></table></figure></p>\n<p><strong>检查点，以前没有用过，需要在研究一下</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate</div><div class=\"line\"> * the logical plan of this Dataset, which is especially useful in iterative algorithms where the</div><div class=\"line\"> * plan may grow exponentially. It will be saved to files inside the checkpoint</div><div class=\"line\"> * directory set with `SparkContext#setCheckpointDir`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 2.1.0</div><div class=\"line\"> */</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def checkpoint(): Dataset[T] = checkpoint(eager = true)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the</div><div class=\"line\"> * logical plan of this Dataset, which is especially useful in iterative algorithms where the</div><div class=\"line\"> * plan may grow exponentially. It will be saved to files inside the checkpoint</div><div class=\"line\"> * directory set with `SparkContext#setCheckpointDir`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 2.1.0</div><div class=\"line\"> */</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def checkpoint(eager: Boolean): Dataset[T] = &#123;</div><div class=\"line\">  val internalRdd = queryExecution.toRdd.map(_.copy())</div><div class=\"line\">  internalRdd.checkpoint()</div><div class=\"line\"></div><div class=\"line\">  if (eager) &#123;</div><div class=\"line\">    internalRdd.count()</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  val physicalPlan = queryExecution.executedPlan</div></pre></td></tr></table></figure></p>\n<p>这个什么鬼需要在分析一下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div></pre></td><td class=\"code\"><pre><div class=\"line\">  // Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the</div><div class=\"line\">  // size of `PartitioningCollection` may grow exponentially for queries involving deep inner</div><div class=\"line\">  // joins.</div><div class=\"line\">  def firstLeafPartitioning(partitioning: Partitioning): Partitioning = &#123;</div><div class=\"line\">    partitioning match &#123;</div><div class=\"line\">      case p: PartitioningCollection =&gt; firstLeafPartitioning(p.partitionings.head)</div><div class=\"line\">      case p =&gt; p</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  val outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning)</div><div class=\"line\"></div><div class=\"line\">  Dataset.ofRows(</div><div class=\"line\">    sparkSession,</div><div class=\"line\">    LogicalRDD(</div><div class=\"line\">      logicalPlan.output,</div><div class=\"line\">      internalRdd,</div><div class=\"line\">      outputPartitioning,</div><div class=\"line\">      physicalPlan.outputOrdering</div><div class=\"line\">    )(sparkSession)).as[T]</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>水印？？？<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time</div><div class=\"line\"> * before which we assume no more late data is going to arrive.</div><div class=\"line\"> *</div><div class=\"line\"> * Spark will use this watermark for several purposes:</div><div class=\"line\"> *  - To know when a given time window aggregation can be finalized and thus can be emitted when</div><div class=\"line\"> *    using output modes that do not allow updates.</div><div class=\"line\"> *  - To minimize the amount of state that we need to keep for on-going aggregations,</div><div class=\"line\"> *    `mapGroupsWithState` and `dropDuplicates` operators.</div><div class=\"line\"> *</div><div class=\"line\"> *  The current watermark is computed by looking at the `MAX(eventTime)` seen across</div><div class=\"line\"> *  all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost</div><div class=\"line\"> *  of coordinating this value across partitions, the actual watermark used is only guaranteed</div><div class=\"line\"> *  to be at least `delayThreshold` behind the actual event time.  In some cases we may still</div><div class=\"line\"> *  process records that arrive more than `delayThreshold` late.</div><div class=\"line\"> *</div><div class=\"line\"> * @param eventTime the name of the column that contains the event time of the row.</div><div class=\"line\"> * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest</div><div class=\"line\"> *                       record that has been processed in the form of an interval</div><div class=\"line\"> *                       (e.g. &quot;1 minute&quot; or &quot;5 hours&quot;).</div><div class=\"line\"> *</div><div class=\"line\"> * @group streaming</div><div class=\"line\"> * @since 2.1.0</div><div class=\"line\"> */</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">// We only accept an existing column name, not a derived column here as a watermark that is</div><div class=\"line\">// defined on a derived column cannot referenced elsewhere in the plan.</div><div class=\"line\">def withWatermark(eventTime: String, delayThreshold: String): Dataset[T] = withTypedPlan &#123;</div><div class=\"line\">  val parsedDelay =</div><div class=\"line\">    Option(CalendarInterval.fromString(&quot;interval &quot; + delayThreshold))</div><div class=\"line\">      .getOrElse(throw new AnalysisException(s&quot;Unable to parse time delay &apos;$delayThreshold&apos;&quot;))</div><div class=\"line\">  EventTimeWatermark(UnresolvedAttribute(eventTime), parsedDelay, logicalPlan)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h2 id=\"数据展示\"><a href=\"#数据展示\" class=\"headerlink\" title=\"数据展示\"></a>数据展示</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div></pre></td><td class=\"code\"><pre><div class=\"line\">  /**</div><div class=\"line\">   * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated,</div><div class=\"line\">   * and all cells will be aligned right. For example:</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   year  month AVG(&apos;Adj Close) MAX(&apos;Adj Close)</div><div class=\"line\">   *   1980  12    0.503218        0.595103</div><div class=\"line\">   *   1981  01    0.523289        0.570307</div><div class=\"line\">   *   1982  02    0.436504        0.475256</div><div class=\"line\">   *   1983  03    0.410516        0.442194</div><div class=\"line\">   *   1984  04    0.450090        0.483521</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * @param numRows Number of rows to show</div><div class=\"line\">   *</div><div class=\"line\">   * @group action</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">显示datafram中的指定数量的数据，默认字段长度超过20位则截断。</div><div class=\"line\">  def show(numRows: Int): Unit = show(numRows, truncate = true)</div><div class=\"line\"></div><div class=\"line\">显示datafram的数据，默认取前面20条记录显示，默认字段长度超过20位则截断。</div><div class=\"line\">  </div><div class=\"line\">  def show(): Unit = show(20)</div><div class=\"line\"></div><div class=\"line\">显示datafram的数据，默认取前面20条记录显示，通过truncate选择是否需要全部显示每一列的信息。</div><div class=\"line\">  def show(truncate: Boolean): Unit = show(20, truncate)</div><div class=\"line\"></div><div class=\"line\">显示datafram的数据，numRows为显示数量，通过truncate选择是否需要全部显示每一列的信息。</div><div class=\"line\">  def show(numRows: Int, truncate: Boolean): Unit = if (truncate) &#123;</div><div class=\"line\">  def show(numRows: Int, truncate: Int): Unit = println(showString(numRows, truncate))</div></pre></td></tr></table></figure>\n<h2 id=\"数据的关联\"><a href=\"#数据的关联\" class=\"headerlink\" title=\"数据的关联\"></a>数据的关联</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div><div class=\"line\">122</div><div class=\"line\">123</div><div class=\"line\">124</div><div class=\"line\">125</div><div class=\"line\">126</div><div class=\"line\">127</div><div class=\"line\">128</div><div class=\"line\">129</div><div class=\"line\">130</div><div class=\"line\">131</div><div class=\"line\">132</div><div class=\"line\">133</div><div class=\"line\">134</div><div class=\"line\">135</div><div class=\"line\">136</div><div class=\"line\">137</div><div class=\"line\">138</div><div class=\"line\">139</div><div class=\"line\">140</div><div class=\"line\">141</div><div class=\"line\">142</div><div class=\"line\">143</div><div class=\"line\">144</div><div class=\"line\">145</div><div class=\"line\">146</div><div class=\"line\">147</div><div class=\"line\">148</div><div class=\"line\">149</div><div class=\"line\">150</div><div class=\"line\">151</div><div class=\"line\">152</div><div class=\"line\">153</div><div class=\"line\">154</div><div class=\"line\">155</div><div class=\"line\">156</div><div class=\"line\">157</div><div class=\"line\">158</div><div class=\"line\">159</div><div class=\"line\">160</div><div class=\"line\">161</div><div class=\"line\">162</div><div class=\"line\">163</div><div class=\"line\">164</div><div class=\"line\">165</div><div class=\"line\">166</div><div class=\"line\">167</div><div class=\"line\">168</div><div class=\"line\">169</div><div class=\"line\">170</div><div class=\"line\">171</div><div class=\"line\">172</div><div class=\"line\">173</div><div class=\"line\">174</div><div class=\"line\">175</div><div class=\"line\">176</div><div class=\"line\">177</div><div class=\"line\">178</div><div class=\"line\">179</div><div class=\"line\">180</div><div class=\"line\">181</div><div class=\"line\">182</div><div class=\"line\">183</div><div class=\"line\">184</div><div class=\"line\">185</div><div class=\"line\">186</div><div class=\"line\">187</div><div class=\"line\">188</div><div class=\"line\">189</div><div class=\"line\">190</div><div class=\"line\">191</div><div class=\"line\">192</div><div class=\"line\">193</div><div class=\"line\">194</div><div class=\"line\">195</div><div class=\"line\">196</div><div class=\"line\">197</div><div class=\"line\">198</div><div class=\"line\">199</div><div class=\"line\">200</div><div class=\"line\">201</div><div class=\"line\">202</div><div class=\"line\">203</div><div class=\"line\">204</div><div class=\"line\">205</div><div class=\"line\">206</div><div class=\"line\">207</div><div class=\"line\">208</div><div class=\"line\">209</div><div class=\"line\">210</div><div class=\"line\">211</div><div class=\"line\">212</div><div class=\"line\">213</div><div class=\"line\">214</div><div class=\"line\">215</div><div class=\"line\">216</div><div class=\"line\">217</div><div class=\"line\">218</div><div class=\"line\">219</div><div class=\"line\">220</div><div class=\"line\">221</div><div class=\"line\">222</div><div class=\"line\">223</div><div class=\"line\">224</div><div class=\"line\">225</div><div class=\"line\">226</div><div class=\"line\">227</div><div class=\"line\">228</div><div class=\"line\">229</div><div class=\"line\">230</div><div class=\"line\">231</div><div class=\"line\">232</div><div class=\"line\">233</div><div class=\"line\">234</div><div class=\"line\">235</div><div class=\"line\">236</div><div class=\"line\">237</div><div class=\"line\">238</div><div class=\"line\">239</div><div class=\"line\">240</div><div class=\"line\">241</div><div class=\"line\">242</div><div class=\"line\">243</div><div class=\"line\">244</div><div class=\"line\">245</div><div class=\"line\">246</div><div class=\"line\">247</div><div class=\"line\">248</div><div class=\"line\">249</div><div class=\"line\">250</div><div class=\"line\">251</div><div class=\"line\">252</div><div class=\"line\">253</div><div class=\"line\">254</div><div class=\"line\">255</div><div class=\"line\">256</div><div class=\"line\">257</div><div class=\"line\">258</div><div class=\"line\">259</div><div class=\"line\">260</div><div class=\"line\">261</div><div class=\"line\">262</div><div class=\"line\">263</div><div class=\"line\">264</div><div class=\"line\">265</div><div class=\"line\">266</div><div class=\"line\">267</div><div class=\"line\">268</div><div class=\"line\">269</div><div class=\"line\">270</div><div class=\"line\">271</div><div class=\"line\">272</div><div class=\"line\">273</div><div class=\"line\">274</div><div class=\"line\">275</div><div class=\"line\">276</div><div class=\"line\">277</div><div class=\"line\">278</div><div class=\"line\">279</div><div class=\"line\">280</div><div class=\"line\">281</div><div class=\"line\">282</div><div class=\"line\">283</div><div class=\"line\">284</div><div class=\"line\">285</div><div class=\"line\">286</div><div class=\"line\">287</div><div class=\"line\">288</div><div class=\"line\">289</div><div class=\"line\">290</div><div class=\"line\">291</div><div class=\"line\">292</div><div class=\"line\">293</div><div class=\"line\">294</div><div class=\"line\">295</div><div class=\"line\">296</div><div class=\"line\">297</div><div class=\"line\">298</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\">返回dataset中空值操作算子</div><div class=\"line\">  def na: DataFrameNaFunctions = new DataFrameNaFunctions(toDF())</div><div class=\"line\"></div><div class=\"line\">返回dataset中统计操作算子</div><div class=\"line\">  def stat: DataFrameStatFunctions = new DataFrameStatFunctions(toDF())</div><div class=\"line\">关联dataframe</div><div class=\"line\">  def join(right: Dataset[_]): DataFrame = withPlan &#123;</div><div class=\"line\">    Join(logicalPlan, right.logicalPlan, joinType = Inner, None)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Inner equi-join with another `DataFrame` using the given column.</div><div class=\"line\">   *</div><div class=\"line\">   * Different from other join functions, the join column will only appear once in the output,</div><div class=\"line\">   * i.e. similar to SQL&apos;s `JOIN USING` syntax.</div><div class=\"line\">   *</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   // Joining df1 and df2 using the column &quot;user_id&quot;</div><div class=\"line\">   *   df1.join(df2, &quot;user_id&quot;)</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * @param right Right side of the join operation.</div><div class=\"line\">   * @param usingColumn Name of the column to join on. This column must exist on both sides.</div><div class=\"line\">   *</div><div class=\"line\">   * @note If you perform a self-join using this function without aliasing the input</div><div class=\"line\">   * `DataFrame`s, you will NOT be able to reference any columns after the join, since</div><div class=\"line\">   * there is no way to disambiguate which side of the join you would like to reference.</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">指定字段关联</div><div class=\"line\">  def join(right: Dataset[_], usingColumn: String): DataFrame = &#123;</div><div class=\"line\">    join(right, Seq(usingColumn))</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Inner equi-join with another `DataFrame` using the given columns.</div><div class=\"line\">   *</div><div class=\"line\">   * Different from other join functions, the join columns will only appear once in the output,</div><div class=\"line\">   * i.e. similar to SQL&apos;s `JOIN USING` syntax.</div><div class=\"line\">   *</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   // Joining df1 and df2 using the columns &quot;user_id&quot; and &quot;user_name&quot;</div><div class=\"line\">   *   df1.join(df2, Seq(&quot;user_id&quot;, &quot;user_name&quot;))</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * @param right Right side of the join operation.</div><div class=\"line\">   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.</div><div class=\"line\">   *</div><div class=\"line\">   * @note If you perform a self-join using this function without aliasing the input</div><div class=\"line\">   * `DataFrame`s, you will NOT be able to reference any columns after the join, since</div><div class=\"line\">   * there is no way to disambiguate which side of the join you would like to reference.</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">指定关联字段，不同dataframe中同名字段的key不重复出现</div><div class=\"line\">  def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame = &#123;</div><div class=\"line\">    join(right, usingColumns, &quot;inner&quot;)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Equi-join with another `DataFrame` using the given columns. A cross join with a predicate</div><div class=\"line\">   * is specified as an inner join. If you would explicitly like to perform a cross join use the</div><div class=\"line\">   * `crossJoin` method.</div><div class=\"line\">   *</div><div class=\"line\">   * Different from other join functions, the join columns will only appear once in the output,</div><div class=\"line\">   * i.e. similar to SQL&apos;s `JOIN USING` syntax.</div><div class=\"line\">   *</div><div class=\"line\">   * @param right Right side of the join operation.</div><div class=\"line\">   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.</div><div class=\"line\">   * @param joinType Type of join to perform. Default `inner`. Must be one of:</div><div class=\"line\">   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,</div><div class=\"line\">   *                 `right`, `right_outer`, `left_semi`, `left_anti`.</div><div class=\"line\">   *</div><div class=\"line\">   * @note If you perform a self-join using this function without aliasing the input</div><div class=\"line\">   * `DataFrame`s, you will NOT be able to reference any columns after the join, since</div><div class=\"line\">   * there is no way to disambiguate which side of the join you would like to reference.</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">指定关联的类型</div><div class=\"line\">  def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame = &#123;</div><div class=\"line\">    // Analyze the self join. The assumption is that the analyzer will disambiguate left vs right</div><div class=\"line\">    // by creating a new instance for one of the branch.</div><div class=\"line\">    val joined = sparkSession.sessionState.executePlan(</div><div class=\"line\">      Join(logicalPlan, right.logicalPlan, joinType = JoinType(joinType), None))</div><div class=\"line\">      .analyzed.asInstanceOf[Join]</div><div class=\"line\"></div><div class=\"line\">    withPlan &#123;</div><div class=\"line\">      Join(</div><div class=\"line\">        joined.left,</div><div class=\"line\">        joined.right,</div><div class=\"line\">        UsingJoin(JoinType(joinType), usingColumns),</div><div class=\"line\">        None)</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Inner join with another `DataFrame`, using the given join expression.</div><div class=\"line\">   *</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   // The following two are equivalent:</div><div class=\"line\">   *   df1.join(df2, $&quot;df1Key&quot; === $&quot;df2Key&quot;)</div><div class=\"line\">   *   df1.join(df2).where($&quot;df1Key&quot; === $&quot;df2Key&quot;)</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">通过表达式进行关联</div><div class=\"line\">  def join(right: Dataset[_], joinExprs: Column): DataFrame = join(right, joinExprs, &quot;inner&quot;)</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Join with another `DataFrame`, using the given join expression. The following performs</div><div class=\"line\">   * a full outer join between `df1` and `df2`.</div><div class=\"line\">   *</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   // Scala:</div><div class=\"line\">   *   import org.apache.spark.sql.functions._</div><div class=\"line\">   *   df1.join(df2, $&quot;df1Key&quot; === $&quot;df2Key&quot;, &quot;outer&quot;)</div><div class=\"line\">   *</div><div class=\"line\">   *   // Java:</div><div class=\"line\">   *   import static org.apache.spark.sql.functions.*;</div><div class=\"line\">   *   df1.join(df2, col(&quot;df1Key&quot;).equalTo(col(&quot;df2Key&quot;)), &quot;outer&quot;);</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * @param right Right side of the join.</div><div class=\"line\">   * @param joinExprs Join expression.</div><div class=\"line\">   * @param joinType Type of join to perform. Default `inner`. Must be one of:</div><div class=\"line\">   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,</div><div class=\"line\">   *                 `right`, `right_outer`, `left_semi`, `left_anti`.</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">通过列名表达式进行关联</div><div class=\"line\">  def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame = &#123;</div><div class=\"line\">    // Note that in this function, we introduce a hack in the case of self-join to automatically</div><div class=\"line\">    // resolve ambiguous join conditions into ones that might make sense [SPARK-6231].</div><div class=\"line\">    // Consider this case: df.join(df, df(&quot;key&quot;) === df(&quot;key&quot;))</div><div class=\"line\">    // Since df(&quot;key&quot;) === df(&quot;key&quot;) is a trivially true condition, this actually becomes a</div><div class=\"line\">    // cartesian join. However, most likely users expect to perform a self join using &quot;key&quot;.</div><div class=\"line\">    // With that assumption, this hack turns the trivially true condition into equality on join</div><div class=\"line\">    // keys that are resolved to both sides.</div><div class=\"line\"></div><div class=\"line\">    // Trigger analysis so in the case of self-join, the analyzer will clone the plan.</div><div class=\"line\">    // After the cloning, left and right side will have distinct expression ids.</div><div class=\"line\">    val plan = withPlan(</div><div class=\"line\">      Join(logicalPlan, right.logicalPlan, JoinType(joinType), Some(joinExprs.expr)))</div><div class=\"line\">      .queryExecution.analyzed.asInstanceOf[Join]</div><div class=\"line\"></div><div class=\"line\">    // If auto self join alias is disabled, return the plan.</div><div class=\"line\">    if (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) &#123;</div><div class=\"line\">      return withPlan(plan)</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    // If left/right have no output set intersection, return the plan.</div><div class=\"line\">    val lanalyzed = withPlan(this.logicalPlan).queryExecution.analyzed</div><div class=\"line\">    val ranalyzed = withPlan(right.logicalPlan).queryExecution.analyzed</div><div class=\"line\">    if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) &#123;</div><div class=\"line\">      return withPlan(plan)</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    // Otherwise, find the trivially true predicates and automatically resolves them to both sides.</div><div class=\"line\">    // By the time we get here, since we have already run analysis, all attributes should&apos;ve been</div><div class=\"line\">    // resolved and become AttributeReference.</div><div class=\"line\">    val cond = plan.condition.map &#123; _.transform &#123;</div><div class=\"line\">      case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference)</div><div class=\"line\">          if a.sameRef(b) =&gt;</div><div class=\"line\">        catalyst.expressions.EqualTo(</div><div class=\"line\">          withPlan(plan.left).resolve(a.name),</div><div class=\"line\">          withPlan(plan.right).resolve(b.name))</div><div class=\"line\">    &#125;&#125;</div><div class=\"line\"></div><div class=\"line\">    withPlan &#123;</div><div class=\"line\">      plan.copy(condition = cond)</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Explicit cartesian join with another `DataFrame`.</div><div class=\"line\">   *</div><div class=\"line\">   * @param right Right side of the join operation.</div><div class=\"line\">   *</div><div class=\"line\">   * @note Cartesian joins are very expensive without an extra filter that can be pushed down.</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.1.0</div><div class=\"line\">   */</div><div class=\"line\">全表关联</div><div class=\"line\">  def crossJoin(right: Dataset[_]): DataFrame = withPlan &#123;</div><div class=\"line\">    Join(logicalPlan, right.logicalPlan, joinType = Cross, None)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * :: Experimental ::</div><div class=\"line\">   * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to</div><div class=\"line\">   * true.</div><div class=\"line\">   *</div><div class=\"line\">   * This is similar to the relation `join` function with one important difference in the</div><div class=\"line\">   * result schema. Since `joinWith` preserves objects present on either side of the join, the</div><div class=\"line\">   * result schema is similarly nested into a tuple under the column names `_1` and `_2`.</div><div class=\"line\">   *</div><div class=\"line\">   * This type of join can be useful both for preserving type-safety with the original object</div><div class=\"line\">   * types as well as working with relational data where either side of the join has column</div><div class=\"line\">   * names in common.</div><div class=\"line\">   *</div><div class=\"line\">   * @param other Right side of the join.</div><div class=\"line\">   * @param condition Join expression.</div><div class=\"line\">   * @param joinType Type of join to perform. Default `inner`. Must be one of:</div><div class=\"line\">   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,</div><div class=\"line\">   *                 `right`, `right_outer`, `left_semi`, `left_anti`.</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">一种特殊的关联，得到的结果集的结构不同于普通的关联结果</div><div class=\"line\">  @Experimental</div><div class=\"line\">  @InterfaceStability.Evolving</div><div class=\"line\">  def joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = &#123;</div><div class=\"line\">    // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved,</div><div class=\"line\">    // etc.</div><div class=\"line\">    val joined = sparkSession.sessionState.executePlan(</div><div class=\"line\">      Join(</div><div class=\"line\">        this.logicalPlan,</div><div class=\"line\">        other.logicalPlan,</div><div class=\"line\">        JoinType(joinType),</div><div class=\"line\">        Some(condition.expr))).analyzed.asInstanceOf[Join]</div><div class=\"line\"></div><div class=\"line\">    // For both join side, combine all outputs into a single column and alias it with &quot;_1&quot; or &quot;_2&quot;,</div><div class=\"line\">    // to match the schema for the encoder of the join result.</div><div class=\"line\">    // Note that we do this before joining them, to enable the join operator to return null for one</div><div class=\"line\">    // side, in cases like outer-join.</div><div class=\"line\">    val left = &#123;</div><div class=\"line\">      val combined = if (this.exprEnc.flat) &#123;</div><div class=\"line\">        assert(joined.left.output.length == 1)</div><div class=\"line\">        Alias(joined.left.output.head, &quot;_1&quot;)()</div><div class=\"line\">      &#125; else &#123;</div><div class=\"line\">        Alias(CreateStruct(joined.left.output), &quot;_1&quot;)()</div><div class=\"line\">      &#125;</div><div class=\"line\">      Project(combined :: Nil, joined.left)</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    val right = &#123;</div><div class=\"line\">      val combined = if (other.exprEnc.flat) &#123;</div><div class=\"line\">        assert(joined.right.output.length == 1)</div><div class=\"line\">        Alias(joined.right.output.head, &quot;_2&quot;)()</div><div class=\"line\">      &#125; else &#123;</div><div class=\"line\">        Alias(CreateStruct(joined.right.output), &quot;_2&quot;)()</div><div class=\"line\">      &#125;</div><div class=\"line\">      Project(combined :: Nil, joined.right)</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    // Rewrites the join condition to make the attribute point to correct column/field, after we</div><div class=\"line\">    // combine the outputs of each join side.</div><div class=\"line\">    val conditionExpr = joined.condition.get transformUp &#123;</div><div class=\"line\">      case a: Attribute if joined.left.outputSet.contains(a) =&gt;</div><div class=\"line\">        if (this.exprEnc.flat) &#123;</div><div class=\"line\">          left.output.head</div><div class=\"line\">        &#125; else &#123;</div><div class=\"line\">          val index = joined.left.output.indexWhere(_.exprId == a.exprId)</div><div class=\"line\">          GetStructField(left.output.head, index)</div><div class=\"line\">        &#125;</div><div class=\"line\">      case a: Attribute if joined.right.outputSet.contains(a) =&gt;</div><div class=\"line\">        if (other.exprEnc.flat) &#123;</div><div class=\"line\">          right.output.head</div><div class=\"line\">        &#125; else &#123;</div><div class=\"line\">          val index = joined.right.output.indexWhere(_.exprId == a.exprId)</div><div class=\"line\">          GetStructField(right.output.head, index)</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    implicit val tuple2Encoder: Encoder[(T, U)] =</div><div class=\"line\">      ExpressionEncoder.tuple(this.exprEnc, other.exprEnc)</div><div class=\"line\"></div><div class=\"line\">    withTypedPlan(Join(left, right, joined.joinType, Some(conditionExpr)))</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * :: Experimental ::</div><div class=\"line\">   * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair</div><div class=\"line\">   * where `condition` evaluates to true.</div><div class=\"line\">   *</div><div class=\"line\">   * @param other Right side of the join.</div><div class=\"line\">   * @param condition Join expression.</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">  @Experimental</div><div class=\"line\">  @InterfaceStability.Evolving</div><div class=\"line\">  def joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = &#123;</div><div class=\"line\">    joinWith(other, condition, &quot;inner&quot;)</div><div class=\"line\">  &#125;</div></pre></td></tr></table></figure>\n<h2 id=\"排序分组\"><a href=\"#排序分组\" class=\"headerlink\" title=\"排序分组\"></a>排序分组</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div></pre></td><td class=\"code\"><pre><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new Dataset with each partition sorted by the given expressions.</div><div class=\"line\">   *</div><div class=\"line\">   * This is the same operation as &quot;SORT BY&quot; in SQL (Hive QL).</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">根据指定字段对每个分区进行排序</div><div class=\"line\">  @scala.annotation.varargs</div><div class=\"line\">  def sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T] = &#123;</div><div class=\"line\">    sortWithinPartitions((sortCol +: sortCols).map(Column(_)) : _*)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new Dataset with each partition sorted by the given expressions.</div><div class=\"line\">   *</div><div class=\"line\">   * This is the same operation as &quot;SORT BY&quot; in SQL (Hive QL).</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\"></div><div class=\"line\">根据指定列对每个分区进行排序</div><div class=\"line\">  @scala.annotation.varargs</div><div class=\"line\">  def sortWithinPartitions(sortExprs: Column*): Dataset[T] = &#123;</div><div class=\"line\">    sortInternal(global = false, sortExprs)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new Dataset sorted by the specified column, all in ascending order.</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   // The following 3 are equivalent</div><div class=\"line\">   *   ds.sort(&quot;sortcol&quot;)</div><div class=\"line\">   *   ds.sort($&quot;sortcol&quot;)</div><div class=\"line\">   *   ds.sort($&quot;sortcol&quot;.asc)</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  @scala.annotation.varargs</div><div class=\"line\">  def sort(sortCol: String, sortCols: String*): Dataset[T] = &#123;</div><div class=\"line\">    sort((sortCol +: sortCols).map(apply) : _*)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new Dataset sorted by the given expressions. For example:</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   ds.sort($&quot;col1&quot;, $&quot;col2&quot;.desc)</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  @scala.annotation.varargs</div><div class=\"line\">  def sort(sortExprs: Column*): Dataset[T] = &#123;</div><div class=\"line\">    sortInternal(global = true, sortExprs)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new Dataset sorted by the given expressions.</div><div class=\"line\">   * This is an alias of the `sort` function.</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  @scala.annotation.varargs</div><div class=\"line\">  def orderBy(sortCol: String, sortCols: String*): Dataset[T] = sort(sortCol, sortCols : _*)</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new Dataset sorted by the given expressions.</div><div class=\"line\">   * This is an alias of the `sort` function.</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  @scala.annotation.varargs</div><div class=\"line\">  def orderBy(sortExprs: Column*): Dataset[T] = sort(sortExprs : _*)</div></pre></td></tr></table></figure>\n<p>提取指定的列<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Selects column based on the column name and return it as a [[Column]].</div><div class=\"line\"> *</div><div class=\"line\"> * @note The column name can also reference to a nested column like `a.b`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group untypedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def apply(colName: String): Column = col(colName)</div><div class=\"line\">/**</div><div class=\"line\"> * Selects column based on the column name and return it as a [[Column]].</div><div class=\"line\"> *</div><div class=\"line\"> * @note The column name can also reference to a nested column like `a.b`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group untypedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def col(colName: String): Column = colName match &#123;</div><div class=\"line\">  case &quot;*&quot; =&gt;</div><div class=\"line\">    Column(ResolvedStar(queryExecution.analyzed.output))</div><div class=\"line\">  case _ =&gt;</div><div class=\"line\">    val expr = resolve(colName)</div><div class=\"line\">    Column(expr)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h2 id=\"别名\"><a href=\"#别名\" class=\"headerlink\" title=\"别名\"></a>别名</h2><p>别名有给列取别名的也有给dataset取别名的这里是给当前dataset取别名<br> <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset with an alias set.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def as(alias: String): Dataset[T] = withTypedPlan &#123;</div><div class=\"line\">  SubqueryAlias(alias, logicalPlan, None)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * (Scala-specific) Returns a new Dataset with an alias set.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def as(alias: Symbol): Dataset[T] = as(alias.name)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset with an alias set. Same as `as`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def alias(alias: String): Dataset[T] = as(alias)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def alias(alias: Symbol): Dataset[T] = as(alias)</div></pre></td></tr></table></figure></p>\n<h2 id=\"查询\"><a href=\"#查询\" class=\"headerlink\" title=\"查询\"></a>查询</h2><p>查询有很多种接口使用的方式不太一样<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div><div class=\"line\">122</div><div class=\"line\">123</div><div class=\"line\">124</div><div class=\"line\">125</div><div class=\"line\">126</div><div class=\"line\">127</div><div class=\"line\">128</div><div class=\"line\">129</div><div class=\"line\">130</div><div class=\"line\">131</div><div class=\"line\">132</div><div class=\"line\">133</div><div class=\"line\">134</div><div class=\"line\">135</div><div class=\"line\">136</div><div class=\"line\">137</div><div class=\"line\">138</div><div class=\"line\">139</div><div class=\"line\">140</div><div class=\"line\">141</div><div class=\"line\">142</div><div class=\"line\">143</div><div class=\"line\">144</div><div class=\"line\">145</div><div class=\"line\">146</div><div class=\"line\">147</div><div class=\"line\">148</div><div class=\"line\">149</div><div class=\"line\">150</div><div class=\"line\">151</div></pre></td><td class=\"code\"><pre><div class=\"line\">  /**</div><div class=\"line\">   * Selects a set of column based expressions.</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   ds.select($&quot;colA&quot;, $&quot;colB&quot; + 1)</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  @scala.annotation.varargs</div><div class=\"line\">  def select(cols: Column*): DataFrame = withPlan &#123;</div><div class=\"line\">    Project(cols.map(_.named), logicalPlan)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Selects a set of columns. This is a variant of `select` that can only select</div><div class=\"line\">   * existing columns using column names (i.e. cannot construct expressions).</div><div class=\"line\">   *</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   // The following two are equivalent:</div><div class=\"line\">   *   ds.select(&quot;colA&quot;, &quot;colB&quot;)</div><div class=\"line\">   *   ds.select($&quot;colA&quot;, $&quot;colB&quot;)</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  @scala.annotation.varargs</div><div class=\"line\">  def select(col: String, cols: String*): DataFrame = select((col +: cols).map(Column(_)) : _*)</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Selects a set of SQL expressions. This is a variant of `select` that accepts</div><div class=\"line\">   * SQL expressions.</div><div class=\"line\">   *</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   // The following are equivalent:</div><div class=\"line\">   *   ds.selectExpr(&quot;colA&quot;, &quot;colB as newName&quot;, &quot;abs(colC)&quot;)</div><div class=\"line\">   *   ds.select(expr(&quot;colA&quot;), expr(&quot;colB as newName&quot;), expr(&quot;abs(colC)&quot;))</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">按照表达式来查询</div><div class=\"line\">  @scala.annotation.varargs</div><div class=\"line\">  def selectExpr(exprs: String*): DataFrame = &#123;</div><div class=\"line\">    select(exprs.map &#123; expr =&gt;</div><div class=\"line\">      Column(sparkSession.sessionState.sqlParser.parseExpression(expr))</div><div class=\"line\">    &#125;: _*)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * :: Experimental ::</div><div class=\"line\">   * Returns a new Dataset by computing the given [[Column]] expression for each element.</div><div class=\"line\">   *</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   val ds = Seq(1, 2, 3).toDS()</div><div class=\"line\">   *   val newDS = ds.select(expr(&quot;value + 1&quot;).as[Int])</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">  @Experimental</div><div class=\"line\">  @InterfaceStability.Evolving</div><div class=\"line\">  def select[U1](c1: TypedColumn[T, U1]): Dataset[U1] = &#123;</div><div class=\"line\">    implicit val encoder = c1.encoder</div><div class=\"line\">    val project = Project(c1.withInputType(exprEnc, logicalPlan.output).named :: Nil,</div><div class=\"line\">      logicalPlan)</div><div class=\"line\"></div><div class=\"line\">    if (encoder.flat) &#123;</div><div class=\"line\">      new Dataset[U1](sparkSession, project, encoder)</div><div class=\"line\">    &#125; else &#123;</div><div class=\"line\">      // Flattens inner fields of U1</div><div class=\"line\">      new Dataset[Tuple1[U1]](sparkSession, project, ExpressionEncoder.tuple(encoder)).map(_._1)</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Internal helper function for building typed selects that return tuples. For simplicity and</div><div class=\"line\">   * code reuse, we do this without the help of the type system and then use helper functions</div><div class=\"line\">   * that cast appropriately for the user facing interface.</div><div class=\"line\">   */</div><div class=\"line\">  ???这个查询怎么用</div><div class=\"line\">  protected def selectUntyped(columns: TypedColumn[_, _]*): Dataset[_] = &#123;</div><div class=\"line\">    val encoders = columns.map(_.encoder)</div><div class=\"line\">    val namedColumns =</div><div class=\"line\">      columns.map(_.withInputType(exprEnc, logicalPlan.output).named)</div><div class=\"line\">    val execution = new QueryExecution(sparkSession, Project(namedColumns, logicalPlan))</div><div class=\"line\">    new Dataset(sparkSession, execution, ExpressionEncoder.tuple(encoders))</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * :: Experimental ::</div><div class=\"line\">   * Returns a new Dataset by computing the given [[Column]] expressions for each element.</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">  @Experimental</div><div class=\"line\">  @InterfaceStability.Evolving</div><div class=\"line\">  def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)] =</div><div class=\"line\">    selectUntyped(c1, c2).asInstanceOf[Dataset[(U1, U2)]]</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * :: Experimental ::</div><div class=\"line\">   * Returns a new Dataset by computing the given [[Column]] expressions for each element.</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">  @Experimental</div><div class=\"line\">  @InterfaceStability.Evolving</div><div class=\"line\">  def select[U1, U2, U3](</div><div class=\"line\">      c1: TypedColumn[T, U1],</div><div class=\"line\">      c2: TypedColumn[T, U2],</div><div class=\"line\">      c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)] =</div><div class=\"line\">    selectUntyped(c1, c2, c3).asInstanceOf[Dataset[(U1, U2, U3)]]</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * :: Experimental ::</div><div class=\"line\">   * Returns a new Dataset by computing the given [[Column]] expressions for each element.</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">  @Experimental</div><div class=\"line\">  @InterfaceStability.Evolving</div><div class=\"line\">  def select[U1, U2, U3, U4](</div><div class=\"line\">      c1: TypedColumn[T, U1],</div><div class=\"line\">      c2: TypedColumn[T, U2],</div><div class=\"line\">      c3: TypedColumn[T, U3],</div><div class=\"line\">      c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)] =</div><div class=\"line\">    selectUntyped(c1, c2, c3, c4).asInstanceOf[Dataset[(U1, U2, U3, U4)]]</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * :: Experimental ::</div><div class=\"line\">   * Returns a new Dataset by computing the given [[Column]] expressions for each element.</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">  @Experimental</div><div class=\"line\">  @InterfaceStability.Evolving</div><div class=\"line\">  def select[U1, U2, U3, U4, U5](</div><div class=\"line\">      c1: TypedColumn[T, U1],</div><div class=\"line\">      c2: TypedColumn[T, U2],</div><div class=\"line\">      c3: TypedColumn[T, U3],</div><div class=\"line\">      c4: TypedColumn[T, U4],</div><div class=\"line\">      c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)] =</div><div class=\"line\">    selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[Dataset[(U1, U2, U3, U4, U5)]]</div></pre></td></tr></table></figure></p>\n<h2 id=\"过滤\"><a href=\"#过滤\" class=\"headerlink\" title=\"过滤\"></a>过滤</h2><p>过滤，这里的过滤和sql里面的where条件是相同的，查询满足一定条件的记录。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Filters rows using the given condition.</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   // The following are equivalent:</div><div class=\"line\"> *   peopleDs.filter($&quot;age&quot; &gt; 15)</div><div class=\"line\"> *   peopleDs.where($&quot;age&quot; &gt; 15)</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def filter(condition: Column): Dataset[T] = withTypedPlan &#123;</div><div class=\"line\">  Filter(condition.expr, logicalPlan)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Filters rows using the given SQL expression.</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   peopleDs.filter(&quot;age &gt; 15&quot;)</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def filter(conditionExpr: String): Dataset[T] = &#123;</div><div class=\"line\">  filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Filters rows using the given condition. This is an alias for `filter`.</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   // The following are equivalent:</div><div class=\"line\"> *   peopleDs.filter($&quot;age&quot; &gt; 15)</div><div class=\"line\"> *   peopleDs.where($&quot;age&quot; &gt; 15)</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def where(condition: Column): Dataset[T] = filter(condition)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Filters rows using the given SQL expression.</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   peopleDs.where(&quot;age &gt; 15&quot;)</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def where(conditionExpr: String): Dataset[T] = &#123;</div><div class=\"line\">  filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h2 id=\"分组查询\"><a href=\"#分组查询\" class=\"headerlink\" title=\"分组查询\"></a>分组查询</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Groups the Dataset using the specified columns, so we can run aggregation on them. See</div><div class=\"line\"> * [[RelationalGroupedDataset]] for all the available aggregate functions.</div><div class=\"line\"> *</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   // Compute the average for all numeric columns grouped by department.</div><div class=\"line\"> *   ds.groupBy($&quot;department&quot;).avg()</div><div class=\"line\"> *</div><div class=\"line\"> *   // Compute the max age and average salary, grouped by department and gender.</div><div class=\"line\"> *   ds.groupBy($&quot;department&quot;, $&quot;gender&quot;).agg(Map(</div><div class=\"line\"> *     &quot;salary&quot; -&gt; &quot;avg&quot;,</div><div class=\"line\"> *     &quot;age&quot; -&gt; &quot;max&quot;</div><div class=\"line\"> *   ))</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group untypedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@scala.annotation.varargs</div><div class=\"line\">def groupBy(cols: Column*): RelationalGroupedDataset = &#123;</div><div class=\"line\">  RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.GroupByType)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Create a multi-dimensional rollup for the current Dataset using the specified columns,</div><div class=\"line\"> * so we can run aggregation on them.</div><div class=\"line\"> * See [[RelationalGroupedDataset]] for all the available aggregate functions.</div><div class=\"line\"> *</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   // Compute the average for all numeric columns rolluped by department and group.</div><div class=\"line\"> *   ds.rollup($&quot;department&quot;, $&quot;group&quot;).avg()</div><div class=\"line\"> *</div><div class=\"line\"> *   // Compute the max age and average salary, rolluped by department and gender.</div><div class=\"line\"> *   ds.rollup($&quot;department&quot;, $&quot;gender&quot;).agg(Map(</div><div class=\"line\"> *     &quot;salary&quot; -&gt; &quot;avg&quot;,</div><div class=\"line\"> *     &quot;age&quot; -&gt; &quot;max&quot;</div><div class=\"line\"> *   ))</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group untypedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">数据归纳</div><div class=\"line\">@scala.annotation.varargs</div><div class=\"line\">def rollup(cols: Column*): RelationalGroupedDataset = &#123;</div><div class=\"line\">  RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.RollupType)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Create a multi-dimensional cube for the current Dataset using the specified columns,</div><div class=\"line\"> * so we can run aggregation on them.</div><div class=\"line\"> * See [[RelationalGroupedDataset]] for all the available aggregate functions.</div><div class=\"line\"> *</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   // Compute the average for all numeric columns cubed by department and group.</div><div class=\"line\"> *   ds.cube($&quot;department&quot;, $&quot;group&quot;).avg()</div><div class=\"line\"> *</div><div class=\"line\"> *   // Compute the max age and average salary, cubed by department and gender.</div><div class=\"line\"> *   ds.cube($&quot;department&quot;, $&quot;gender&quot;).agg(Map(</div><div class=\"line\"> *     &quot;salary&quot; -&gt; &quot;avg&quot;,</div><div class=\"line\"> *     &quot;age&quot; -&gt; &quot;max&quot;</div><div class=\"line\"> *   ))</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group untypedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@scala.annotation.varargs</div><div class=\"line\">def cube(cols: Column*): RelationalGroupedDataset = &#123;</div><div class=\"line\">  RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.CubeType)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Groups the Dataset using the specified columns, so that we can run aggregation on them.</div><div class=\"line\"> * See [[RelationalGroupedDataset]] for all the available aggregate functions.</div><div class=\"line\"> *</div><div class=\"line\"> * This is a variant of groupBy that can only group by existing columns using column names</div><div class=\"line\"> * (i.e. cannot construct expressions).</div><div class=\"line\"> *</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   // Compute the average for all numeric columns grouped by department.</div><div class=\"line\"> *   ds.groupBy(&quot;department&quot;).avg()</div><div class=\"line\"> *</div><div class=\"line\"> *   // Compute the max age and average salary, grouped by department and gender.</div><div class=\"line\"> *   ds.groupBy($&quot;department&quot;, $&quot;gender&quot;).agg(Map(</div><div class=\"line\"> *     &quot;salary&quot; -&gt; &quot;avg&quot;,</div><div class=\"line\"> *     &quot;age&quot; -&gt; &quot;max&quot;</div><div class=\"line\"> *   ))</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> * @group untypedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@scala.annotation.varargs</div><div class=\"line\">def groupBy(col1: String, cols: String*): RelationalGroupedDataset = &#123;</div><div class=\"line\">  val colNames: Seq[String] = col1 +: cols</div><div class=\"line\">  RelationalGroupedDataset(</div><div class=\"line\">    toDF(), colNames.map(colName =&gt; resolve(colName)), RelationalGroupedDataset.GroupByType)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"reduce操作\"><a href=\"#reduce操作\" class=\"headerlink\" title=\"reduce操作\"></a>reduce操作</h2>  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * (Scala-specific)</div><div class=\"line\"> * Reduces the elements of this Dataset using the specified binary function. The given `func`</div><div class=\"line\"> * must be commutative and associative or the result may be non-deterministic.</div><div class=\"line\"> *</div><div class=\"line\"> * @group action</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">合并操作</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def reduce(func: (T, T) =&gt; T): T = rdd.reduce(func)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * (Java-specific)</div><div class=\"line\"> * Reduces the elements of this Dataset using the specified binary function. The given `func`</div><div class=\"line\"> * must be commutative and associative or the result may be non-deterministic.</div><div class=\"line\"> *</div><div class=\"line\"> * @group action</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def reduce(func: ReduceFunction[T]): T = reduce(func.call(_, _))</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * (Scala-specific)</div><div class=\"line\"> * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">分组                                  </div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def groupByKey[K: Encoder](func: T =&gt; K): KeyValueGroupedDataset[K, T] = &#123;</div><div class=\"line\">  val inputPlan = logicalPlan</div><div class=\"line\">  val withGroupingKey = AppendColumns(func, inputPlan)</div><div class=\"line\">  val executed = sparkSession.sessionState.executePlan(withGroupingKey)</div><div class=\"line\"></div><div class=\"line\">  new KeyValueGroupedDataset(</div><div class=\"line\">    encoderFor[K],</div><div class=\"line\">    encoderFor[T],</div><div class=\"line\">    executed,</div><div class=\"line\">    inputPlan.output,</div><div class=\"line\">    withGroupingKey.newColumns)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * (Java-specific)</div><div class=\"line\"> * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T] =</div><div class=\"line\">  groupByKey(func.call(_))(encoder)</div></pre></td></tr></table></figure>\n<h2 id=\"数据钻取与聚合操作\"><a href=\"#数据钻取与聚合操作\" class=\"headerlink\" title=\"数据钻取与聚合操作\"></a>数据钻取与聚合操作</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div><div class=\"line\">122</div><div class=\"line\">123</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Create a multi-dimensional rollup for the current Dataset using the specified columns,</div><div class=\"line\"> * so we can run aggregation on them.</div><div class=\"line\"> * See [[RelationalGroupedDataset]] for all the available aggregate functions.</div><div class=\"line\"> *</div><div class=\"line\"> * This is a variant of rollup that can only group by existing columns using column names</div><div class=\"line\"> * (i.e. cannot construct expressions).</div><div class=\"line\"> *</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   // Compute the average for all numeric columns rolluped by department and group.</div><div class=\"line\"> *   ds.rollup(&quot;department&quot;, &quot;group&quot;).avg()</div><div class=\"line\"> *</div><div class=\"line\"> *   // Compute the max age and average salary, rolluped by department and gender.</div><div class=\"line\"> *   ds.rollup($&quot;department&quot;, $&quot;gender&quot;).agg(Map(</div><div class=\"line\"> *     &quot;salary&quot; -&gt; &quot;avg&quot;,</div><div class=\"line\"> *     &quot;age&quot; -&gt; &quot;max&quot;</div><div class=\"line\"> *   ))</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group untypedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@scala.annotation.varargs</div><div class=\"line\">def rollup(col1: String, cols: String*): RelationalGroupedDataset = &#123;</div><div class=\"line\">  val colNames: Seq[String] = col1 +: cols</div><div class=\"line\">  RelationalGroupedDataset(</div><div class=\"line\">    toDF(), colNames.map(colName =&gt; resolve(colName)), RelationalGroupedDataset.RollupType)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Create a multi-dimensional cube for the current Dataset using the specified columns,</div><div class=\"line\"> * so we can run aggregation on them.</div><div class=\"line\"> * See [[RelationalGroupedDataset]] for all the available aggregate functions.</div><div class=\"line\"> *</div><div class=\"line\"> * This is a variant of cube that can only group by existing columns using column names</div><div class=\"line\"> * (i.e. cannot construct expressions).</div><div class=\"line\"> *</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   // Compute the average for all numeric columns cubed by department and group.</div><div class=\"line\"> *   ds.cube(&quot;department&quot;, &quot;group&quot;).avg()</div><div class=\"line\"> *</div><div class=\"line\"> *   // Compute the max age and average salary, cubed by department and gender.</div><div class=\"line\"> *   ds.cube($&quot;department&quot;, $&quot;gender&quot;).agg(Map(</div><div class=\"line\"> *     &quot;salary&quot; -&gt; &quot;avg&quot;,</div><div class=\"line\"> *     &quot;age&quot; -&gt; &quot;max&quot;</div><div class=\"line\"> *   ))</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> * @group untypedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@scala.annotation.varargs</div><div class=\"line\">def cube(col1: String, cols: String*): RelationalGroupedDataset = &#123;</div><div class=\"line\">  val colNames: Seq[String] = col1 +: cols</div><div class=\"line\">  RelationalGroupedDataset(</div><div class=\"line\">    toDF(), colNames.map(colName =&gt; resolve(colName)), RelationalGroupedDataset.CubeType)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * (Scala-specific) Aggregates on the entire Dataset without groups.</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)</div><div class=\"line\"> *   ds.agg(&quot;age&quot; -&gt; &quot;max&quot;, &quot;salary&quot; -&gt; &quot;avg&quot;)</div><div class=\"line\"> *   ds.groupBy().agg(&quot;age&quot; -&gt; &quot;max&quot;, &quot;salary&quot; -&gt; &quot;avg&quot;)</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group untypedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = &#123;</div><div class=\"line\">  groupBy().agg(aggExpr, aggExprs : _*)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * (Scala-specific) Aggregates on the entire Dataset without groups.</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)</div><div class=\"line\"> *   ds.agg(Map(&quot;age&quot; -&gt; &quot;max&quot;, &quot;salary&quot; -&gt; &quot;avg&quot;))</div><div class=\"line\"> *   ds.groupBy().agg(Map(&quot;age&quot; -&gt; &quot;max&quot;, &quot;salary&quot; -&gt; &quot;avg&quot;))</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group untypedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def agg(exprs: Map[String, String]): DataFrame = groupBy().agg(exprs)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * (Java-specific) Aggregates on the entire Dataset without groups.</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)</div><div class=\"line\"> *   ds.agg(Map(&quot;age&quot; -&gt; &quot;max&quot;, &quot;salary&quot; -&gt; &quot;avg&quot;))</div><div class=\"line\"> *   ds.groupBy().agg(Map(&quot;age&quot; -&gt; &quot;max&quot;, &quot;salary&quot; -&gt; &quot;avg&quot;))</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group untypedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def agg(exprs: java.util.Map[String, String]): DataFrame = groupBy().agg(exprs)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Aggregates on the entire Dataset without groups.</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)</div><div class=\"line\"> *   ds.agg(max($&quot;age&quot;), avg($&quot;salary&quot;))</div><div class=\"line\"> *   ds.groupBy().agg(max($&quot;age&quot;), avg($&quot;salary&quot;))</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group untypedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@scala.annotation.varargs</div><div class=\"line\">def agg(expr: Column, exprs: Column*): DataFrame = groupBy().agg(expr, exprs : _*)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset by taking the first `n` rows. The difference between this function</div><div class=\"line\"> * and `head` is that `head` is an action and returns an array (by triggering query execution)</div><div class=\"line\"> * while `limit` returns a new Dataset.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def limit(n: Int): Dataset[T] = withTypedPlan &#123;</div><div class=\"line\">  Limit(Literal(n), logicalPlan)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"集合的交并补接口\"><a href=\"#集合的交并补接口\" class=\"headerlink\" title=\"集合的交并补接口\"></a>集合的交并补接口</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset containing union of rows in this Dataset and another Dataset.</div><div class=\"line\"> * This is equivalent to `UNION ALL` in SQL.</div><div class=\"line\"> *</div><div class=\"line\"> * To do a SQL-style set union (that does deduplication of elements), use this function followed</div><div class=\"line\"> * by a [[distinct]].</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@deprecated(&quot;use union()&quot;, &quot;2.0.0&quot;)</div><div class=\"line\">def unionAll(other: Dataset[T]): Dataset[T] = union(other)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset containing union of rows in this Dataset and another Dataset.</div><div class=\"line\"> * This is equivalent to `UNION ALL` in SQL.</div><div class=\"line\"> *</div><div class=\"line\"> * To do a SQL-style set union (that does deduplication of elements), use this function followed</div><div class=\"line\"> * by a [[distinct]].</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def union(other: Dataset[T]): Dataset[T] = withSetOperator &#123;</div><div class=\"line\">  // This breaks caching, but it&apos;s usually ok because it addresses a very specific use case:</div><div class=\"line\">  // using union to union many files or partitions.</div><div class=\"line\">  CombineUnions(Union(logicalPlan, other.logicalPlan))</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset containing rows only in both this Dataset and another Dataset.</div><div class=\"line\"> * This is equivalent to `INTERSECT` in SQL.</div><div class=\"line\"> *</div><div class=\"line\"> * @note Equality checking is performed directly on the encoded representation of the data</div><div class=\"line\"> * and thus is not affected by a custom `equals` function defined on `T`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def intersect(other: Dataset[T]): Dataset[T] = withSetOperator &#123;</div><div class=\"line\">  Intersect(logicalPlan, other.logicalPlan)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset containing rows in this Dataset but not in another Dataset.</div><div class=\"line\"> * This is equivalent to `EXCEPT` in SQL.</div><div class=\"line\"> *</div><div class=\"line\"> * @note Equality checking is performed directly on the encoded representation of the data</div><div class=\"line\"> * and thus is not affected by a custom `equals` function defined on `T`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">取补集</div><div class=\"line\">def except(other: Dataset[T]): Dataset[T] = withSetOperator &#123;</div><div class=\"line\">  Except(logicalPlan, other.logicalPlan)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"取样与切分\"><a href=\"#取样与切分\" class=\"headerlink\" title=\"取样与切分\"></a>取样与切分</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div><div class=\"line\">122</div><div class=\"line\">123</div><div class=\"line\">124</div><div class=\"line\">125</div><div class=\"line\">126</div><div class=\"line\">127</div><div class=\"line\">128</div><div class=\"line\">129</div><div class=\"line\">130</div><div class=\"line\">131</div><div class=\"line\">132</div><div class=\"line\">133</div><div class=\"line\">134</div><div class=\"line\">135</div><div class=\"line\">136</div><div class=\"line\">137</div><div class=\"line\">138</div><div class=\"line\">139</div><div class=\"line\">140</div><div class=\"line\">141</div><div class=\"line\">142</div><div class=\"line\">143</div><div class=\"line\">144</div><div class=\"line\">145</div><div class=\"line\">146</div><div class=\"line\">147</div><div class=\"line\">148</div><div class=\"line\">149</div><div class=\"line\">150</div><div class=\"line\">151</div><div class=\"line\">152</div><div class=\"line\">153</div><div class=\"line\">154</div><div class=\"line\">155</div><div class=\"line\">156</div><div class=\"line\">157</div><div class=\"line\">158</div><div class=\"line\">159</div><div class=\"line\">160</div><div class=\"line\">161</div><div class=\"line\">162</div><div class=\"line\">163</div><div class=\"line\">164</div><div class=\"line\">165</div><div class=\"line\">166</div><div class=\"line\">167</div><div class=\"line\">168</div><div class=\"line\">169</div><div class=\"line\">170</div><div class=\"line\">171</div><div class=\"line\">172</div><div class=\"line\">173</div><div class=\"line\">174</div><div class=\"line\">175</div><div class=\"line\">176</div><div class=\"line\">177</div><div class=\"line\">178</div><div class=\"line\">179</div><div class=\"line\">180</div><div class=\"line\">181</div><div class=\"line\">182</div><div class=\"line\">183</div><div class=\"line\">184</div><div class=\"line\">185</div><div class=\"line\">186</div><div class=\"line\">187</div><div class=\"line\">188</div><div class=\"line\">189</div><div class=\"line\">190</div><div class=\"line\">191</div><div class=\"line\">192</div><div class=\"line\">193</div><div class=\"line\">194</div><div class=\"line\">195</div><div class=\"line\">196</div><div class=\"line\">197</div><div class=\"line\">198</div><div class=\"line\">199</div><div class=\"line\">200</div><div class=\"line\">201</div><div class=\"line\">202</div><div class=\"line\">203</div><div class=\"line\">204</div><div class=\"line\">205</div><div class=\"line\">206</div><div class=\"line\">207</div><div class=\"line\">208</div><div class=\"line\">209</div><div class=\"line\">210</div><div class=\"line\">211</div><div class=\"line\">212</div><div class=\"line\">213</div><div class=\"line\">214</div><div class=\"line\">215</div><div class=\"line\">216</div><div class=\"line\">217</div><div class=\"line\">218</div><div class=\"line\">219</div><div class=\"line\">220</div><div class=\"line\">221</div><div class=\"line\">222</div><div class=\"line\">223</div><div class=\"line\">224</div><div class=\"line\">225</div><div class=\"line\">226</div><div class=\"line\">227</div><div class=\"line\">228</div><div class=\"line\">229</div><div class=\"line\">230</div><div class=\"line\">231</div><div class=\"line\">232</div><div class=\"line\">233</div><div class=\"line\">234</div><div class=\"line\">235</div><div class=\"line\">236</div><div class=\"line\">237</div><div class=\"line\">238</div><div class=\"line\">239</div><div class=\"line\">240</div><div class=\"line\">241</div><div class=\"line\">242</div><div class=\"line\">243</div><div class=\"line\">244</div><div class=\"line\">245</div><div class=\"line\">246</div><div class=\"line\">247</div><div class=\"line\">248</div><div class=\"line\">249</div><div class=\"line\">250</div><div class=\"line\">251</div><div class=\"line\">252</div><div class=\"line\">253</div><div class=\"line\">254</div><div class=\"line\">255</div><div class=\"line\">256</div><div class=\"line\">257</div><div class=\"line\">258</div><div class=\"line\">259</div><div class=\"line\">260</div><div class=\"line\">261</div><div class=\"line\">262</div><div class=\"line\">263</div><div class=\"line\">264</div><div class=\"line\">265</div><div class=\"line\">266</div><div class=\"line\">267</div><div class=\"line\">268</div><div class=\"line\">269</div><div class=\"line\">270</div><div class=\"line\">271</div><div class=\"line\">272</div><div class=\"line\">273</div><div class=\"line\">274</div><div class=\"line\">275</div><div class=\"line\">276</div><div class=\"line\">277</div><div class=\"line\">278</div><div class=\"line\">279</div><div class=\"line\">280</div><div class=\"line\">281</div><div class=\"line\">282</div><div class=\"line\">283</div><div class=\"line\">284</div><div class=\"line\">285</div><div class=\"line\">286</div><div class=\"line\">287</div><div class=\"line\">288</div><div class=\"line\">289</div><div class=\"line\">290</div><div class=\"line\">291</div><div class=\"line\">292</div><div class=\"line\">293</div><div class=\"line\">294</div><div class=\"line\">295</div><div class=\"line\">296</div><div class=\"line\">297</div><div class=\"line\">298</div><div class=\"line\">299</div><div class=\"line\">300</div><div class=\"line\">301</div><div class=\"line\">302</div><div class=\"line\">303</div><div class=\"line\">304</div><div class=\"line\">305</div><div class=\"line\">306</div><div class=\"line\">307</div><div class=\"line\">308</div><div class=\"line\">309</div><div class=\"line\">310</div><div class=\"line\">311</div></pre></td><td class=\"code\"><pre><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed.</div><div class=\"line\">   *</div><div class=\"line\">   * @param withReplacement Sample with replacement or not.</div><div class=\"line\">   * @param fraction Fraction of rows to generate.</div><div class=\"line\">   * @param seed Seed for sampling.</div><div class=\"line\">   *</div><div class=\"line\">   * @note This is NOT guaranteed to provide exactly the fraction of the count</div><div class=\"line\">   * of the given [[Dataset]].</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">  def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = &#123;</div><div class=\"line\">    require(fraction &gt;= 0,</div><div class=\"line\">      s&quot;Fraction must be nonnegative, but got $&#123;fraction&#125;&quot;)</div><div class=\"line\"></div><div class=\"line\">    withTypedPlan &#123;</div><div class=\"line\">      Sample(0.0, fraction, withReplacement, seed, logicalPlan)()</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed.</div><div class=\"line\">   *</div><div class=\"line\">   * @param withReplacement Sample with replacement or not.</div><div class=\"line\">   * @param fraction Fraction of rows to generate.</div><div class=\"line\">   *</div><div class=\"line\">   * @note This is NOT guaranteed to provide exactly the fraction of the total count</div><div class=\"line\">   * of the given [[Dataset]].</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">  随即取样</div><div class=\"line\">  def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = &#123;</div><div class=\"line\">    sample(withReplacement, fraction, Utils.random.nextLong)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Randomly splits this Dataset with the provided weights.</div><div class=\"line\">   *</div><div class=\"line\">   * @param weights weights for splits, will be normalized if they don&apos;t sum to 1.</div><div class=\"line\">   * @param seed Seed for sampling.</div><div class=\"line\">   *</div><div class=\"line\">   * For Java API, use [[randomSplitAsList]].</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  随即切分???</div><div class=\"line\">  def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]] = &#123;</div><div class=\"line\">    require(weights.forall(_ &gt;= 0),</div><div class=\"line\">      s&quot;Weights must be nonnegative, but got $&#123;weights.mkString(&quot;[&quot;, &quot;,&quot;, &quot;]&quot;)&#125;&quot;)</div><div class=\"line\">    require(weights.sum &gt; 0,</div><div class=\"line\">      s&quot;Sum of weights must be positive, but got $&#123;weights.mkString(&quot;[&quot;, &quot;,&quot;, &quot;]&quot;)&#125;&quot;)</div><div class=\"line\"></div><div class=\"line\">    // It is possible that the underlying dataframe doesn&apos;t guarantee the ordering of rows in its</div><div class=\"line\">    // constituent partitions each time a split is materialized which could result in</div><div class=\"line\">    // overlapping splits. To prevent this, we explicitly sort each input partition to make the</div><div class=\"line\">    // ordering deterministic.</div><div class=\"line\">    // MapType cannot be sorted.</div><div class=\"line\">    val sorted = Sort(logicalPlan.output.filterNot(_.dataType.isInstanceOf[MapType])</div><div class=\"line\">      .map(SortOrder(_, Ascending)), global = false, logicalPlan)</div><div class=\"line\">    val sum = weights.sum</div><div class=\"line\">    val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _)</div><div class=\"line\">    normalizedCumWeights.sliding(2).map &#123; x =&gt;</div><div class=\"line\">      new Dataset[T](</div><div class=\"line\">        sparkSession, Sample(x(0), x(1), withReplacement = false, seed, sorted)(), encoder)</div><div class=\"line\">    &#125;.toArray</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a Java list that contains randomly split Dataset with the provided weights.</div><div class=\"line\">   *</div><div class=\"line\">   * @param weights weights for splits, will be normalized if they don&apos;t sum to 1.</div><div class=\"line\">   * @param seed Seed for sampling.</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  def randomSplitAsList(weights: Array[Double], seed: Long): java.util.List[Dataset[T]] = &#123;</div><div class=\"line\">    val values = randomSplit(weights, seed)</div><div class=\"line\">    java.util.Arrays.asList(values : _*)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Randomly splits this Dataset with the provided weights.</div><div class=\"line\">   *</div><div class=\"line\">   * @param weights weights for splits, will be normalized if they don&apos;t sum to 1.</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  def randomSplit(weights: Array[Double]): Array[Dataset[T]] = &#123;</div><div class=\"line\">    randomSplit(weights, Utils.random.nextLong)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Randomly splits this Dataset with the provided weights. Provided for the Python Api.</div><div class=\"line\">   *</div><div class=\"line\">   * @param weights weights for splits, will be normalized if they don&apos;t sum to 1.</div><div class=\"line\">   * @param seed Seed for sampling.</div><div class=\"line\">   */</div><div class=\"line\">  private[spark] def randomSplit(weights: List[Double], seed: Long): Array[Dataset[T]] = &#123;</div><div class=\"line\">    randomSplit(weights.toArray, seed)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more</div><div class=\"line\">   * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of</div><div class=\"line\">   * the input row are implicitly joined with each row that is output by the function.</div><div class=\"line\">   *</div><div class=\"line\">   * Given that this is deprecated, as an alternative, you can explode columns either using</div><div class=\"line\">   * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count</div><div class=\"line\">   * the number of books that contain a given word:</div><div class=\"line\">   *</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   case class Book(title: String, words: String)</div><div class=\"line\">   *   val ds: Dataset[Book]</div><div class=\"line\">   *</div><div class=\"line\">   *   val allWords = ds.select(&apos;title, explode(split(&apos;words, &quot; &quot;)).as(&quot;word&quot;))</div><div class=\"line\">   *</div><div class=\"line\">   *   val bookCountPerWord = allWords.groupBy(&quot;word&quot;).agg(countDistinct(&quot;title&quot;))</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * Using `flatMap()` this can similarly be exploded as:</div><div class=\"line\">   *</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   ds.flatMap(_.words.split(&quot; &quot;))</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  将字段再处理</div><div class=\"line\">  @deprecated(&quot;use flatMap() or select() with functions.explode() instead&quot;, &quot;2.0.0&quot;)</div><div class=\"line\">  def explode[A &lt;: Product : TypeTag](input: Column*)(f: Row =&gt; TraversableOnce[A]): DataFrame = &#123;</div><div class=\"line\">    val elementSchema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]</div><div class=\"line\"></div><div class=\"line\">    val convert = CatalystTypeConverters.createToCatalystConverter(elementSchema)</div><div class=\"line\"></div><div class=\"line\">    val rowFunction =</div><div class=\"line\">      f.andThen(_.map(convert(_).asInstanceOf[InternalRow]))</div><div class=\"line\">    val generator = UserDefinedGenerator(elementSchema, rowFunction, input.map(_.expr))</div><div class=\"line\"></div><div class=\"line\">    withPlan &#123;</div><div class=\"line\">      Generate(generator, join = true, outer = false,</div><div class=\"line\">        qualifier = None, generatorOutput = Nil, logicalPlan)</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero</div><div class=\"line\">   * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All</div><div class=\"line\">   * columns of the input row are implicitly joined with each value that is output by the function.</div><div class=\"line\">   *</div><div class=\"line\">   * Given that this is deprecated, as an alternative, you can explode columns either using</div><div class=\"line\">   * `functions.explode()`:</div><div class=\"line\">   *</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   ds.select(explode(split(&apos;words, &quot; &quot;)).as(&quot;word&quot;))</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * or `flatMap()`:</div><div class=\"line\">   *</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   ds.flatMap(_.words.split(&quot; &quot;))</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  @deprecated(&quot;use flatMap() or select() with functions.explode() instead&quot;, &quot;2.0.0&quot;)</div><div class=\"line\">  def explode[A, B : TypeTag](inputColumn: String, outputColumn: String)(f: A =&gt; TraversableOnce[B])</div><div class=\"line\">    : DataFrame = &#123;</div><div class=\"line\">    val dataType = ScalaReflection.schemaFor[B].dataType</div><div class=\"line\">    val attributes = AttributeReference(outputColumn, dataType)() :: Nil</div><div class=\"line\">    // TODO handle the metadata?</div><div class=\"line\">    val elementSchema = attributes.toStructType</div><div class=\"line\"></div><div class=\"line\">    def rowFunction(row: Row): TraversableOnce[InternalRow] = &#123;</div><div class=\"line\">      val convert = CatalystTypeConverters.createToCatalystConverter(dataType)</div><div class=\"line\">      f(row(0).asInstanceOf[A]).map(o =&gt; InternalRow(convert(o)))</div><div class=\"line\">    &#125;</div><div class=\"line\">    val generator = UserDefinedGenerator(elementSchema, rowFunction, apply(inputColumn).expr :: Nil)</div><div class=\"line\"></div><div class=\"line\">    withPlan &#123;</div><div class=\"line\">      Generate(generator, join = true, outer = false,</div><div class=\"line\">        qualifier = None, generatorOutput = Nil, logicalPlan)</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">## 列操作</div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new Dataset by adding a column or replacing the existing column that has</div><div class=\"line\">   * the same name.</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  列操作</div><div class=\"line\">  def withColumn(colName: String, col: Column): DataFrame = &#123;</div><div class=\"line\">    val resolver = sparkSession.sessionState.analyzer.resolver</div><div class=\"line\">    val output = queryExecution.analyzed.output</div><div class=\"line\">    val shouldReplace = output.exists(f =&gt; resolver(f.name, colName))</div><div class=\"line\">    if (shouldReplace) &#123;</div><div class=\"line\">      val columns = output.map &#123; field =&gt;</div><div class=\"line\">        if (resolver(field.name, colName)) &#123;</div><div class=\"line\">          col.as(colName)</div><div class=\"line\">        &#125; else &#123;</div><div class=\"line\">          Column(field)</div><div class=\"line\">        &#125;</div><div class=\"line\">      &#125;</div><div class=\"line\">      select(columns : _*)</div><div class=\"line\">    &#125; else &#123;</div><div class=\"line\">      select(Column(&quot;*&quot;), col.as(colName))</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new Dataset by adding a column with metadata.</div><div class=\"line\">   */</div><div class=\"line\">  private[spark] def withColumn(colName: String, col: Column, metadata: Metadata): DataFrame = &#123;</div><div class=\"line\">    withColumn(colName, col.as(colName, metadata))</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new Dataset with a column renamed.</div><div class=\"line\">   * This is a no-op if schema doesn&apos;t contain existingName.</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  def withColumnRenamed(existingName: String, newName: String): DataFrame = &#123;</div><div class=\"line\">    val resolver = sparkSession.sessionState.analyzer.resolver</div><div class=\"line\">    val output = queryExecution.analyzed.output</div><div class=\"line\">    val shouldRename = output.exists(f =&gt; resolver(f.name, existingName))</div><div class=\"line\">    if (shouldRename) &#123;</div><div class=\"line\">      val columns = output.map &#123; col =&gt;</div><div class=\"line\">        if (resolver(col.name, existingName)) &#123;</div><div class=\"line\">          Column(col).as(newName)</div><div class=\"line\">        &#125; else &#123;</div><div class=\"line\">          Column(col)</div><div class=\"line\">        &#125;</div><div class=\"line\">      &#125;</div><div class=\"line\">      select(columns : _*)</div><div class=\"line\">    &#125; else &#123;</div><div class=\"line\">      toDF()</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new Dataset with a column dropped. This is a no-op if schema doesn&apos;t contain</div><div class=\"line\">   * column name.</div><div class=\"line\">   *</div><div class=\"line\">   * This method can only be used to drop top level columns. the colName string is treated</div><div class=\"line\">   * literally without further interpretation.</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  删除指定列</div><div class=\"line\">  def drop(colName: String): DataFrame = &#123;</div><div class=\"line\">    drop(Seq(colName) : _*)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new Dataset with columns dropped.</div><div class=\"line\">   * This is a no-op if schema doesn&apos;t contain column name(s).</div><div class=\"line\">   *</div><div class=\"line\">   * This method can only be used to drop top level columns. the colName string is treated literally</div><div class=\"line\">   * without further interpretation.</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  @scala.annotation.varargs</div><div class=\"line\">  def drop(colNames: String*): DataFrame = &#123;</div><div class=\"line\">    val resolver = sparkSession.sessionState.analyzer.resolver</div><div class=\"line\">    val allColumns = queryExecution.analyzed.output</div><div class=\"line\">    val remainingCols = allColumns.filter &#123; attribute =&gt;</div><div class=\"line\">      colNames.forall(n =&gt; !resolver(attribute.name, n))</div><div class=\"line\">    &#125;.map(attribute =&gt; Column(attribute))</div><div class=\"line\">    if (remainingCols.size == allColumns.size) &#123;</div><div class=\"line\">      toDF()</div><div class=\"line\">    &#125; else &#123;</div><div class=\"line\">      this.select(remainingCols: _*)</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new Dataset with a column dropped.</div><div class=\"line\">   * This version of drop accepts a [[Column]] rather than a name.</div><div class=\"line\">   * This is a no-op if the Dataset doesn&apos;t have a column</div><div class=\"line\">   * with an equivalent expression.</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  def drop(col: Column): DataFrame = &#123;</div><div class=\"line\">    val expression = col match &#123;</div><div class=\"line\">      case Column(u: UnresolvedAttribute) =&gt;</div><div class=\"line\">        queryExecution.analyzed.resolveQuoted(</div><div class=\"line\">          u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u)</div><div class=\"line\">      case Column(expr: Expression) =&gt; expr</div><div class=\"line\">    &#125;</div><div class=\"line\">    val attrs = this.logicalPlan.output</div><div class=\"line\">    val colsAfterDrop = attrs.filter &#123; attr =&gt;</div><div class=\"line\">      attr != expression</div><div class=\"line\">    &#125;.map(attr =&gt; Column(attr))</div><div class=\"line\">    select(colsAfterDrop : _*)</div><div class=\"line\">  &#125;</div></pre></td></tr></table></figure>\n<h2 id=\"去重\"><a href=\"#去重\" class=\"headerlink\" title=\"去重\"></a>去重</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset that contains only the unique rows from this Dataset.</div><div class=\"line\"> * This is an alias for `distinct`.</div><div class=\"line\"> *</div><div class=\"line\"> * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it</div><div class=\"line\"> * will keep all data across triggers as intermediate state to drop duplicates rows. You can use</div><div class=\"line\"> * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit</div><div class=\"line\"> * the state. In addition, too late data older than watermark will be dropped to avoid any</div><div class=\"line\"> * possibility of duplicates.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">删除重复的行</div><div class=\"line\">def dropDuplicates(): Dataset[T] = dropDuplicates(this.columns)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only</div><div class=\"line\"> * the subset of columns.</div><div class=\"line\"> *</div><div class=\"line\"> * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it</div><div class=\"line\"> * will keep all data across triggers as intermediate state to drop duplicates rows. You can use</div><div class=\"line\"> * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit</div><div class=\"line\"> * the state. In addition, too late data older than watermark will be dropped to avoid any</div><div class=\"line\"> * possibility of duplicates.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def dropDuplicates(colNames: Seq[String]): Dataset[T] = withTypedPlan &#123;</div><div class=\"line\">  val resolver = sparkSession.sessionState.analyzer.resolver</div><div class=\"line\">  val allColumns = queryExecution.analyzed.output</div><div class=\"line\">  val groupCols = colNames.toSet.toSeq.flatMap &#123; (colName: String) =&gt;</div><div class=\"line\">    // It is possibly there are more than one columns with the same name,</div><div class=\"line\">    // so we call filter instead of find.</div><div class=\"line\">    val cols = allColumns.filter(col =&gt; resolver(col.name, colName))</div><div class=\"line\">    if (cols.isEmpty) &#123;</div><div class=\"line\">      throw new AnalysisException(</div><div class=\"line\">        s&quot;&quot;&quot;Cannot resolve column name &quot;$colName&quot; among ($&#123;schema.fieldNames.mkString(&quot;, &quot;)&#125;)&quot;&quot;&quot;)</div><div class=\"line\">    &#125;</div><div class=\"line\">    cols</div><div class=\"line\">  &#125;</div><div class=\"line\">  Deduplicate(groupCols, logicalPlan, isStreaming)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset with duplicate rows removed, considering only</div><div class=\"line\"> * the subset of columns.</div><div class=\"line\"> *</div><div class=\"line\"> * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it</div><div class=\"line\"> * will keep all data across triggers as intermediate state to drop duplicates rows. You can use</div><div class=\"line\"> * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit</div><div class=\"line\"> * the state. In addition, too late data older than watermark will be dropped to avoid any</div><div class=\"line\"> * possibility of duplicates.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def dropDuplicates(colNames: Array[String]): Dataset[T] = dropDuplicates(colNames.toSeq)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a new [[Dataset]] with duplicate rows removed, considering only</div><div class=\"line\"> * the subset of columns.</div><div class=\"line\"> *</div><div class=\"line\"> * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it</div><div class=\"line\"> * will keep all data across triggers as intermediate state to drop duplicates rows. You can use</div><div class=\"line\"> * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit</div><div class=\"line\"> * the state. In addition, too late data older than watermark will be dropped to avoid any</div><div class=\"line\"> * possibility of duplicates.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@scala.annotation.varargs</div><div class=\"line\">def dropDuplicates(col1: String, cols: String*): Dataset[T] = &#123;</div><div class=\"line\">  val colNames: Seq[String] = col1 +: cols</div><div class=\"line\">  dropDuplicates(colNames)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"统计指定的列\"><a href=\"#统计指定的列\" class=\"headerlink\" title=\"统计指定的列\"></a>统计指定的列</h2><pre><code><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div><div class=\"line\">122</div><div class=\"line\">123</div><div class=\"line\">124</div><div class=\"line\">125</div><div class=\"line\">126</div><div class=\"line\">127</div><div class=\"line\">128</div><div class=\"line\">129</div><div class=\"line\">130</div><div class=\"line\">131</div><div class=\"line\">132</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Computes statistics for numeric and string columns, including count, mean, stddev, min, and</div><div class=\"line\"> * max. If no columns are given, this function computes statistics for all numerical or string</div><div class=\"line\"> * columns.</div><div class=\"line\"> *</div><div class=\"line\"> * This function is meant for exploratory data analysis, as we make no guarantee about the</div><div class=\"line\"> * backward compatibility of the schema of the resulting Dataset. If you want to</div><div class=\"line\"> * programmatically compute summary statistics, use the `agg` function instead.</div><div class=\"line\"> *</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   ds.describe(&quot;age&quot;, &quot;height&quot;).show()</div><div class=\"line\"> *</div><div class=\"line\"> *   // output:</div><div class=\"line\"> *   // summary age   height</div><div class=\"line\"> *   // count   10.0  10.0</div><div class=\"line\"> *   // mean    53.3  178.05</div><div class=\"line\"> *   // stddev  11.6  15.7</div><div class=\"line\"> *   // min     18.0  163.0</div><div class=\"line\"> *   // max     92.0  192.0</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group action</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">获得指定列的描述性统计量</div><div class=\"line\">@scala.annotation.varargs</div><div class=\"line\">def describe(cols: String*): DataFrame = withPlan &#123;</div><div class=\"line\"></div><div class=\"line\">  // The list of summary statistics to compute, in the form of expressions.</div><div class=\"line\">  val statistics = List[(String, Expression =&gt; Expression)](</div><div class=\"line\">    &quot;count&quot; -&gt; ((child: Expression) =&gt; Count(child).toAggregateExpression()),</div><div class=\"line\">    &quot;mean&quot; -&gt; ((child: Expression) =&gt; Average(child).toAggregateExpression()),</div><div class=\"line\">    &quot;stddev&quot; -&gt; ((child: Expression) =&gt; StddevSamp(child).toAggregateExpression()),</div><div class=\"line\">    &quot;min&quot; -&gt; ((child: Expression) =&gt; Min(child).toAggregateExpression()),</div><div class=\"line\">    &quot;max&quot; -&gt; ((child: Expression) =&gt; Max(child).toAggregateExpression()))</div><div class=\"line\"></div><div class=\"line\">  val outputCols =</div><div class=\"line\">    (if (cols.isEmpty) aggregatableColumns.map(usePrettyExpression(_).sql) else cols).toList</div><div class=\"line\"></div><div class=\"line\">  val ret: Seq[Row] = if (outputCols.nonEmpty) &#123;</div><div class=\"line\">    val aggExprs = statistics.flatMap &#123; case (_, colToAgg) =&gt;</div><div class=\"line\">      outputCols.map(c =&gt; Column(Cast(colToAgg(Column(c).expr), StringType)).as(c))</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    val row = groupBy().agg(aggExprs.head, aggExprs.tail: _*).head().toSeq</div><div class=\"line\"></div><div class=\"line\">    // Pivot the data so each summary is one row</div><div class=\"line\">    row.grouped(outputCols.size).toSeq.zip(statistics).map &#123; case (aggregation, (statistic, _)) =&gt;</div><div class=\"line\">      Row(statistic :: aggregation.toList: _*)</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125; else &#123;</div><div class=\"line\">    // If there are no output columns, just output a single column that contains the stats.</div><div class=\"line\">    statistics.map &#123; case (name, _) =&gt; Row(name) &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  // All columns are string type</div><div class=\"line\">  val schema = StructType(</div><div class=\"line\">    StructField(&quot;summary&quot;, StringType) :: outputCols.map(StructField(_, StringType))).toAttributes</div><div class=\"line\">  // `toArray` forces materialization to make the seq serializable</div><div class=\"line\">  LocalRelation.fromExternalRows(schema, ret.toArray.toSeq)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns the first `n` rows.</div><div class=\"line\"> *</div><div class=\"line\"> * @note this method should only be used if the resulting array is expected to be small, as</div><div class=\"line\"> * all the data is loaded into the driver&apos;s memory.</div><div class=\"line\"> *</div><div class=\"line\"> * @group action</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">取得前n行数据</div><div class=\"line\">def head(n: Int): Array[T] = withAction(&quot;head&quot;, limit(n).queryExecution)(collectFromPlan)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns the first row.</div><div class=\"line\"> * @group action</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def head(): T = head(1).head</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns the first row. Alias for head().</div><div class=\"line\"> * @group action</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def first(): T = head()</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Concise syntax for chaining custom transformations.</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   def featurize(ds: Dataset[T]): Dataset[U] = ...</div><div class=\"line\"> *</div><div class=\"line\"> *   ds</div><div class=\"line\"> *     .transform(featurize)</div><div class=\"line\"> *     .transform(...)</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">转换？？</div><div class=\"line\">def transform[U](t: Dataset[T] =&gt; Dataset[U]): Dataset[U] = t(this)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * (Scala-specific)</div><div class=\"line\"> * Returns a new Dataset that only contains elements where `func` returns `true`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">过滤</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def filter(func: T =&gt; Boolean): Dataset[T] = &#123;</div><div class=\"line\">  withTypedPlan(TypedFilter(func, logicalPlan))</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * (Java-specific)</div><div class=\"line\"> * Returns a new Dataset that only contains elements where `func` returns `true`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def filter(func: FilterFunction[T]): Dataset[T] = &#123;</div><div class=\"line\">  withTypedPlan(TypedFilter(func, logicalPlan))</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</code></pre><h2 id=\"数据的转换\"><a href=\"#数据的转换\" class=\"headerlink\" title=\"数据的转换\"></a>数据的转换</h2>  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div><div class=\"line\">122</div><div class=\"line\">123</div><div class=\"line\">124</div><div class=\"line\">125</div><div class=\"line\">126</div><div class=\"line\">127</div><div class=\"line\">128</div><div class=\"line\">129</div><div class=\"line\">130</div><div class=\"line\">131</div><div class=\"line\">132</div><div class=\"line\">133</div><div class=\"line\">134</div><div class=\"line\">135</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * (Scala-specific)</div><div class=\"line\"> * Returns a new Dataset that contains the result of applying `func` to each element.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">映射操作</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def map[U : Encoder](func: T =&gt; U): Dataset[U] = withTypedPlan &#123;</div><div class=\"line\">  MapElements[T, U](func, logicalPlan)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * (Java-specific)</div><div class=\"line\"> * Returns a new Dataset that contains the result of applying `func` to each element.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U] = &#123;</div><div class=\"line\">  implicit val uEnc = encoder</div><div class=\"line\">  withTypedPlan(MapElements[T, U](func, logicalPlan))</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * (Scala-specific)</div><div class=\"line\"> * Returns a new Dataset that contains the result of applying `func` to each partition.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def mapPartitions[U : Encoder](func: Iterator[T] =&gt; Iterator[U]): Dataset[U] = &#123;</div><div class=\"line\">  new Dataset[U](</div><div class=\"line\">    sparkSession,</div><div class=\"line\">    MapPartitions[T, U](func, logicalPlan),</div><div class=\"line\">    implicitly[Encoder[U]])</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * (Java-specific)</div><div class=\"line\"> * Returns a new Dataset that contains the result of applying `f` to each partition.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U] = &#123;</div><div class=\"line\">  val func: (Iterator[T]) =&gt; Iterator[U] = x =&gt; f.call(x.asJava).asScala</div><div class=\"line\">  mapPartitions(func)(encoder)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a new `DataFrame` that contains the result of applying a serialized R function</div><div class=\"line\"> * `func` to each partition.</div><div class=\"line\"> */</div><div class=\"line\">private[sql] def mapPartitionsInR(</div><div class=\"line\">    func: Array[Byte],</div><div class=\"line\">    packageNames: Array[Byte],</div><div class=\"line\">    broadcastVars: Array[Broadcast[Object]],</div><div class=\"line\">    schema: StructType): DataFrame = &#123;</div><div class=\"line\">  val rowEncoder = encoder.asInstanceOf[ExpressionEncoder[Row]]</div><div class=\"line\">  Dataset.ofRows(</div><div class=\"line\">    sparkSession,</div><div class=\"line\">    MapPartitionsInR(func, packageNames, broadcastVars, schema, rowEncoder, logicalPlan))</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * (Scala-specific)</div><div class=\"line\"> * Returns a new Dataset by first applying a function to all elements of this Dataset,</div><div class=\"line\"> * and then flattening the results.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def flatMap[U : Encoder](func: T =&gt; TraversableOnce[U]): Dataset[U] =</div><div class=\"line\">  mapPartitions(_.flatMap(func))</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * (Java-specific)</div><div class=\"line\"> * Returns a new Dataset by first applying a function to all elements of this Dataset,</div><div class=\"line\"> * and then flattening the results.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U] = &#123;</div><div class=\"line\">  val func: (T) =&gt; Iterator[U] = x =&gt; f.call(x).asScala</div><div class=\"line\">  flatMap(func)(encoder)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Applies a function `f` to all rows.</div><div class=\"line\"> *</div><div class=\"line\"> * @group action</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def foreach(f: T =&gt; Unit): Unit = withNewExecutionId &#123;</div><div class=\"line\">  rdd.foreach(f)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * (Java-specific)</div><div class=\"line\"> * Runs `func` on each element of this Dataset.</div><div class=\"line\"> *</div><div class=\"line\"> * @group action</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def foreach(func: ForeachFunction[T]): Unit = foreach(func.call(_))</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Applies a function `f` to each partition of this Dataset.</div><div class=\"line\"> *</div><div class=\"line\"> * @group action</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def foreachPartition(f: Iterator[T] =&gt; Unit): Unit = withNewExecutionId &#123;</div><div class=\"line\">  rdd.foreachPartition(f)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"数据的提取与聚合\"><a href=\"#数据的提取与聚合\" class=\"headerlink\" title=\"数据的提取与聚合\"></a>数据的提取与聚合</h2>  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div></pre></td><td class=\"code\"><pre><div class=\"line\">  /**</div><div class=\"line\">   * (Java-specific)</div><div class=\"line\">   * Runs `func` on each partition of this Dataset.</div><div class=\"line\">   *</div><div class=\"line\">   * @group action</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">  def foreachPartition(func: ForeachPartitionFunction[T]): Unit =</div><div class=\"line\">    foreachPartition(it =&gt; func.call(it.asJava))</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns the first `n` rows in the Dataset.</div><div class=\"line\">   *</div><div class=\"line\">   * Running take requires moving data into the application&apos;s driver process, and doing so with</div><div class=\"line\">   * a very large `n` can crash the driver process with OutOfMemoryError.</div><div class=\"line\">   *</div><div class=\"line\">   * @group action</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">  def take(n: Int): Array[T] = head(n)</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns the first `n` rows in the Dataset as a list.</div><div class=\"line\">   *</div><div class=\"line\">   * Running take requires moving data into the application&apos;s driver process, and doing so with</div><div class=\"line\">   * a very large `n` can crash the driver process with OutOfMemoryError.</div><div class=\"line\">   *</div><div class=\"line\">   * @group action</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">    获取数据成一个列表</div><div class=\"line\">  def takeAsList(n: Int): java.util.List[T] = java.util.Arrays.asList(take(n) : _*)</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns an array that contains all rows in this Dataset.</div><div class=\"line\">   *</div><div class=\"line\">   * Running collect requires moving all the data into the application&apos;s driver process, and</div><div class=\"line\">   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.</div><div class=\"line\">   *</div><div class=\"line\">   * For Java API, use [[collectAsList]].</div><div class=\"line\">   *</div><div class=\"line\">   * @group action</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\"></div><div class=\"line\">统计数据</div><div class=\"line\">  def collect(): Array[T] = withAction(&quot;collect&quot;, queryExecution)(collectFromPlan)</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a Java list that contains all rows in this Dataset.</div><div class=\"line\">   *</div><div class=\"line\">   * Running collect requires moving all the data into the application&apos;s driver process, and</div><div class=\"line\">   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.</div><div class=\"line\">   *</div><div class=\"line\">   * @group action</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">  def collectAsList(): java.util.List[T] = withAction(&quot;collectAsList&quot;, queryExecution) &#123; plan =&gt;</div><div class=\"line\">    val values = collectFromPlan(plan)</div><div class=\"line\">    java.util.Arrays.asList(values : _*)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Return an iterator that contains all rows in this Dataset.</div><div class=\"line\">   *</div><div class=\"line\">   * The iterator will consume as much memory as the largest partition in this Dataset.</div><div class=\"line\">   *</div><div class=\"line\">   * @note this results in multiple Spark jobs, and if the input Dataset is the result</div><div class=\"line\">   * of a wide transformation (e.g. join with different partitioners), to avoid</div><div class=\"line\">   * recomputing the input Dataset should be cached first.</div><div class=\"line\">   *</div><div class=\"line\">   * @group action</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  def toLocalIterator(): java.util.Iterator[T] = &#123;</div><div class=\"line\">    withAction(&quot;toLocalIterator&quot;, queryExecution) &#123; plan =&gt;</div><div class=\"line\">      plan.executeToIterator().map(boundEnc.fromRow).asJava</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns the number of rows in the Dataset.</div><div class=\"line\">   * @group action</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">  统计行</div><div class=\"line\">  def count(): Long = withAction(&quot;count&quot;, groupBy().count().queryExecution) &#123; plan =&gt;</div><div class=\"line\">    plan.executeCollect().head.getLong(0)</div><div class=\"line\">  &#125;</div></pre></td></tr></table></figure>\n<h2 id=\"分区\"><a href=\"#分区\" class=\"headerlink\" title=\"分区\"></a>分区</h2><pre><code><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset that has exactly `numPartitions` partitions.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">分区</div><div class=\"line\">def repartition(numPartitions: Int): Dataset[T] = withTypedPlan &#123;</div><div class=\"line\">  Repartition(numPartitions, shuffle = true, logicalPlan)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset partitioned by the given partitioning expressions into</div><div class=\"line\"> * `numPartitions`. The resulting Dataset is hash partitioned.</div><div class=\"line\"> *</div><div class=\"line\"> * This is the same operation as &quot;DISTRIBUTE BY&quot; in SQL (Hive QL).</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@scala.annotation.varargs</div><div class=\"line\">def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T] = withTypedPlan &#123;</div><div class=\"line\">  RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, numPartitions)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset partitioned by the given partitioning expressions, using</div><div class=\"line\"> * `spark.sql.shuffle.partitions` as number of partitions.</div><div class=\"line\"> * The resulting Dataset is hash partitioned.</div><div class=\"line\"> *</div><div class=\"line\"> * This is the same operation as &quot;DISTRIBUTE BY&quot; in SQL (Hive QL).</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@scala.annotation.varargs</div><div class=\"line\">def repartition(partitionExprs: Column*): Dataset[T] = withTypedPlan &#123;</div><div class=\"line\">  RepartitionByExpression(</div><div class=\"line\">    partitionExprs.map(_.expr), logicalPlan, sparkSession.sessionState.conf.numShufflePartitions)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset that has exactly `numPartitions` partitions.</div><div class=\"line\"> * Similar to coalesce defined on an `RDD`, this operation results in a narrow dependency, e.g.</div><div class=\"line\"> * if you go from 1000 partitions to 100 partitions, there will not be a shuffle, instead each of</div><div class=\"line\"> * the 100 new partitions will claim 10 of the current partitions.  If a larger number of</div><div class=\"line\"> * partitions is requested, it will stay at the current number of partitions.</div><div class=\"line\"> *</div><div class=\"line\"> * However, if you&apos;re doing a drastic coalesce, e.g. to numPartitions = 1,</div><div class=\"line\"> * this may result in your computation taking place on fewer nodes than</div><div class=\"line\"> * you like (e.g. one node in the case of numPartitions = 1). To avoid this,</div><div class=\"line\"> * you can call repartition. This will add a shuffle step, but means the</div><div class=\"line\"> * current upstream partitions will be executed in parallel (per whatever</div><div class=\"line\"> * the current partitioning is).</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def coalesce(numPartitions: Int): Dataset[T] = withTypedPlan &#123;</div><div class=\"line\">  Repartition(numPartitions, shuffle = false, logicalPlan)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset that contains only the unique rows from this Dataset.</div><div class=\"line\"> * This is an alias for `dropDuplicates`.</div><div class=\"line\"> *</div><div class=\"line\"> * @note Equality checking is performed directly on the encoded representation of the data</div><div class=\"line\"> * and thus is not affected by a custom `equals` function defined on `T`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def distinct(): Dataset[T] = dropDuplicates()</div></pre></td></tr></table></figure>\n</code></pre><h2 id=\"数据的持久化\"><a href=\"#数据的持久化\" class=\"headerlink\" title=\"数据的持久化\"></a>数据的持久化</h2><p>  数据的持久化和缓存策略，一般我们操作rdd都是延迟计算，但是当我们多次重复使用一个rdd的时候可以选择将其缓存而不是每次进行一个计算，可以提高效率。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">                持久化</div><div class=\"line\">def persist(): this.type = &#123;</div><div class=\"line\">  sparkSession.sharedState.cacheManager.cacheQuery(this)</div><div class=\"line\">  this</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def cache(): this.type = persist()</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Persist this Dataset with the given storage level.</div><div class=\"line\"> * @param newLevel One of: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`,</div><div class=\"line\"> *                 `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`,</div><div class=\"line\"> *                 `MEMORY_AND_DISK_2`, etc.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def persist(newLevel: StorageLevel): this.type = &#123;</div><div class=\"line\">  sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel)</div><div class=\"line\">  this</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Get the Dataset&apos;s current storage level, or StorageLevel.NONE if not persisted.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 2.1.0</div><div class=\"line\"> */</div><div class=\"line\">def storageLevel: StorageLevel = &#123;</div><div class=\"line\">  sparkSession.sharedState.cacheManager.lookupCachedData(this).map &#123; cachedData =&gt;</div><div class=\"line\">    cachedData.cachedRepresentation.storageLevel</div><div class=\"line\">  &#125;.getOrElse(StorageLevel.NONE)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.</div><div class=\"line\"> *</div><div class=\"line\"> * @param blocking Whether to block until all blocks are deleted.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def unpersist(blocking: Boolean): this.type = &#123;</div><div class=\"line\">  sparkSession.sharedState.cacheManager.uncacheQuery(this, blocking)</div><div class=\"line\">  this</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def unpersist(): this.type = unpersist(blocking = false)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Represents the content of the Dataset as an `RDD` of `T`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">lazy val rdd: RDD[T] = &#123;</div><div class=\"line\">  val objectType = exprEnc.deserializer.dataType</div><div class=\"line\">  val deserialized = CatalystSerde.deserialize[T](logicalPlan)</div><div class=\"line\">  sparkSession.sessionState.executePlan(deserialized).toRdd.mapPartitions &#123; rows =&gt;</div><div class=\"line\">    rows.map(_.get(0, objectType).asInstanceOf[T])</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns the content of the Dataset as a `JavaRDD` of `T`s.</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def toJavaRDD: JavaRDD[T] = rdd.toJavaRDD()</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns the content of the Dataset as a `JavaRDD` of `T`s.</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def javaRDD: JavaRDD[T] = toJavaRDD</div></pre></td></tr></table></figure>\n<h2 id=\"注册临时表\"><a href=\"#注册临时表\" class=\"headerlink\" title=\"注册临时表\"></a>注册临时表</h2><p>通过注册可以将一个dataset直接当作一个表来操作，这样就可以直接通过sql来执行了，不过返回的结果又是一个dataset<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Registers this Dataset as a temporary table using the given name. The lifetime of this</div><div class=\"line\"> * temporary table is tied to the [[SparkSession]] that was used to create this Dataset.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">注册</div><div class=\"line\">@deprecated(&quot;Use createOrReplaceTempView(viewName) instead.&quot;, &quot;2.0.0&quot;)</div><div class=\"line\">def registerTempTable(tableName: String): Unit = &#123;</div><div class=\"line\">  createOrReplaceTempView(tableName)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Creates a local temporary view using the given name. The lifetime of this</div><div class=\"line\"> * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.</div><div class=\"line\"> *</div><div class=\"line\"> * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that</div><div class=\"line\"> * created it, i.e. it will be automatically dropped when the session terminates. It&apos;s not</div><div class=\"line\"> * tied to any databases, i.e. we can&apos;t use `db1.view1` to reference a local temporary view.</div><div class=\"line\"> *</div><div class=\"line\"> * @throws AnalysisException if the view name is invalid or already exists</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">创建表</div><div class=\"line\">@throws[AnalysisException]</div><div class=\"line\">def createTempView(viewName: String): Unit = withPlan &#123;</div><div class=\"line\">  createTempViewCommand(viewName, replace = false, global = false)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Creates a local temporary view using the given name. The lifetime of this</div><div class=\"line\"> * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def createOrReplaceTempView(viewName: String): Unit = withPlan &#123;</div><div class=\"line\">  createTempViewCommand(viewName, replace = true, global = false)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Creates a global temporary view using the given name. The lifetime of this</div><div class=\"line\"> * temporary view is tied to this Spark application.</div><div class=\"line\"> *</div><div class=\"line\"> * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,</div><div class=\"line\"> * i.e. it will be automatically dropped when the application terminates. It&apos;s tied to a system</div><div class=\"line\"> * preserved database `global_temp`, and we must use the qualified name to refer a global temp</div><div class=\"line\"> * view, e.g. `SELECT * FROM global_temp.view1`.</div><div class=\"line\"> *</div><div class=\"line\"> * @throws AnalysisException if the view name is invalid or already exists</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 2.1.0</div><div class=\"line\"> */</div><div class=\"line\">@throws[AnalysisException]</div><div class=\"line\">def createGlobalTempView(viewName: String): Unit = withPlan &#123;</div><div class=\"line\">  createTempViewCommand(viewName, replace = false, global = true)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">private def createTempViewCommand(</div><div class=\"line\">    viewName: String,</div><div class=\"line\">    replace: Boolean,</div><div class=\"line\">    global: Boolean): CreateViewCommand = &#123;</div><div class=\"line\">  val viewType = if (global) GlobalTempView else LocalTempView</div><div class=\"line\"></div><div class=\"line\">  val tableIdentifier = try &#123;</div><div class=\"line\">    sparkSession.sessionState.sqlParser.parseTableIdentifier(viewName)</div><div class=\"line\">  &#125; catch &#123;</div><div class=\"line\">    case _: ParseException =&gt; throw new AnalysisException(s&quot;Invalid view name: $viewName&quot;)</div><div class=\"line\">  &#125;</div><div class=\"line\">  CreateViewCommand(</div><div class=\"line\">    name = tableIdentifier,</div><div class=\"line\">    userSpecifiedColumns = Nil,</div><div class=\"line\">    comment = None,</div><div class=\"line\">    properties = Map.empty,</div><div class=\"line\">    originalText = None,</div><div class=\"line\">    child = logicalPlan,</div><div class=\"line\">    allowExisting = false,</div><div class=\"line\">    replace = replace,</div><div class=\"line\">    viewType = viewType)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h2 id=\"数据保存\"><a href=\"#数据保存\" class=\"headerlink\" title=\"数据保存\"></a>数据保存</h2><pre><code>数据保存有一个专门的write类来处理，这里就是调用write方法返回一个write对象来实现的\n</code></pre><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Interface for saving the content of the non-streaming Dataset out into external storage.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def write: DataFrameWriter[T] = &#123;</div><div class=\"line\">  if (isStreaming) &#123;</div><div class=\"line\">    logicalPlan.failAnalysis(</div><div class=\"line\">      &quot;&apos;write&apos; can not be called on streaming Dataset/DataFrame&quot;)</div><div class=\"line\">  &#125;</div><div class=\"line\">  new DataFrameWriter[T](this)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * Interface for saving the content of the streaming Dataset out into external storage.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def writeStream: DataStreamWriter[T] = &#123;</div><div class=\"line\">  if (!isStreaming) &#123;</div><div class=\"line\">    logicalPlan.failAnalysis(</div><div class=\"line\">      &quot;&apos;writeStream&apos; can be called only on streaming Dataset/DataFrame&quot;)</div><div class=\"line\">  &#125;</div><div class=\"line\">  new DataStreamWriter[T](this)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"转换成json格式\"><a href=\"#转换成json格式\" class=\"headerlink\" title=\"转换成json格式\"></a>转换成json格式</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Returns the content of the Dataset as a Dataset of JSON strings.</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def toJSON: Dataset[String] = &#123;</div><div class=\"line\">  val rowSchema = this.schema</div><div class=\"line\">  val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone</div><div class=\"line\">  val rdd: RDD[String] = queryExecution.toRdd.mapPartitions &#123; iter =&gt;</div><div class=\"line\">    val writer = new CharArrayWriter()</div><div class=\"line\">    // create the Generator without separator inserted between 2 records</div><div class=\"line\">    val gen = new JacksonGenerator(rowSchema, writer,</div><div class=\"line\">      new JSONOptions(Map.empty[String, String], sessionLocalTimeZone))</div><div class=\"line\"></div><div class=\"line\">    new Iterator[String] &#123;</div><div class=\"line\">      override def hasNext: Boolean = iter.hasNext</div><div class=\"line\">      override def next(): String = &#123;</div><div class=\"line\">        gen.write(iter.next())</div><div class=\"line\">        gen.flush()</div><div class=\"line\"></div><div class=\"line\">        val json = writer.toString</div><div class=\"line\">        if (hasNext) &#123;</div><div class=\"line\">          writer.reset()</div><div class=\"line\">        &#125; else &#123;</div><div class=\"line\">          gen.close()</div><div class=\"line\">        &#125;</div><div class=\"line\"></div><div class=\"line\">        json</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">  import sparkSession.implicits.newStringEncoder</div><div class=\"line\">  sparkSession.createDataset(rdd)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"获取文件列表\"><a href=\"#获取文件列表\" class=\"headerlink\" title=\"获取文件列表\"></a>获取文件列表</h2><p>  可以获取当前dataSet都加载了那些文件<br>  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Returns a best-effort snapshot of the files that compose this Dataset. This method simply</div><div class=\"line\"> * asks each constituent BaseRelation for its respective files and takes the union of all results.</div><div class=\"line\"> * Depending on the source relations, this may not find all input files. Duplicates are removed.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def inputFiles: Array[String] = &#123;</div><div class=\"line\">  val files: Seq[String] = queryExecution.optimizedPlan.collect &#123;</div><div class=\"line\">    case LogicalRelation(fsBasedRelation: FileRelation, _, _) =&gt;</div><div class=\"line\">      fsBasedRelation.inputFiles</div><div class=\"line\">    case fr: FileRelation =&gt;</div><div class=\"line\">      fr.inputFiles</div><div class=\"line\">  &#125;.flatten</div><div class=\"line\">  files.toSet.toArray</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n","excerpt":"<p>记录一下sparksql的dataframe 中常用的操作，spark在大数据处理方面有很广泛的应供，每天都在研究spark的源码，简单记录一下以便后续查阅,今天先简单整理一下，后续逐步完善.<br>版本:spark 2.0.1<br>","more":"</p>\n<h2 id=\"数据显示\"><a href=\"#数据显示\" class=\"headerlink\" title=\"数据显示\"></a>数据显示</h2><p>这个showString 是spark内部的方法，我们实际是调用不到的，但是我们调用的show方法最终都是调用了这个showString<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Compose the string representing rows for output</div><div class=\"line\"> *</div><div class=\"line\"> * @param _numRows Number of rows to show</div><div class=\"line\"> * @param truncate If set to more than 0, truncates strings to `truncate` characters and</div><div class=\"line\"> *                   all cells will be aligned right.</div><div class=\"line\"> */</div><div class=\"line\">private[sql] def showString(_numRows: Int, truncate: Int = 20): String</div></pre></td></tr></table></figure></p>\n<h2 id=\"将dataSet转换成dataFrame\"><a href=\"#将dataSet转换成dataFrame\" class=\"headerlink\" title=\"将dataSet转换成dataFrame\"></a>将dataSet转换成dataFrame</h2><p>   datafram其实是按列来存储的dataset<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed.</div><div class=\"line\"> * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with</div><div class=\"line\"> * meaningful names. For example:</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   val rdd: RDD[(Int, String)] = ...</div><div class=\"line\"> *   rdd.toDF()  // this implicit conversion creates a DataFrame with column name `_1` and `_2`</div><div class=\"line\"> *   rdd.toDF(&quot;id&quot;, &quot;name&quot;)  // this creates a DataFrame with column name &quot;id&quot; and &quot;name&quot;</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@scala.annotation.varargs</div><div class=\"line\">def toDF(colNames: String*): DataFrame = &#123;</div><div class=\"line\">  require(schema.size == colNames.size,</div><div class=\"line\">    &quot;The number of columns doesn&apos;t match.\\n&quot; +</div><div class=\"line\">      s&quot;Old column names ($&#123;schema.size&#125;): &quot; + schema.fields.map(_.name).mkString(&quot;, &quot;) + &quot;\\n&quot; +</div><div class=\"line\">      s&quot;New column names ($&#123;colNames.size&#125;): &quot; + colNames.mkString(&quot;, &quot;))</div><div class=\"line\"></div><div class=\"line\">  val newCols = logicalPlan.output.zip(colNames).map &#123; case (oldAttribute, newName) =&gt;</div><div class=\"line\">    Column(oldAttribute).as(newName)</div><div class=\"line\">  &#125;</div><div class=\"line\">  select(newCols : _*)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>输出当前dataset的结构信息<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Returns the schema of this Dataset.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def schema: StructType = queryExecution.analyzed.schema</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Prints the schema to the console in a nice tree format.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">// scalastyle:off println</div><div class=\"line\">def printSchema(): Unit = println(schema.treeString)</div><div class=\"line\">// scalastyle:on println</div></pre></td></tr></table></figure></p>\n<p>输出一些调试信息<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Prints the plans (logical and physical) to the console for debugging purposes.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def explain(extended: Boolean): Unit = &#123;</div><div class=\"line\">  val explain = ExplainCommand(queryExecution.logical, extended = extended)</div><div class=\"line\">  sparkSession.sessionState.executePlan(explain).executedPlan.executeCollect().foreach &#123;</div><div class=\"line\">    // scalastyle:off println</div><div class=\"line\">    r =&gt; println(r.getString(0))</div><div class=\"line\">    // scalastyle:on println</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Prints the physical plan to the console for debugging purposes.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def explain(): Unit = explain(extended = false)</div></pre></td></tr></table></figure></p>\n<p>输出列名以及每个列的类型<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Returns all column names and their data types as an array.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def dtypes: Array[(String, String)] = schema.fields.map &#123; field =&gt;</div><div class=\"line\">  (field.name, field.dataType.toString)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns all column names as an array.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def columns: Array[String] = schema.fields.map(_.name)</div></pre></td></tr></table></figure></p>\n<p>是否能够获取数据<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Returns true if the `collect` and `take` methods can be run locally</div><div class=\"line\"> * (without any Spark executors).</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation]</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns true if this Dataset contains one or more sources that continuously</div><div class=\"line\"> * return data as it arrives. A Dataset that reads data from a streaming source</div><div class=\"line\"> * must be executed as a `StreamingQuery` using the `start()` method in</div><div class=\"line\"> * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or</div><div class=\"line\"> * `collect()`, will throw an [[AnalysisException]] when there is a streaming</div><div class=\"line\"> * source present.</div><div class=\"line\"> *</div><div class=\"line\"> * @group streaming</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def isStreaming: Boolean = logicalPlan.isStreaming</div></pre></td></tr></table></figure></p>\n<p><strong>检查点，以前没有用过，需要在研究一下</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate</div><div class=\"line\"> * the logical plan of this Dataset, which is especially useful in iterative algorithms where the</div><div class=\"line\"> * plan may grow exponentially. It will be saved to files inside the checkpoint</div><div class=\"line\"> * directory set with `SparkContext#setCheckpointDir`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 2.1.0</div><div class=\"line\"> */</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def checkpoint(): Dataset[T] = checkpoint(eager = true)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the</div><div class=\"line\"> * logical plan of this Dataset, which is especially useful in iterative algorithms where the</div><div class=\"line\"> * plan may grow exponentially. It will be saved to files inside the checkpoint</div><div class=\"line\"> * directory set with `SparkContext#setCheckpointDir`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 2.1.0</div><div class=\"line\"> */</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def checkpoint(eager: Boolean): Dataset[T] = &#123;</div><div class=\"line\">  val internalRdd = queryExecution.toRdd.map(_.copy())</div><div class=\"line\">  internalRdd.checkpoint()</div><div class=\"line\"></div><div class=\"line\">  if (eager) &#123;</div><div class=\"line\">    internalRdd.count()</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  val physicalPlan = queryExecution.executedPlan</div></pre></td></tr></table></figure></p>\n<p>这个什么鬼需要在分析一下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div></pre></td><td class=\"code\"><pre><div class=\"line\">  // Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the</div><div class=\"line\">  // size of `PartitioningCollection` may grow exponentially for queries involving deep inner</div><div class=\"line\">  // joins.</div><div class=\"line\">  def firstLeafPartitioning(partitioning: Partitioning): Partitioning = &#123;</div><div class=\"line\">    partitioning match &#123;</div><div class=\"line\">      case p: PartitioningCollection =&gt; firstLeafPartitioning(p.partitionings.head)</div><div class=\"line\">      case p =&gt; p</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  val outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning)</div><div class=\"line\"></div><div class=\"line\">  Dataset.ofRows(</div><div class=\"line\">    sparkSession,</div><div class=\"line\">    LogicalRDD(</div><div class=\"line\">      logicalPlan.output,</div><div class=\"line\">      internalRdd,</div><div class=\"line\">      outputPartitioning,</div><div class=\"line\">      physicalPlan.outputOrdering</div><div class=\"line\">    )(sparkSession)).as[T]</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>水印？？？<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time</div><div class=\"line\"> * before which we assume no more late data is going to arrive.</div><div class=\"line\"> *</div><div class=\"line\"> * Spark will use this watermark for several purposes:</div><div class=\"line\"> *  - To know when a given time window aggregation can be finalized and thus can be emitted when</div><div class=\"line\"> *    using output modes that do not allow updates.</div><div class=\"line\"> *  - To minimize the amount of state that we need to keep for on-going aggregations,</div><div class=\"line\"> *    `mapGroupsWithState` and `dropDuplicates` operators.</div><div class=\"line\"> *</div><div class=\"line\"> *  The current watermark is computed by looking at the `MAX(eventTime)` seen across</div><div class=\"line\"> *  all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost</div><div class=\"line\"> *  of coordinating this value across partitions, the actual watermark used is only guaranteed</div><div class=\"line\"> *  to be at least `delayThreshold` behind the actual event time.  In some cases we may still</div><div class=\"line\"> *  process records that arrive more than `delayThreshold` late.</div><div class=\"line\"> *</div><div class=\"line\"> * @param eventTime the name of the column that contains the event time of the row.</div><div class=\"line\"> * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest</div><div class=\"line\"> *                       record that has been processed in the form of an interval</div><div class=\"line\"> *                       (e.g. &quot;1 minute&quot; or &quot;5 hours&quot;).</div><div class=\"line\"> *</div><div class=\"line\"> * @group streaming</div><div class=\"line\"> * @since 2.1.0</div><div class=\"line\"> */</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">// We only accept an existing column name, not a derived column here as a watermark that is</div><div class=\"line\">// defined on a derived column cannot referenced elsewhere in the plan.</div><div class=\"line\">def withWatermark(eventTime: String, delayThreshold: String): Dataset[T] = withTypedPlan &#123;</div><div class=\"line\">  val parsedDelay =</div><div class=\"line\">    Option(CalendarInterval.fromString(&quot;interval &quot; + delayThreshold))</div><div class=\"line\">      .getOrElse(throw new AnalysisException(s&quot;Unable to parse time delay &apos;$delayThreshold&apos;&quot;))</div><div class=\"line\">  EventTimeWatermark(UnresolvedAttribute(eventTime), parsedDelay, logicalPlan)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h2 id=\"数据展示\"><a href=\"#数据展示\" class=\"headerlink\" title=\"数据展示\"></a>数据展示</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div></pre></td><td class=\"code\"><pre><div class=\"line\">  /**</div><div class=\"line\">   * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated,</div><div class=\"line\">   * and all cells will be aligned right. For example:</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   year  month AVG(&apos;Adj Close) MAX(&apos;Adj Close)</div><div class=\"line\">   *   1980  12    0.503218        0.595103</div><div class=\"line\">   *   1981  01    0.523289        0.570307</div><div class=\"line\">   *   1982  02    0.436504        0.475256</div><div class=\"line\">   *   1983  03    0.410516        0.442194</div><div class=\"line\">   *   1984  04    0.450090        0.483521</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * @param numRows Number of rows to show</div><div class=\"line\">   *</div><div class=\"line\">   * @group action</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">显示datafram中的指定数量的数据，默认字段长度超过20位则截断。</div><div class=\"line\">  def show(numRows: Int): Unit = show(numRows, truncate = true)</div><div class=\"line\"></div><div class=\"line\">显示datafram的数据，默认取前面20条记录显示，默认字段长度超过20位则截断。</div><div class=\"line\">  </div><div class=\"line\">  def show(): Unit = show(20)</div><div class=\"line\"></div><div class=\"line\">显示datafram的数据，默认取前面20条记录显示，通过truncate选择是否需要全部显示每一列的信息。</div><div class=\"line\">  def show(truncate: Boolean): Unit = show(20, truncate)</div><div class=\"line\"></div><div class=\"line\">显示datafram的数据，numRows为显示数量，通过truncate选择是否需要全部显示每一列的信息。</div><div class=\"line\">  def show(numRows: Int, truncate: Boolean): Unit = if (truncate) &#123;</div><div class=\"line\">  def show(numRows: Int, truncate: Int): Unit = println(showString(numRows, truncate))</div></pre></td></tr></table></figure>\n<h2 id=\"数据的关联\"><a href=\"#数据的关联\" class=\"headerlink\" title=\"数据的关联\"></a>数据的关联</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div><div class=\"line\">122</div><div class=\"line\">123</div><div class=\"line\">124</div><div class=\"line\">125</div><div class=\"line\">126</div><div class=\"line\">127</div><div class=\"line\">128</div><div class=\"line\">129</div><div class=\"line\">130</div><div class=\"line\">131</div><div class=\"line\">132</div><div class=\"line\">133</div><div class=\"line\">134</div><div class=\"line\">135</div><div class=\"line\">136</div><div class=\"line\">137</div><div class=\"line\">138</div><div class=\"line\">139</div><div class=\"line\">140</div><div class=\"line\">141</div><div class=\"line\">142</div><div class=\"line\">143</div><div class=\"line\">144</div><div class=\"line\">145</div><div class=\"line\">146</div><div class=\"line\">147</div><div class=\"line\">148</div><div class=\"line\">149</div><div class=\"line\">150</div><div class=\"line\">151</div><div class=\"line\">152</div><div class=\"line\">153</div><div class=\"line\">154</div><div class=\"line\">155</div><div class=\"line\">156</div><div class=\"line\">157</div><div class=\"line\">158</div><div class=\"line\">159</div><div class=\"line\">160</div><div class=\"line\">161</div><div class=\"line\">162</div><div class=\"line\">163</div><div class=\"line\">164</div><div class=\"line\">165</div><div class=\"line\">166</div><div class=\"line\">167</div><div class=\"line\">168</div><div class=\"line\">169</div><div class=\"line\">170</div><div class=\"line\">171</div><div class=\"line\">172</div><div class=\"line\">173</div><div class=\"line\">174</div><div class=\"line\">175</div><div class=\"line\">176</div><div class=\"line\">177</div><div class=\"line\">178</div><div class=\"line\">179</div><div class=\"line\">180</div><div class=\"line\">181</div><div class=\"line\">182</div><div class=\"line\">183</div><div class=\"line\">184</div><div class=\"line\">185</div><div class=\"line\">186</div><div class=\"line\">187</div><div class=\"line\">188</div><div class=\"line\">189</div><div class=\"line\">190</div><div class=\"line\">191</div><div class=\"line\">192</div><div class=\"line\">193</div><div class=\"line\">194</div><div class=\"line\">195</div><div class=\"line\">196</div><div class=\"line\">197</div><div class=\"line\">198</div><div class=\"line\">199</div><div class=\"line\">200</div><div class=\"line\">201</div><div class=\"line\">202</div><div class=\"line\">203</div><div class=\"line\">204</div><div class=\"line\">205</div><div class=\"line\">206</div><div class=\"line\">207</div><div class=\"line\">208</div><div class=\"line\">209</div><div class=\"line\">210</div><div class=\"line\">211</div><div class=\"line\">212</div><div class=\"line\">213</div><div class=\"line\">214</div><div class=\"line\">215</div><div class=\"line\">216</div><div class=\"line\">217</div><div class=\"line\">218</div><div class=\"line\">219</div><div class=\"line\">220</div><div class=\"line\">221</div><div class=\"line\">222</div><div class=\"line\">223</div><div class=\"line\">224</div><div class=\"line\">225</div><div class=\"line\">226</div><div class=\"line\">227</div><div class=\"line\">228</div><div class=\"line\">229</div><div class=\"line\">230</div><div class=\"line\">231</div><div class=\"line\">232</div><div class=\"line\">233</div><div class=\"line\">234</div><div class=\"line\">235</div><div class=\"line\">236</div><div class=\"line\">237</div><div class=\"line\">238</div><div class=\"line\">239</div><div class=\"line\">240</div><div class=\"line\">241</div><div class=\"line\">242</div><div class=\"line\">243</div><div class=\"line\">244</div><div class=\"line\">245</div><div class=\"line\">246</div><div class=\"line\">247</div><div class=\"line\">248</div><div class=\"line\">249</div><div class=\"line\">250</div><div class=\"line\">251</div><div class=\"line\">252</div><div class=\"line\">253</div><div class=\"line\">254</div><div class=\"line\">255</div><div class=\"line\">256</div><div class=\"line\">257</div><div class=\"line\">258</div><div class=\"line\">259</div><div class=\"line\">260</div><div class=\"line\">261</div><div class=\"line\">262</div><div class=\"line\">263</div><div class=\"line\">264</div><div class=\"line\">265</div><div class=\"line\">266</div><div class=\"line\">267</div><div class=\"line\">268</div><div class=\"line\">269</div><div class=\"line\">270</div><div class=\"line\">271</div><div class=\"line\">272</div><div class=\"line\">273</div><div class=\"line\">274</div><div class=\"line\">275</div><div class=\"line\">276</div><div class=\"line\">277</div><div class=\"line\">278</div><div class=\"line\">279</div><div class=\"line\">280</div><div class=\"line\">281</div><div class=\"line\">282</div><div class=\"line\">283</div><div class=\"line\">284</div><div class=\"line\">285</div><div class=\"line\">286</div><div class=\"line\">287</div><div class=\"line\">288</div><div class=\"line\">289</div><div class=\"line\">290</div><div class=\"line\">291</div><div class=\"line\">292</div><div class=\"line\">293</div><div class=\"line\">294</div><div class=\"line\">295</div><div class=\"line\">296</div><div class=\"line\">297</div><div class=\"line\">298</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\">返回dataset中空值操作算子</div><div class=\"line\">  def na: DataFrameNaFunctions = new DataFrameNaFunctions(toDF())</div><div class=\"line\"></div><div class=\"line\">返回dataset中统计操作算子</div><div class=\"line\">  def stat: DataFrameStatFunctions = new DataFrameStatFunctions(toDF())</div><div class=\"line\">关联dataframe</div><div class=\"line\">  def join(right: Dataset[_]): DataFrame = withPlan &#123;</div><div class=\"line\">    Join(logicalPlan, right.logicalPlan, joinType = Inner, None)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Inner equi-join with another `DataFrame` using the given column.</div><div class=\"line\">   *</div><div class=\"line\">   * Different from other join functions, the join column will only appear once in the output,</div><div class=\"line\">   * i.e. similar to SQL&apos;s `JOIN USING` syntax.</div><div class=\"line\">   *</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   // Joining df1 and df2 using the column &quot;user_id&quot;</div><div class=\"line\">   *   df1.join(df2, &quot;user_id&quot;)</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * @param right Right side of the join operation.</div><div class=\"line\">   * @param usingColumn Name of the column to join on. This column must exist on both sides.</div><div class=\"line\">   *</div><div class=\"line\">   * @note If you perform a self-join using this function without aliasing the input</div><div class=\"line\">   * `DataFrame`s, you will NOT be able to reference any columns after the join, since</div><div class=\"line\">   * there is no way to disambiguate which side of the join you would like to reference.</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">指定字段关联</div><div class=\"line\">  def join(right: Dataset[_], usingColumn: String): DataFrame = &#123;</div><div class=\"line\">    join(right, Seq(usingColumn))</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Inner equi-join with another `DataFrame` using the given columns.</div><div class=\"line\">   *</div><div class=\"line\">   * Different from other join functions, the join columns will only appear once in the output,</div><div class=\"line\">   * i.e. similar to SQL&apos;s `JOIN USING` syntax.</div><div class=\"line\">   *</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   // Joining df1 and df2 using the columns &quot;user_id&quot; and &quot;user_name&quot;</div><div class=\"line\">   *   df1.join(df2, Seq(&quot;user_id&quot;, &quot;user_name&quot;))</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * @param right Right side of the join operation.</div><div class=\"line\">   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.</div><div class=\"line\">   *</div><div class=\"line\">   * @note If you perform a self-join using this function without aliasing the input</div><div class=\"line\">   * `DataFrame`s, you will NOT be able to reference any columns after the join, since</div><div class=\"line\">   * there is no way to disambiguate which side of the join you would like to reference.</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">指定关联字段，不同dataframe中同名字段的key不重复出现</div><div class=\"line\">  def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame = &#123;</div><div class=\"line\">    join(right, usingColumns, &quot;inner&quot;)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Equi-join with another `DataFrame` using the given columns. A cross join with a predicate</div><div class=\"line\">   * is specified as an inner join. If you would explicitly like to perform a cross join use the</div><div class=\"line\">   * `crossJoin` method.</div><div class=\"line\">   *</div><div class=\"line\">   * Different from other join functions, the join columns will only appear once in the output,</div><div class=\"line\">   * i.e. similar to SQL&apos;s `JOIN USING` syntax.</div><div class=\"line\">   *</div><div class=\"line\">   * @param right Right side of the join operation.</div><div class=\"line\">   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.</div><div class=\"line\">   * @param joinType Type of join to perform. Default `inner`. Must be one of:</div><div class=\"line\">   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,</div><div class=\"line\">   *                 `right`, `right_outer`, `left_semi`, `left_anti`.</div><div class=\"line\">   *</div><div class=\"line\">   * @note If you perform a self-join using this function without aliasing the input</div><div class=\"line\">   * `DataFrame`s, you will NOT be able to reference any columns after the join, since</div><div class=\"line\">   * there is no way to disambiguate which side of the join you would like to reference.</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">指定关联的类型</div><div class=\"line\">  def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame = &#123;</div><div class=\"line\">    // Analyze the self join. The assumption is that the analyzer will disambiguate left vs right</div><div class=\"line\">    // by creating a new instance for one of the branch.</div><div class=\"line\">    val joined = sparkSession.sessionState.executePlan(</div><div class=\"line\">      Join(logicalPlan, right.logicalPlan, joinType = JoinType(joinType), None))</div><div class=\"line\">      .analyzed.asInstanceOf[Join]</div><div class=\"line\"></div><div class=\"line\">    withPlan &#123;</div><div class=\"line\">      Join(</div><div class=\"line\">        joined.left,</div><div class=\"line\">        joined.right,</div><div class=\"line\">        UsingJoin(JoinType(joinType), usingColumns),</div><div class=\"line\">        None)</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Inner join with another `DataFrame`, using the given join expression.</div><div class=\"line\">   *</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   // The following two are equivalent:</div><div class=\"line\">   *   df1.join(df2, $&quot;df1Key&quot; === $&quot;df2Key&quot;)</div><div class=\"line\">   *   df1.join(df2).where($&quot;df1Key&quot; === $&quot;df2Key&quot;)</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">通过表达式进行关联</div><div class=\"line\">  def join(right: Dataset[_], joinExprs: Column): DataFrame = join(right, joinExprs, &quot;inner&quot;)</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Join with another `DataFrame`, using the given join expression. The following performs</div><div class=\"line\">   * a full outer join between `df1` and `df2`.</div><div class=\"line\">   *</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   // Scala:</div><div class=\"line\">   *   import org.apache.spark.sql.functions._</div><div class=\"line\">   *   df1.join(df2, $&quot;df1Key&quot; === $&quot;df2Key&quot;, &quot;outer&quot;)</div><div class=\"line\">   *</div><div class=\"line\">   *   // Java:</div><div class=\"line\">   *   import static org.apache.spark.sql.functions.*;</div><div class=\"line\">   *   df1.join(df2, col(&quot;df1Key&quot;).equalTo(col(&quot;df2Key&quot;)), &quot;outer&quot;);</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * @param right Right side of the join.</div><div class=\"line\">   * @param joinExprs Join expression.</div><div class=\"line\">   * @param joinType Type of join to perform. Default `inner`. Must be one of:</div><div class=\"line\">   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,</div><div class=\"line\">   *                 `right`, `right_outer`, `left_semi`, `left_anti`.</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">通过列名表达式进行关联</div><div class=\"line\">  def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame = &#123;</div><div class=\"line\">    // Note that in this function, we introduce a hack in the case of self-join to automatically</div><div class=\"line\">    // resolve ambiguous join conditions into ones that might make sense [SPARK-6231].</div><div class=\"line\">    // Consider this case: df.join(df, df(&quot;key&quot;) === df(&quot;key&quot;))</div><div class=\"line\">    // Since df(&quot;key&quot;) === df(&quot;key&quot;) is a trivially true condition, this actually becomes a</div><div class=\"line\">    // cartesian join. However, most likely users expect to perform a self join using &quot;key&quot;.</div><div class=\"line\">    // With that assumption, this hack turns the trivially true condition into equality on join</div><div class=\"line\">    // keys that are resolved to both sides.</div><div class=\"line\"></div><div class=\"line\">    // Trigger analysis so in the case of self-join, the analyzer will clone the plan.</div><div class=\"line\">    // After the cloning, left and right side will have distinct expression ids.</div><div class=\"line\">    val plan = withPlan(</div><div class=\"line\">      Join(logicalPlan, right.logicalPlan, JoinType(joinType), Some(joinExprs.expr)))</div><div class=\"line\">      .queryExecution.analyzed.asInstanceOf[Join]</div><div class=\"line\"></div><div class=\"line\">    // If auto self join alias is disabled, return the plan.</div><div class=\"line\">    if (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) &#123;</div><div class=\"line\">      return withPlan(plan)</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    // If left/right have no output set intersection, return the plan.</div><div class=\"line\">    val lanalyzed = withPlan(this.logicalPlan).queryExecution.analyzed</div><div class=\"line\">    val ranalyzed = withPlan(right.logicalPlan).queryExecution.analyzed</div><div class=\"line\">    if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) &#123;</div><div class=\"line\">      return withPlan(plan)</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    // Otherwise, find the trivially true predicates and automatically resolves them to both sides.</div><div class=\"line\">    // By the time we get here, since we have already run analysis, all attributes should&apos;ve been</div><div class=\"line\">    // resolved and become AttributeReference.</div><div class=\"line\">    val cond = plan.condition.map &#123; _.transform &#123;</div><div class=\"line\">      case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference)</div><div class=\"line\">          if a.sameRef(b) =&gt;</div><div class=\"line\">        catalyst.expressions.EqualTo(</div><div class=\"line\">          withPlan(plan.left).resolve(a.name),</div><div class=\"line\">          withPlan(plan.right).resolve(b.name))</div><div class=\"line\">    &#125;&#125;</div><div class=\"line\"></div><div class=\"line\">    withPlan &#123;</div><div class=\"line\">      plan.copy(condition = cond)</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Explicit cartesian join with another `DataFrame`.</div><div class=\"line\">   *</div><div class=\"line\">   * @param right Right side of the join operation.</div><div class=\"line\">   *</div><div class=\"line\">   * @note Cartesian joins are very expensive without an extra filter that can be pushed down.</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.1.0</div><div class=\"line\">   */</div><div class=\"line\">全表关联</div><div class=\"line\">  def crossJoin(right: Dataset[_]): DataFrame = withPlan &#123;</div><div class=\"line\">    Join(logicalPlan, right.logicalPlan, joinType = Cross, None)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * :: Experimental ::</div><div class=\"line\">   * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to</div><div class=\"line\">   * true.</div><div class=\"line\">   *</div><div class=\"line\">   * This is similar to the relation `join` function with one important difference in the</div><div class=\"line\">   * result schema. Since `joinWith` preserves objects present on either side of the join, the</div><div class=\"line\">   * result schema is similarly nested into a tuple under the column names `_1` and `_2`.</div><div class=\"line\">   *</div><div class=\"line\">   * This type of join can be useful both for preserving type-safety with the original object</div><div class=\"line\">   * types as well as working with relational data where either side of the join has column</div><div class=\"line\">   * names in common.</div><div class=\"line\">   *</div><div class=\"line\">   * @param other Right side of the join.</div><div class=\"line\">   * @param condition Join expression.</div><div class=\"line\">   * @param joinType Type of join to perform. Default `inner`. Must be one of:</div><div class=\"line\">   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,</div><div class=\"line\">   *                 `right`, `right_outer`, `left_semi`, `left_anti`.</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">一种特殊的关联，得到的结果集的结构不同于普通的关联结果</div><div class=\"line\">  @Experimental</div><div class=\"line\">  @InterfaceStability.Evolving</div><div class=\"line\">  def joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = &#123;</div><div class=\"line\">    // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved,</div><div class=\"line\">    // etc.</div><div class=\"line\">    val joined = sparkSession.sessionState.executePlan(</div><div class=\"line\">      Join(</div><div class=\"line\">        this.logicalPlan,</div><div class=\"line\">        other.logicalPlan,</div><div class=\"line\">        JoinType(joinType),</div><div class=\"line\">        Some(condition.expr))).analyzed.asInstanceOf[Join]</div><div class=\"line\"></div><div class=\"line\">    // For both join side, combine all outputs into a single column and alias it with &quot;_1&quot; or &quot;_2&quot;,</div><div class=\"line\">    // to match the schema for the encoder of the join result.</div><div class=\"line\">    // Note that we do this before joining them, to enable the join operator to return null for one</div><div class=\"line\">    // side, in cases like outer-join.</div><div class=\"line\">    val left = &#123;</div><div class=\"line\">      val combined = if (this.exprEnc.flat) &#123;</div><div class=\"line\">        assert(joined.left.output.length == 1)</div><div class=\"line\">        Alias(joined.left.output.head, &quot;_1&quot;)()</div><div class=\"line\">      &#125; else &#123;</div><div class=\"line\">        Alias(CreateStruct(joined.left.output), &quot;_1&quot;)()</div><div class=\"line\">      &#125;</div><div class=\"line\">      Project(combined :: Nil, joined.left)</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    val right = &#123;</div><div class=\"line\">      val combined = if (other.exprEnc.flat) &#123;</div><div class=\"line\">        assert(joined.right.output.length == 1)</div><div class=\"line\">        Alias(joined.right.output.head, &quot;_2&quot;)()</div><div class=\"line\">      &#125; else &#123;</div><div class=\"line\">        Alias(CreateStruct(joined.right.output), &quot;_2&quot;)()</div><div class=\"line\">      &#125;</div><div class=\"line\">      Project(combined :: Nil, joined.right)</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    // Rewrites the join condition to make the attribute point to correct column/field, after we</div><div class=\"line\">    // combine the outputs of each join side.</div><div class=\"line\">    val conditionExpr = joined.condition.get transformUp &#123;</div><div class=\"line\">      case a: Attribute if joined.left.outputSet.contains(a) =&gt;</div><div class=\"line\">        if (this.exprEnc.flat) &#123;</div><div class=\"line\">          left.output.head</div><div class=\"line\">        &#125; else &#123;</div><div class=\"line\">          val index = joined.left.output.indexWhere(_.exprId == a.exprId)</div><div class=\"line\">          GetStructField(left.output.head, index)</div><div class=\"line\">        &#125;</div><div class=\"line\">      case a: Attribute if joined.right.outputSet.contains(a) =&gt;</div><div class=\"line\">        if (other.exprEnc.flat) &#123;</div><div class=\"line\">          right.output.head</div><div class=\"line\">        &#125; else &#123;</div><div class=\"line\">          val index = joined.right.output.indexWhere(_.exprId == a.exprId)</div><div class=\"line\">          GetStructField(right.output.head, index)</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    implicit val tuple2Encoder: Encoder[(T, U)] =</div><div class=\"line\">      ExpressionEncoder.tuple(this.exprEnc, other.exprEnc)</div><div class=\"line\"></div><div class=\"line\">    withTypedPlan(Join(left, right, joined.joinType, Some(conditionExpr)))</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * :: Experimental ::</div><div class=\"line\">   * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair</div><div class=\"line\">   * where `condition` evaluates to true.</div><div class=\"line\">   *</div><div class=\"line\">   * @param other Right side of the join.</div><div class=\"line\">   * @param condition Join expression.</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">  @Experimental</div><div class=\"line\">  @InterfaceStability.Evolving</div><div class=\"line\">  def joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = &#123;</div><div class=\"line\">    joinWith(other, condition, &quot;inner&quot;)</div><div class=\"line\">  &#125;</div></pre></td></tr></table></figure>\n<h2 id=\"排序分组\"><a href=\"#排序分组\" class=\"headerlink\" title=\"排序分组\"></a>排序分组</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div></pre></td><td class=\"code\"><pre><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new Dataset with each partition sorted by the given expressions.</div><div class=\"line\">   *</div><div class=\"line\">   * This is the same operation as &quot;SORT BY&quot; in SQL (Hive QL).</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">根据指定字段对每个分区进行排序</div><div class=\"line\">  @scala.annotation.varargs</div><div class=\"line\">  def sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T] = &#123;</div><div class=\"line\">    sortWithinPartitions((sortCol +: sortCols).map(Column(_)) : _*)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new Dataset with each partition sorted by the given expressions.</div><div class=\"line\">   *</div><div class=\"line\">   * This is the same operation as &quot;SORT BY&quot; in SQL (Hive QL).</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\"></div><div class=\"line\">根据指定列对每个分区进行排序</div><div class=\"line\">  @scala.annotation.varargs</div><div class=\"line\">  def sortWithinPartitions(sortExprs: Column*): Dataset[T] = &#123;</div><div class=\"line\">    sortInternal(global = false, sortExprs)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new Dataset sorted by the specified column, all in ascending order.</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   // The following 3 are equivalent</div><div class=\"line\">   *   ds.sort(&quot;sortcol&quot;)</div><div class=\"line\">   *   ds.sort($&quot;sortcol&quot;)</div><div class=\"line\">   *   ds.sort($&quot;sortcol&quot;.asc)</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  @scala.annotation.varargs</div><div class=\"line\">  def sort(sortCol: String, sortCols: String*): Dataset[T] = &#123;</div><div class=\"line\">    sort((sortCol +: sortCols).map(apply) : _*)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new Dataset sorted by the given expressions. For example:</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   ds.sort($&quot;col1&quot;, $&quot;col2&quot;.desc)</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  @scala.annotation.varargs</div><div class=\"line\">  def sort(sortExprs: Column*): Dataset[T] = &#123;</div><div class=\"line\">    sortInternal(global = true, sortExprs)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new Dataset sorted by the given expressions.</div><div class=\"line\">   * This is an alias of the `sort` function.</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  @scala.annotation.varargs</div><div class=\"line\">  def orderBy(sortCol: String, sortCols: String*): Dataset[T] = sort(sortCol, sortCols : _*)</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new Dataset sorted by the given expressions.</div><div class=\"line\">   * This is an alias of the `sort` function.</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  @scala.annotation.varargs</div><div class=\"line\">  def orderBy(sortExprs: Column*): Dataset[T] = sort(sortExprs : _*)</div></pre></td></tr></table></figure>\n<p>提取指定的列<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Selects column based on the column name and return it as a [[Column]].</div><div class=\"line\"> *</div><div class=\"line\"> * @note The column name can also reference to a nested column like `a.b`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group untypedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def apply(colName: String): Column = col(colName)</div><div class=\"line\">/**</div><div class=\"line\"> * Selects column based on the column name and return it as a [[Column]].</div><div class=\"line\"> *</div><div class=\"line\"> * @note The column name can also reference to a nested column like `a.b`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group untypedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def col(colName: String): Column = colName match &#123;</div><div class=\"line\">  case &quot;*&quot; =&gt;</div><div class=\"line\">    Column(ResolvedStar(queryExecution.analyzed.output))</div><div class=\"line\">  case _ =&gt;</div><div class=\"line\">    val expr = resolve(colName)</div><div class=\"line\">    Column(expr)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h2 id=\"别名\"><a href=\"#别名\" class=\"headerlink\" title=\"别名\"></a>别名</h2><p>别名有给列取别名的也有给dataset取别名的这里是给当前dataset取别名<br> <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset with an alias set.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def as(alias: String): Dataset[T] = withTypedPlan &#123;</div><div class=\"line\">  SubqueryAlias(alias, logicalPlan, None)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * (Scala-specific) Returns a new Dataset with an alias set.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def as(alias: Symbol): Dataset[T] = as(alias.name)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset with an alias set. Same as `as`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def alias(alias: String): Dataset[T] = as(alias)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def alias(alias: Symbol): Dataset[T] = as(alias)</div></pre></td></tr></table></figure></p>\n<h2 id=\"查询\"><a href=\"#查询\" class=\"headerlink\" title=\"查询\"></a>查询</h2><p>查询有很多种接口使用的方式不太一样<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div><div class=\"line\">122</div><div class=\"line\">123</div><div class=\"line\">124</div><div class=\"line\">125</div><div class=\"line\">126</div><div class=\"line\">127</div><div class=\"line\">128</div><div class=\"line\">129</div><div class=\"line\">130</div><div class=\"line\">131</div><div class=\"line\">132</div><div class=\"line\">133</div><div class=\"line\">134</div><div class=\"line\">135</div><div class=\"line\">136</div><div class=\"line\">137</div><div class=\"line\">138</div><div class=\"line\">139</div><div class=\"line\">140</div><div class=\"line\">141</div><div class=\"line\">142</div><div class=\"line\">143</div><div class=\"line\">144</div><div class=\"line\">145</div><div class=\"line\">146</div><div class=\"line\">147</div><div class=\"line\">148</div><div class=\"line\">149</div><div class=\"line\">150</div><div class=\"line\">151</div></pre></td><td class=\"code\"><pre><div class=\"line\">  /**</div><div class=\"line\">   * Selects a set of column based expressions.</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   ds.select($&quot;colA&quot;, $&quot;colB&quot; + 1)</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  @scala.annotation.varargs</div><div class=\"line\">  def select(cols: Column*): DataFrame = withPlan &#123;</div><div class=\"line\">    Project(cols.map(_.named), logicalPlan)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Selects a set of columns. This is a variant of `select` that can only select</div><div class=\"line\">   * existing columns using column names (i.e. cannot construct expressions).</div><div class=\"line\">   *</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   // The following two are equivalent:</div><div class=\"line\">   *   ds.select(&quot;colA&quot;, &quot;colB&quot;)</div><div class=\"line\">   *   ds.select($&quot;colA&quot;, $&quot;colB&quot;)</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  @scala.annotation.varargs</div><div class=\"line\">  def select(col: String, cols: String*): DataFrame = select((col +: cols).map(Column(_)) : _*)</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Selects a set of SQL expressions. This is a variant of `select` that accepts</div><div class=\"line\">   * SQL expressions.</div><div class=\"line\">   *</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   // The following are equivalent:</div><div class=\"line\">   *   ds.selectExpr(&quot;colA&quot;, &quot;colB as newName&quot;, &quot;abs(colC)&quot;)</div><div class=\"line\">   *   ds.select(expr(&quot;colA&quot;), expr(&quot;colB as newName&quot;), expr(&quot;abs(colC)&quot;))</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">按照表达式来查询</div><div class=\"line\">  @scala.annotation.varargs</div><div class=\"line\">  def selectExpr(exprs: String*): DataFrame = &#123;</div><div class=\"line\">    select(exprs.map &#123; expr =&gt;</div><div class=\"line\">      Column(sparkSession.sessionState.sqlParser.parseExpression(expr))</div><div class=\"line\">    &#125;: _*)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * :: Experimental ::</div><div class=\"line\">   * Returns a new Dataset by computing the given [[Column]] expression for each element.</div><div class=\"line\">   *</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   val ds = Seq(1, 2, 3).toDS()</div><div class=\"line\">   *   val newDS = ds.select(expr(&quot;value + 1&quot;).as[Int])</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">  @Experimental</div><div class=\"line\">  @InterfaceStability.Evolving</div><div class=\"line\">  def select[U1](c1: TypedColumn[T, U1]): Dataset[U1] = &#123;</div><div class=\"line\">    implicit val encoder = c1.encoder</div><div class=\"line\">    val project = Project(c1.withInputType(exprEnc, logicalPlan.output).named :: Nil,</div><div class=\"line\">      logicalPlan)</div><div class=\"line\"></div><div class=\"line\">    if (encoder.flat) &#123;</div><div class=\"line\">      new Dataset[U1](sparkSession, project, encoder)</div><div class=\"line\">    &#125; else &#123;</div><div class=\"line\">      // Flattens inner fields of U1</div><div class=\"line\">      new Dataset[Tuple1[U1]](sparkSession, project, ExpressionEncoder.tuple(encoder)).map(_._1)</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Internal helper function for building typed selects that return tuples. For simplicity and</div><div class=\"line\">   * code reuse, we do this without the help of the type system and then use helper functions</div><div class=\"line\">   * that cast appropriately for the user facing interface.</div><div class=\"line\">   */</div><div class=\"line\">  ???这个查询怎么用</div><div class=\"line\">  protected def selectUntyped(columns: TypedColumn[_, _]*): Dataset[_] = &#123;</div><div class=\"line\">    val encoders = columns.map(_.encoder)</div><div class=\"line\">    val namedColumns =</div><div class=\"line\">      columns.map(_.withInputType(exprEnc, logicalPlan.output).named)</div><div class=\"line\">    val execution = new QueryExecution(sparkSession, Project(namedColumns, logicalPlan))</div><div class=\"line\">    new Dataset(sparkSession, execution, ExpressionEncoder.tuple(encoders))</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * :: Experimental ::</div><div class=\"line\">   * Returns a new Dataset by computing the given [[Column]] expressions for each element.</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">  @Experimental</div><div class=\"line\">  @InterfaceStability.Evolving</div><div class=\"line\">  def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)] =</div><div class=\"line\">    selectUntyped(c1, c2).asInstanceOf[Dataset[(U1, U2)]]</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * :: Experimental ::</div><div class=\"line\">   * Returns a new Dataset by computing the given [[Column]] expressions for each element.</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">  @Experimental</div><div class=\"line\">  @InterfaceStability.Evolving</div><div class=\"line\">  def select[U1, U2, U3](</div><div class=\"line\">      c1: TypedColumn[T, U1],</div><div class=\"line\">      c2: TypedColumn[T, U2],</div><div class=\"line\">      c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)] =</div><div class=\"line\">    selectUntyped(c1, c2, c3).asInstanceOf[Dataset[(U1, U2, U3)]]</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * :: Experimental ::</div><div class=\"line\">   * Returns a new Dataset by computing the given [[Column]] expressions for each element.</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">  @Experimental</div><div class=\"line\">  @InterfaceStability.Evolving</div><div class=\"line\">  def select[U1, U2, U3, U4](</div><div class=\"line\">      c1: TypedColumn[T, U1],</div><div class=\"line\">      c2: TypedColumn[T, U2],</div><div class=\"line\">      c3: TypedColumn[T, U3],</div><div class=\"line\">      c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)] =</div><div class=\"line\">    selectUntyped(c1, c2, c3, c4).asInstanceOf[Dataset[(U1, U2, U3, U4)]]</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * :: Experimental ::</div><div class=\"line\">   * Returns a new Dataset by computing the given [[Column]] expressions for each element.</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">  @Experimental</div><div class=\"line\">  @InterfaceStability.Evolving</div><div class=\"line\">  def select[U1, U2, U3, U4, U5](</div><div class=\"line\">      c1: TypedColumn[T, U1],</div><div class=\"line\">      c2: TypedColumn[T, U2],</div><div class=\"line\">      c3: TypedColumn[T, U3],</div><div class=\"line\">      c4: TypedColumn[T, U4],</div><div class=\"line\">      c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)] =</div><div class=\"line\">    selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[Dataset[(U1, U2, U3, U4, U5)]]</div></pre></td></tr></table></figure></p>\n<h2 id=\"过滤\"><a href=\"#过滤\" class=\"headerlink\" title=\"过滤\"></a>过滤</h2><p>过滤，这里的过滤和sql里面的where条件是相同的，查询满足一定条件的记录。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Filters rows using the given condition.</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   // The following are equivalent:</div><div class=\"line\"> *   peopleDs.filter($&quot;age&quot; &gt; 15)</div><div class=\"line\"> *   peopleDs.where($&quot;age&quot; &gt; 15)</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def filter(condition: Column): Dataset[T] = withTypedPlan &#123;</div><div class=\"line\">  Filter(condition.expr, logicalPlan)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Filters rows using the given SQL expression.</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   peopleDs.filter(&quot;age &gt; 15&quot;)</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def filter(conditionExpr: String): Dataset[T] = &#123;</div><div class=\"line\">  filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Filters rows using the given condition. This is an alias for `filter`.</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   // The following are equivalent:</div><div class=\"line\"> *   peopleDs.filter($&quot;age&quot; &gt; 15)</div><div class=\"line\"> *   peopleDs.where($&quot;age&quot; &gt; 15)</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def where(condition: Column): Dataset[T] = filter(condition)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Filters rows using the given SQL expression.</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   peopleDs.where(&quot;age &gt; 15&quot;)</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def where(conditionExpr: String): Dataset[T] = &#123;</div><div class=\"line\">  filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h2 id=\"分组查询\"><a href=\"#分组查询\" class=\"headerlink\" title=\"分组查询\"></a>分组查询</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Groups the Dataset using the specified columns, so we can run aggregation on them. See</div><div class=\"line\"> * [[RelationalGroupedDataset]] for all the available aggregate functions.</div><div class=\"line\"> *</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   // Compute the average for all numeric columns grouped by department.</div><div class=\"line\"> *   ds.groupBy($&quot;department&quot;).avg()</div><div class=\"line\"> *</div><div class=\"line\"> *   // Compute the max age and average salary, grouped by department and gender.</div><div class=\"line\"> *   ds.groupBy($&quot;department&quot;, $&quot;gender&quot;).agg(Map(</div><div class=\"line\"> *     &quot;salary&quot; -&gt; &quot;avg&quot;,</div><div class=\"line\"> *     &quot;age&quot; -&gt; &quot;max&quot;</div><div class=\"line\"> *   ))</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group untypedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@scala.annotation.varargs</div><div class=\"line\">def groupBy(cols: Column*): RelationalGroupedDataset = &#123;</div><div class=\"line\">  RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.GroupByType)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Create a multi-dimensional rollup for the current Dataset using the specified columns,</div><div class=\"line\"> * so we can run aggregation on them.</div><div class=\"line\"> * See [[RelationalGroupedDataset]] for all the available aggregate functions.</div><div class=\"line\"> *</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   // Compute the average for all numeric columns rolluped by department and group.</div><div class=\"line\"> *   ds.rollup($&quot;department&quot;, $&quot;group&quot;).avg()</div><div class=\"line\"> *</div><div class=\"line\"> *   // Compute the max age and average salary, rolluped by department and gender.</div><div class=\"line\"> *   ds.rollup($&quot;department&quot;, $&quot;gender&quot;).agg(Map(</div><div class=\"line\"> *     &quot;salary&quot; -&gt; &quot;avg&quot;,</div><div class=\"line\"> *     &quot;age&quot; -&gt; &quot;max&quot;</div><div class=\"line\"> *   ))</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group untypedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">数据归纳</div><div class=\"line\">@scala.annotation.varargs</div><div class=\"line\">def rollup(cols: Column*): RelationalGroupedDataset = &#123;</div><div class=\"line\">  RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.RollupType)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Create a multi-dimensional cube for the current Dataset using the specified columns,</div><div class=\"line\"> * so we can run aggregation on them.</div><div class=\"line\"> * See [[RelationalGroupedDataset]] for all the available aggregate functions.</div><div class=\"line\"> *</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   // Compute the average for all numeric columns cubed by department and group.</div><div class=\"line\"> *   ds.cube($&quot;department&quot;, $&quot;group&quot;).avg()</div><div class=\"line\"> *</div><div class=\"line\"> *   // Compute the max age and average salary, cubed by department and gender.</div><div class=\"line\"> *   ds.cube($&quot;department&quot;, $&quot;gender&quot;).agg(Map(</div><div class=\"line\"> *     &quot;salary&quot; -&gt; &quot;avg&quot;,</div><div class=\"line\"> *     &quot;age&quot; -&gt; &quot;max&quot;</div><div class=\"line\"> *   ))</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group untypedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@scala.annotation.varargs</div><div class=\"line\">def cube(cols: Column*): RelationalGroupedDataset = &#123;</div><div class=\"line\">  RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.CubeType)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Groups the Dataset using the specified columns, so that we can run aggregation on them.</div><div class=\"line\"> * See [[RelationalGroupedDataset]] for all the available aggregate functions.</div><div class=\"line\"> *</div><div class=\"line\"> * This is a variant of groupBy that can only group by existing columns using column names</div><div class=\"line\"> * (i.e. cannot construct expressions).</div><div class=\"line\"> *</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   // Compute the average for all numeric columns grouped by department.</div><div class=\"line\"> *   ds.groupBy(&quot;department&quot;).avg()</div><div class=\"line\"> *</div><div class=\"line\"> *   // Compute the max age and average salary, grouped by department and gender.</div><div class=\"line\"> *   ds.groupBy($&quot;department&quot;, $&quot;gender&quot;).agg(Map(</div><div class=\"line\"> *     &quot;salary&quot; -&gt; &quot;avg&quot;,</div><div class=\"line\"> *     &quot;age&quot; -&gt; &quot;max&quot;</div><div class=\"line\"> *   ))</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> * @group untypedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@scala.annotation.varargs</div><div class=\"line\">def groupBy(col1: String, cols: String*): RelationalGroupedDataset = &#123;</div><div class=\"line\">  val colNames: Seq[String] = col1 +: cols</div><div class=\"line\">  RelationalGroupedDataset(</div><div class=\"line\">    toDF(), colNames.map(colName =&gt; resolve(colName)), RelationalGroupedDataset.GroupByType)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"reduce操作\"><a href=\"#reduce操作\" class=\"headerlink\" title=\"reduce操作\"></a>reduce操作</h2>  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * (Scala-specific)</div><div class=\"line\"> * Reduces the elements of this Dataset using the specified binary function. The given `func`</div><div class=\"line\"> * must be commutative and associative or the result may be non-deterministic.</div><div class=\"line\"> *</div><div class=\"line\"> * @group action</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">合并操作</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def reduce(func: (T, T) =&gt; T): T = rdd.reduce(func)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * (Java-specific)</div><div class=\"line\"> * Reduces the elements of this Dataset using the specified binary function. The given `func`</div><div class=\"line\"> * must be commutative and associative or the result may be non-deterministic.</div><div class=\"line\"> *</div><div class=\"line\"> * @group action</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def reduce(func: ReduceFunction[T]): T = reduce(func.call(_, _))</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * (Scala-specific)</div><div class=\"line\"> * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">分组                                  </div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def groupByKey[K: Encoder](func: T =&gt; K): KeyValueGroupedDataset[K, T] = &#123;</div><div class=\"line\">  val inputPlan = logicalPlan</div><div class=\"line\">  val withGroupingKey = AppendColumns(func, inputPlan)</div><div class=\"line\">  val executed = sparkSession.sessionState.executePlan(withGroupingKey)</div><div class=\"line\"></div><div class=\"line\">  new KeyValueGroupedDataset(</div><div class=\"line\">    encoderFor[K],</div><div class=\"line\">    encoderFor[T],</div><div class=\"line\">    executed,</div><div class=\"line\">    inputPlan.output,</div><div class=\"line\">    withGroupingKey.newColumns)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * (Java-specific)</div><div class=\"line\"> * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T] =</div><div class=\"line\">  groupByKey(func.call(_))(encoder)</div></pre></td></tr></table></figure>\n<h2 id=\"数据钻取与聚合操作\"><a href=\"#数据钻取与聚合操作\" class=\"headerlink\" title=\"数据钻取与聚合操作\"></a>数据钻取与聚合操作</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div><div class=\"line\">122</div><div class=\"line\">123</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Create a multi-dimensional rollup for the current Dataset using the specified columns,</div><div class=\"line\"> * so we can run aggregation on them.</div><div class=\"line\"> * See [[RelationalGroupedDataset]] for all the available aggregate functions.</div><div class=\"line\"> *</div><div class=\"line\"> * This is a variant of rollup that can only group by existing columns using column names</div><div class=\"line\"> * (i.e. cannot construct expressions).</div><div class=\"line\"> *</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   // Compute the average for all numeric columns rolluped by department and group.</div><div class=\"line\"> *   ds.rollup(&quot;department&quot;, &quot;group&quot;).avg()</div><div class=\"line\"> *</div><div class=\"line\"> *   // Compute the max age and average salary, rolluped by department and gender.</div><div class=\"line\"> *   ds.rollup($&quot;department&quot;, $&quot;gender&quot;).agg(Map(</div><div class=\"line\"> *     &quot;salary&quot; -&gt; &quot;avg&quot;,</div><div class=\"line\"> *     &quot;age&quot; -&gt; &quot;max&quot;</div><div class=\"line\"> *   ))</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group untypedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@scala.annotation.varargs</div><div class=\"line\">def rollup(col1: String, cols: String*): RelationalGroupedDataset = &#123;</div><div class=\"line\">  val colNames: Seq[String] = col1 +: cols</div><div class=\"line\">  RelationalGroupedDataset(</div><div class=\"line\">    toDF(), colNames.map(colName =&gt; resolve(colName)), RelationalGroupedDataset.RollupType)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Create a multi-dimensional cube for the current Dataset using the specified columns,</div><div class=\"line\"> * so we can run aggregation on them.</div><div class=\"line\"> * See [[RelationalGroupedDataset]] for all the available aggregate functions.</div><div class=\"line\"> *</div><div class=\"line\"> * This is a variant of cube that can only group by existing columns using column names</div><div class=\"line\"> * (i.e. cannot construct expressions).</div><div class=\"line\"> *</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   // Compute the average for all numeric columns cubed by department and group.</div><div class=\"line\"> *   ds.cube(&quot;department&quot;, &quot;group&quot;).avg()</div><div class=\"line\"> *</div><div class=\"line\"> *   // Compute the max age and average salary, cubed by department and gender.</div><div class=\"line\"> *   ds.cube($&quot;department&quot;, $&quot;gender&quot;).agg(Map(</div><div class=\"line\"> *     &quot;salary&quot; -&gt; &quot;avg&quot;,</div><div class=\"line\"> *     &quot;age&quot; -&gt; &quot;max&quot;</div><div class=\"line\"> *   ))</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> * @group untypedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@scala.annotation.varargs</div><div class=\"line\">def cube(col1: String, cols: String*): RelationalGroupedDataset = &#123;</div><div class=\"line\">  val colNames: Seq[String] = col1 +: cols</div><div class=\"line\">  RelationalGroupedDataset(</div><div class=\"line\">    toDF(), colNames.map(colName =&gt; resolve(colName)), RelationalGroupedDataset.CubeType)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * (Scala-specific) Aggregates on the entire Dataset without groups.</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)</div><div class=\"line\"> *   ds.agg(&quot;age&quot; -&gt; &quot;max&quot;, &quot;salary&quot; -&gt; &quot;avg&quot;)</div><div class=\"line\"> *   ds.groupBy().agg(&quot;age&quot; -&gt; &quot;max&quot;, &quot;salary&quot; -&gt; &quot;avg&quot;)</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group untypedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = &#123;</div><div class=\"line\">  groupBy().agg(aggExpr, aggExprs : _*)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * (Scala-specific) Aggregates on the entire Dataset without groups.</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)</div><div class=\"line\"> *   ds.agg(Map(&quot;age&quot; -&gt; &quot;max&quot;, &quot;salary&quot; -&gt; &quot;avg&quot;))</div><div class=\"line\"> *   ds.groupBy().agg(Map(&quot;age&quot; -&gt; &quot;max&quot;, &quot;salary&quot; -&gt; &quot;avg&quot;))</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group untypedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def agg(exprs: Map[String, String]): DataFrame = groupBy().agg(exprs)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * (Java-specific) Aggregates on the entire Dataset without groups.</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)</div><div class=\"line\"> *   ds.agg(Map(&quot;age&quot; -&gt; &quot;max&quot;, &quot;salary&quot; -&gt; &quot;avg&quot;))</div><div class=\"line\"> *   ds.groupBy().agg(Map(&quot;age&quot; -&gt; &quot;max&quot;, &quot;salary&quot; -&gt; &quot;avg&quot;))</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group untypedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def agg(exprs: java.util.Map[String, String]): DataFrame = groupBy().agg(exprs)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Aggregates on the entire Dataset without groups.</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)</div><div class=\"line\"> *   ds.agg(max($&quot;age&quot;), avg($&quot;salary&quot;))</div><div class=\"line\"> *   ds.groupBy().agg(max($&quot;age&quot;), avg($&quot;salary&quot;))</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group untypedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@scala.annotation.varargs</div><div class=\"line\">def agg(expr: Column, exprs: Column*): DataFrame = groupBy().agg(expr, exprs : _*)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset by taking the first `n` rows. The difference between this function</div><div class=\"line\"> * and `head` is that `head` is an action and returns an array (by triggering query execution)</div><div class=\"line\"> * while `limit` returns a new Dataset.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def limit(n: Int): Dataset[T] = withTypedPlan &#123;</div><div class=\"line\">  Limit(Literal(n), logicalPlan)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"集合的交并补接口\"><a href=\"#集合的交并补接口\" class=\"headerlink\" title=\"集合的交并补接口\"></a>集合的交并补接口</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset containing union of rows in this Dataset and another Dataset.</div><div class=\"line\"> * This is equivalent to `UNION ALL` in SQL.</div><div class=\"line\"> *</div><div class=\"line\"> * To do a SQL-style set union (that does deduplication of elements), use this function followed</div><div class=\"line\"> * by a [[distinct]].</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@deprecated(&quot;use union()&quot;, &quot;2.0.0&quot;)</div><div class=\"line\">def unionAll(other: Dataset[T]): Dataset[T] = union(other)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset containing union of rows in this Dataset and another Dataset.</div><div class=\"line\"> * This is equivalent to `UNION ALL` in SQL.</div><div class=\"line\"> *</div><div class=\"line\"> * To do a SQL-style set union (that does deduplication of elements), use this function followed</div><div class=\"line\"> * by a [[distinct]].</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def union(other: Dataset[T]): Dataset[T] = withSetOperator &#123;</div><div class=\"line\">  // This breaks caching, but it&apos;s usually ok because it addresses a very specific use case:</div><div class=\"line\">  // using union to union many files or partitions.</div><div class=\"line\">  CombineUnions(Union(logicalPlan, other.logicalPlan))</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset containing rows only in both this Dataset and another Dataset.</div><div class=\"line\"> * This is equivalent to `INTERSECT` in SQL.</div><div class=\"line\"> *</div><div class=\"line\"> * @note Equality checking is performed directly on the encoded representation of the data</div><div class=\"line\"> * and thus is not affected by a custom `equals` function defined on `T`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def intersect(other: Dataset[T]): Dataset[T] = withSetOperator &#123;</div><div class=\"line\">  Intersect(logicalPlan, other.logicalPlan)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset containing rows in this Dataset but not in another Dataset.</div><div class=\"line\"> * This is equivalent to `EXCEPT` in SQL.</div><div class=\"line\"> *</div><div class=\"line\"> * @note Equality checking is performed directly on the encoded representation of the data</div><div class=\"line\"> * and thus is not affected by a custom `equals` function defined on `T`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">取补集</div><div class=\"line\">def except(other: Dataset[T]): Dataset[T] = withSetOperator &#123;</div><div class=\"line\">  Except(logicalPlan, other.logicalPlan)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"取样与切分\"><a href=\"#取样与切分\" class=\"headerlink\" title=\"取样与切分\"></a>取样与切分</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div><div class=\"line\">122</div><div class=\"line\">123</div><div class=\"line\">124</div><div class=\"line\">125</div><div class=\"line\">126</div><div class=\"line\">127</div><div class=\"line\">128</div><div class=\"line\">129</div><div class=\"line\">130</div><div class=\"line\">131</div><div class=\"line\">132</div><div class=\"line\">133</div><div class=\"line\">134</div><div class=\"line\">135</div><div class=\"line\">136</div><div class=\"line\">137</div><div class=\"line\">138</div><div class=\"line\">139</div><div class=\"line\">140</div><div class=\"line\">141</div><div class=\"line\">142</div><div class=\"line\">143</div><div class=\"line\">144</div><div class=\"line\">145</div><div class=\"line\">146</div><div class=\"line\">147</div><div class=\"line\">148</div><div class=\"line\">149</div><div class=\"line\">150</div><div class=\"line\">151</div><div class=\"line\">152</div><div class=\"line\">153</div><div class=\"line\">154</div><div class=\"line\">155</div><div class=\"line\">156</div><div class=\"line\">157</div><div class=\"line\">158</div><div class=\"line\">159</div><div class=\"line\">160</div><div class=\"line\">161</div><div class=\"line\">162</div><div class=\"line\">163</div><div class=\"line\">164</div><div class=\"line\">165</div><div class=\"line\">166</div><div class=\"line\">167</div><div class=\"line\">168</div><div class=\"line\">169</div><div class=\"line\">170</div><div class=\"line\">171</div><div class=\"line\">172</div><div class=\"line\">173</div><div class=\"line\">174</div><div class=\"line\">175</div><div class=\"line\">176</div><div class=\"line\">177</div><div class=\"line\">178</div><div class=\"line\">179</div><div class=\"line\">180</div><div class=\"line\">181</div><div class=\"line\">182</div><div class=\"line\">183</div><div class=\"line\">184</div><div class=\"line\">185</div><div class=\"line\">186</div><div class=\"line\">187</div><div class=\"line\">188</div><div class=\"line\">189</div><div class=\"line\">190</div><div class=\"line\">191</div><div class=\"line\">192</div><div class=\"line\">193</div><div class=\"line\">194</div><div class=\"line\">195</div><div class=\"line\">196</div><div class=\"line\">197</div><div class=\"line\">198</div><div class=\"line\">199</div><div class=\"line\">200</div><div class=\"line\">201</div><div class=\"line\">202</div><div class=\"line\">203</div><div class=\"line\">204</div><div class=\"line\">205</div><div class=\"line\">206</div><div class=\"line\">207</div><div class=\"line\">208</div><div class=\"line\">209</div><div class=\"line\">210</div><div class=\"line\">211</div><div class=\"line\">212</div><div class=\"line\">213</div><div class=\"line\">214</div><div class=\"line\">215</div><div class=\"line\">216</div><div class=\"line\">217</div><div class=\"line\">218</div><div class=\"line\">219</div><div class=\"line\">220</div><div class=\"line\">221</div><div class=\"line\">222</div><div class=\"line\">223</div><div class=\"line\">224</div><div class=\"line\">225</div><div class=\"line\">226</div><div class=\"line\">227</div><div class=\"line\">228</div><div class=\"line\">229</div><div class=\"line\">230</div><div class=\"line\">231</div><div class=\"line\">232</div><div class=\"line\">233</div><div class=\"line\">234</div><div class=\"line\">235</div><div class=\"line\">236</div><div class=\"line\">237</div><div class=\"line\">238</div><div class=\"line\">239</div><div class=\"line\">240</div><div class=\"line\">241</div><div class=\"line\">242</div><div class=\"line\">243</div><div class=\"line\">244</div><div class=\"line\">245</div><div class=\"line\">246</div><div class=\"line\">247</div><div class=\"line\">248</div><div class=\"line\">249</div><div class=\"line\">250</div><div class=\"line\">251</div><div class=\"line\">252</div><div class=\"line\">253</div><div class=\"line\">254</div><div class=\"line\">255</div><div class=\"line\">256</div><div class=\"line\">257</div><div class=\"line\">258</div><div class=\"line\">259</div><div class=\"line\">260</div><div class=\"line\">261</div><div class=\"line\">262</div><div class=\"line\">263</div><div class=\"line\">264</div><div class=\"line\">265</div><div class=\"line\">266</div><div class=\"line\">267</div><div class=\"line\">268</div><div class=\"line\">269</div><div class=\"line\">270</div><div class=\"line\">271</div><div class=\"line\">272</div><div class=\"line\">273</div><div class=\"line\">274</div><div class=\"line\">275</div><div class=\"line\">276</div><div class=\"line\">277</div><div class=\"line\">278</div><div class=\"line\">279</div><div class=\"line\">280</div><div class=\"line\">281</div><div class=\"line\">282</div><div class=\"line\">283</div><div class=\"line\">284</div><div class=\"line\">285</div><div class=\"line\">286</div><div class=\"line\">287</div><div class=\"line\">288</div><div class=\"line\">289</div><div class=\"line\">290</div><div class=\"line\">291</div><div class=\"line\">292</div><div class=\"line\">293</div><div class=\"line\">294</div><div class=\"line\">295</div><div class=\"line\">296</div><div class=\"line\">297</div><div class=\"line\">298</div><div class=\"line\">299</div><div class=\"line\">300</div><div class=\"line\">301</div><div class=\"line\">302</div><div class=\"line\">303</div><div class=\"line\">304</div><div class=\"line\">305</div><div class=\"line\">306</div><div class=\"line\">307</div><div class=\"line\">308</div><div class=\"line\">309</div><div class=\"line\">310</div><div class=\"line\">311</div></pre></td><td class=\"code\"><pre><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed.</div><div class=\"line\">   *</div><div class=\"line\">   * @param withReplacement Sample with replacement or not.</div><div class=\"line\">   * @param fraction Fraction of rows to generate.</div><div class=\"line\">   * @param seed Seed for sampling.</div><div class=\"line\">   *</div><div class=\"line\">   * @note This is NOT guaranteed to provide exactly the fraction of the count</div><div class=\"line\">   * of the given [[Dataset]].</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">  def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = &#123;</div><div class=\"line\">    require(fraction &gt;= 0,</div><div class=\"line\">      s&quot;Fraction must be nonnegative, but got $&#123;fraction&#125;&quot;)</div><div class=\"line\"></div><div class=\"line\">    withTypedPlan &#123;</div><div class=\"line\">      Sample(0.0, fraction, withReplacement, seed, logicalPlan)()</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed.</div><div class=\"line\">   *</div><div class=\"line\">   * @param withReplacement Sample with replacement or not.</div><div class=\"line\">   * @param fraction Fraction of rows to generate.</div><div class=\"line\">   *</div><div class=\"line\">   * @note This is NOT guaranteed to provide exactly the fraction of the total count</div><div class=\"line\">   * of the given [[Dataset]].</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">  随即取样</div><div class=\"line\">  def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = &#123;</div><div class=\"line\">    sample(withReplacement, fraction, Utils.random.nextLong)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Randomly splits this Dataset with the provided weights.</div><div class=\"line\">   *</div><div class=\"line\">   * @param weights weights for splits, will be normalized if they don&apos;t sum to 1.</div><div class=\"line\">   * @param seed Seed for sampling.</div><div class=\"line\">   *</div><div class=\"line\">   * For Java API, use [[randomSplitAsList]].</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  随即切分???</div><div class=\"line\">  def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]] = &#123;</div><div class=\"line\">    require(weights.forall(_ &gt;= 0),</div><div class=\"line\">      s&quot;Weights must be nonnegative, but got $&#123;weights.mkString(&quot;[&quot;, &quot;,&quot;, &quot;]&quot;)&#125;&quot;)</div><div class=\"line\">    require(weights.sum &gt; 0,</div><div class=\"line\">      s&quot;Sum of weights must be positive, but got $&#123;weights.mkString(&quot;[&quot;, &quot;,&quot;, &quot;]&quot;)&#125;&quot;)</div><div class=\"line\"></div><div class=\"line\">    // It is possible that the underlying dataframe doesn&apos;t guarantee the ordering of rows in its</div><div class=\"line\">    // constituent partitions each time a split is materialized which could result in</div><div class=\"line\">    // overlapping splits. To prevent this, we explicitly sort each input partition to make the</div><div class=\"line\">    // ordering deterministic.</div><div class=\"line\">    // MapType cannot be sorted.</div><div class=\"line\">    val sorted = Sort(logicalPlan.output.filterNot(_.dataType.isInstanceOf[MapType])</div><div class=\"line\">      .map(SortOrder(_, Ascending)), global = false, logicalPlan)</div><div class=\"line\">    val sum = weights.sum</div><div class=\"line\">    val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _)</div><div class=\"line\">    normalizedCumWeights.sliding(2).map &#123; x =&gt;</div><div class=\"line\">      new Dataset[T](</div><div class=\"line\">        sparkSession, Sample(x(0), x(1), withReplacement = false, seed, sorted)(), encoder)</div><div class=\"line\">    &#125;.toArray</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a Java list that contains randomly split Dataset with the provided weights.</div><div class=\"line\">   *</div><div class=\"line\">   * @param weights weights for splits, will be normalized if they don&apos;t sum to 1.</div><div class=\"line\">   * @param seed Seed for sampling.</div><div class=\"line\">   *</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  def randomSplitAsList(weights: Array[Double], seed: Long): java.util.List[Dataset[T]] = &#123;</div><div class=\"line\">    val values = randomSplit(weights, seed)</div><div class=\"line\">    java.util.Arrays.asList(values : _*)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Randomly splits this Dataset with the provided weights.</div><div class=\"line\">   *</div><div class=\"line\">   * @param weights weights for splits, will be normalized if they don&apos;t sum to 1.</div><div class=\"line\">   * @group typedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  def randomSplit(weights: Array[Double]): Array[Dataset[T]] = &#123;</div><div class=\"line\">    randomSplit(weights, Utils.random.nextLong)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Randomly splits this Dataset with the provided weights. Provided for the Python Api.</div><div class=\"line\">   *</div><div class=\"line\">   * @param weights weights for splits, will be normalized if they don&apos;t sum to 1.</div><div class=\"line\">   * @param seed Seed for sampling.</div><div class=\"line\">   */</div><div class=\"line\">  private[spark] def randomSplit(weights: List[Double], seed: Long): Array[Dataset[T]] = &#123;</div><div class=\"line\">    randomSplit(weights.toArray, seed)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more</div><div class=\"line\">   * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of</div><div class=\"line\">   * the input row are implicitly joined with each row that is output by the function.</div><div class=\"line\">   *</div><div class=\"line\">   * Given that this is deprecated, as an alternative, you can explode columns either using</div><div class=\"line\">   * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count</div><div class=\"line\">   * the number of books that contain a given word:</div><div class=\"line\">   *</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   case class Book(title: String, words: String)</div><div class=\"line\">   *   val ds: Dataset[Book]</div><div class=\"line\">   *</div><div class=\"line\">   *   val allWords = ds.select(&apos;title, explode(split(&apos;words, &quot; &quot;)).as(&quot;word&quot;))</div><div class=\"line\">   *</div><div class=\"line\">   *   val bookCountPerWord = allWords.groupBy(&quot;word&quot;).agg(countDistinct(&quot;title&quot;))</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * Using `flatMap()` this can similarly be exploded as:</div><div class=\"line\">   *</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   ds.flatMap(_.words.split(&quot; &quot;))</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  将字段再处理</div><div class=\"line\">  @deprecated(&quot;use flatMap() or select() with functions.explode() instead&quot;, &quot;2.0.0&quot;)</div><div class=\"line\">  def explode[A &lt;: Product : TypeTag](input: Column*)(f: Row =&gt; TraversableOnce[A]): DataFrame = &#123;</div><div class=\"line\">    val elementSchema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]</div><div class=\"line\"></div><div class=\"line\">    val convert = CatalystTypeConverters.createToCatalystConverter(elementSchema)</div><div class=\"line\"></div><div class=\"line\">    val rowFunction =</div><div class=\"line\">      f.andThen(_.map(convert(_).asInstanceOf[InternalRow]))</div><div class=\"line\">    val generator = UserDefinedGenerator(elementSchema, rowFunction, input.map(_.expr))</div><div class=\"line\"></div><div class=\"line\">    withPlan &#123;</div><div class=\"line\">      Generate(generator, join = true, outer = false,</div><div class=\"line\">        qualifier = None, generatorOutput = Nil, logicalPlan)</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero</div><div class=\"line\">   * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All</div><div class=\"line\">   * columns of the input row are implicitly joined with each value that is output by the function.</div><div class=\"line\">   *</div><div class=\"line\">   * Given that this is deprecated, as an alternative, you can explode columns either using</div><div class=\"line\">   * `functions.explode()`:</div><div class=\"line\">   *</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   ds.select(explode(split(&apos;words, &quot; &quot;)).as(&quot;word&quot;))</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * or `flatMap()`:</div><div class=\"line\">   *</div><div class=\"line\">   * &#123;&#123;&#123;</div><div class=\"line\">   *   ds.flatMap(_.words.split(&quot; &quot;))</div><div class=\"line\">   * &#125;&#125;&#125;</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  @deprecated(&quot;use flatMap() or select() with functions.explode() instead&quot;, &quot;2.0.0&quot;)</div><div class=\"line\">  def explode[A, B : TypeTag](inputColumn: String, outputColumn: String)(f: A =&gt; TraversableOnce[B])</div><div class=\"line\">    : DataFrame = &#123;</div><div class=\"line\">    val dataType = ScalaReflection.schemaFor[B].dataType</div><div class=\"line\">    val attributes = AttributeReference(outputColumn, dataType)() :: Nil</div><div class=\"line\">    // TODO handle the metadata?</div><div class=\"line\">    val elementSchema = attributes.toStructType</div><div class=\"line\"></div><div class=\"line\">    def rowFunction(row: Row): TraversableOnce[InternalRow] = &#123;</div><div class=\"line\">      val convert = CatalystTypeConverters.createToCatalystConverter(dataType)</div><div class=\"line\">      f(row(0).asInstanceOf[A]).map(o =&gt; InternalRow(convert(o)))</div><div class=\"line\">    &#125;</div><div class=\"line\">    val generator = UserDefinedGenerator(elementSchema, rowFunction, apply(inputColumn).expr :: Nil)</div><div class=\"line\"></div><div class=\"line\">    withPlan &#123;</div><div class=\"line\">      Generate(generator, join = true, outer = false,</div><div class=\"line\">        qualifier = None, generatorOutput = Nil, logicalPlan)</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">## 列操作</div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new Dataset by adding a column or replacing the existing column that has</div><div class=\"line\">   * the same name.</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  列操作</div><div class=\"line\">  def withColumn(colName: String, col: Column): DataFrame = &#123;</div><div class=\"line\">    val resolver = sparkSession.sessionState.analyzer.resolver</div><div class=\"line\">    val output = queryExecution.analyzed.output</div><div class=\"line\">    val shouldReplace = output.exists(f =&gt; resolver(f.name, colName))</div><div class=\"line\">    if (shouldReplace) &#123;</div><div class=\"line\">      val columns = output.map &#123; field =&gt;</div><div class=\"line\">        if (resolver(field.name, colName)) &#123;</div><div class=\"line\">          col.as(colName)</div><div class=\"line\">        &#125; else &#123;</div><div class=\"line\">          Column(field)</div><div class=\"line\">        &#125;</div><div class=\"line\">      &#125;</div><div class=\"line\">      select(columns : _*)</div><div class=\"line\">    &#125; else &#123;</div><div class=\"line\">      select(Column(&quot;*&quot;), col.as(colName))</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new Dataset by adding a column with metadata.</div><div class=\"line\">   */</div><div class=\"line\">  private[spark] def withColumn(colName: String, col: Column, metadata: Metadata): DataFrame = &#123;</div><div class=\"line\">    withColumn(colName, col.as(colName, metadata))</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new Dataset with a column renamed.</div><div class=\"line\">   * This is a no-op if schema doesn&apos;t contain existingName.</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  def withColumnRenamed(existingName: String, newName: String): DataFrame = &#123;</div><div class=\"line\">    val resolver = sparkSession.sessionState.analyzer.resolver</div><div class=\"line\">    val output = queryExecution.analyzed.output</div><div class=\"line\">    val shouldRename = output.exists(f =&gt; resolver(f.name, existingName))</div><div class=\"line\">    if (shouldRename) &#123;</div><div class=\"line\">      val columns = output.map &#123; col =&gt;</div><div class=\"line\">        if (resolver(col.name, existingName)) &#123;</div><div class=\"line\">          Column(col).as(newName)</div><div class=\"line\">        &#125; else &#123;</div><div class=\"line\">          Column(col)</div><div class=\"line\">        &#125;</div><div class=\"line\">      &#125;</div><div class=\"line\">      select(columns : _*)</div><div class=\"line\">    &#125; else &#123;</div><div class=\"line\">      toDF()</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new Dataset with a column dropped. This is a no-op if schema doesn&apos;t contain</div><div class=\"line\">   * column name.</div><div class=\"line\">   *</div><div class=\"line\">   * This method can only be used to drop top level columns. the colName string is treated</div><div class=\"line\">   * literally without further interpretation.</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  删除指定列</div><div class=\"line\">  def drop(colName: String): DataFrame = &#123;</div><div class=\"line\">    drop(Seq(colName) : _*)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new Dataset with columns dropped.</div><div class=\"line\">   * This is a no-op if schema doesn&apos;t contain column name(s).</div><div class=\"line\">   *</div><div class=\"line\">   * This method can only be used to drop top level columns. the colName string is treated literally</div><div class=\"line\">   * without further interpretation.</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  @scala.annotation.varargs</div><div class=\"line\">  def drop(colNames: String*): DataFrame = &#123;</div><div class=\"line\">    val resolver = sparkSession.sessionState.analyzer.resolver</div><div class=\"line\">    val allColumns = queryExecution.analyzed.output</div><div class=\"line\">    val remainingCols = allColumns.filter &#123; attribute =&gt;</div><div class=\"line\">      colNames.forall(n =&gt; !resolver(attribute.name, n))</div><div class=\"line\">    &#125;.map(attribute =&gt; Column(attribute))</div><div class=\"line\">    if (remainingCols.size == allColumns.size) &#123;</div><div class=\"line\">      toDF()</div><div class=\"line\">    &#125; else &#123;</div><div class=\"line\">      this.select(remainingCols: _*)</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a new Dataset with a column dropped.</div><div class=\"line\">   * This version of drop accepts a [[Column]] rather than a name.</div><div class=\"line\">   * This is a no-op if the Dataset doesn&apos;t have a column</div><div class=\"line\">   * with an equivalent expression.</div><div class=\"line\">   *</div><div class=\"line\">   * @group untypedrel</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  def drop(col: Column): DataFrame = &#123;</div><div class=\"line\">    val expression = col match &#123;</div><div class=\"line\">      case Column(u: UnresolvedAttribute) =&gt;</div><div class=\"line\">        queryExecution.analyzed.resolveQuoted(</div><div class=\"line\">          u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u)</div><div class=\"line\">      case Column(expr: Expression) =&gt; expr</div><div class=\"line\">    &#125;</div><div class=\"line\">    val attrs = this.logicalPlan.output</div><div class=\"line\">    val colsAfterDrop = attrs.filter &#123; attr =&gt;</div><div class=\"line\">      attr != expression</div><div class=\"line\">    &#125;.map(attr =&gt; Column(attr))</div><div class=\"line\">    select(colsAfterDrop : _*)</div><div class=\"line\">  &#125;</div></pre></td></tr></table></figure>\n<h2 id=\"去重\"><a href=\"#去重\" class=\"headerlink\" title=\"去重\"></a>去重</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset that contains only the unique rows from this Dataset.</div><div class=\"line\"> * This is an alias for `distinct`.</div><div class=\"line\"> *</div><div class=\"line\"> * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it</div><div class=\"line\"> * will keep all data across triggers as intermediate state to drop duplicates rows. You can use</div><div class=\"line\"> * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit</div><div class=\"line\"> * the state. In addition, too late data older than watermark will be dropped to avoid any</div><div class=\"line\"> * possibility of duplicates.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">删除重复的行</div><div class=\"line\">def dropDuplicates(): Dataset[T] = dropDuplicates(this.columns)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only</div><div class=\"line\"> * the subset of columns.</div><div class=\"line\"> *</div><div class=\"line\"> * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it</div><div class=\"line\"> * will keep all data across triggers as intermediate state to drop duplicates rows. You can use</div><div class=\"line\"> * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit</div><div class=\"line\"> * the state. In addition, too late data older than watermark will be dropped to avoid any</div><div class=\"line\"> * possibility of duplicates.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def dropDuplicates(colNames: Seq[String]): Dataset[T] = withTypedPlan &#123;</div><div class=\"line\">  val resolver = sparkSession.sessionState.analyzer.resolver</div><div class=\"line\">  val allColumns = queryExecution.analyzed.output</div><div class=\"line\">  val groupCols = colNames.toSet.toSeq.flatMap &#123; (colName: String) =&gt;</div><div class=\"line\">    // It is possibly there are more than one columns with the same name,</div><div class=\"line\">    // so we call filter instead of find.</div><div class=\"line\">    val cols = allColumns.filter(col =&gt; resolver(col.name, colName))</div><div class=\"line\">    if (cols.isEmpty) &#123;</div><div class=\"line\">      throw new AnalysisException(</div><div class=\"line\">        s&quot;&quot;&quot;Cannot resolve column name &quot;$colName&quot; among ($&#123;schema.fieldNames.mkString(&quot;, &quot;)&#125;)&quot;&quot;&quot;)</div><div class=\"line\">    &#125;</div><div class=\"line\">    cols</div><div class=\"line\">  &#125;</div><div class=\"line\">  Deduplicate(groupCols, logicalPlan, isStreaming)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset with duplicate rows removed, considering only</div><div class=\"line\"> * the subset of columns.</div><div class=\"line\"> *</div><div class=\"line\"> * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it</div><div class=\"line\"> * will keep all data across triggers as intermediate state to drop duplicates rows. You can use</div><div class=\"line\"> * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit</div><div class=\"line\"> * the state. In addition, too late data older than watermark will be dropped to avoid any</div><div class=\"line\"> * possibility of duplicates.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def dropDuplicates(colNames: Array[String]): Dataset[T] = dropDuplicates(colNames.toSeq)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a new [[Dataset]] with duplicate rows removed, considering only</div><div class=\"line\"> * the subset of columns.</div><div class=\"line\"> *</div><div class=\"line\"> * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it</div><div class=\"line\"> * will keep all data across triggers as intermediate state to drop duplicates rows. You can use</div><div class=\"line\"> * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit</div><div class=\"line\"> * the state. In addition, too late data older than watermark will be dropped to avoid any</div><div class=\"line\"> * possibility of duplicates.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@scala.annotation.varargs</div><div class=\"line\">def dropDuplicates(col1: String, cols: String*): Dataset[T] = &#123;</div><div class=\"line\">  val colNames: Seq[String] = col1 +: cols</div><div class=\"line\">  dropDuplicates(colNames)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"统计指定的列\"><a href=\"#统计指定的列\" class=\"headerlink\" title=\"统计指定的列\"></a>统计指定的列</h2><pre><code><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div><div class=\"line\">122</div><div class=\"line\">123</div><div class=\"line\">124</div><div class=\"line\">125</div><div class=\"line\">126</div><div class=\"line\">127</div><div class=\"line\">128</div><div class=\"line\">129</div><div class=\"line\">130</div><div class=\"line\">131</div><div class=\"line\">132</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Computes statistics for numeric and string columns, including count, mean, stddev, min, and</div><div class=\"line\"> * max. If no columns are given, this function computes statistics for all numerical or string</div><div class=\"line\"> * columns.</div><div class=\"line\"> *</div><div class=\"line\"> * This function is meant for exploratory data analysis, as we make no guarantee about the</div><div class=\"line\"> * backward compatibility of the schema of the resulting Dataset. If you want to</div><div class=\"line\"> * programmatically compute summary statistics, use the `agg` function instead.</div><div class=\"line\"> *</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   ds.describe(&quot;age&quot;, &quot;height&quot;).show()</div><div class=\"line\"> *</div><div class=\"line\"> *   // output:</div><div class=\"line\"> *   // summary age   height</div><div class=\"line\"> *   // count   10.0  10.0</div><div class=\"line\"> *   // mean    53.3  178.05</div><div class=\"line\"> *   // stddev  11.6  15.7</div><div class=\"line\"> *   // min     18.0  163.0</div><div class=\"line\"> *   // max     92.0  192.0</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group action</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">获得指定列的描述性统计量</div><div class=\"line\">@scala.annotation.varargs</div><div class=\"line\">def describe(cols: String*): DataFrame = withPlan &#123;</div><div class=\"line\"></div><div class=\"line\">  // The list of summary statistics to compute, in the form of expressions.</div><div class=\"line\">  val statistics = List[(String, Expression =&gt; Expression)](</div><div class=\"line\">    &quot;count&quot; -&gt; ((child: Expression) =&gt; Count(child).toAggregateExpression()),</div><div class=\"line\">    &quot;mean&quot; -&gt; ((child: Expression) =&gt; Average(child).toAggregateExpression()),</div><div class=\"line\">    &quot;stddev&quot; -&gt; ((child: Expression) =&gt; StddevSamp(child).toAggregateExpression()),</div><div class=\"line\">    &quot;min&quot; -&gt; ((child: Expression) =&gt; Min(child).toAggregateExpression()),</div><div class=\"line\">    &quot;max&quot; -&gt; ((child: Expression) =&gt; Max(child).toAggregateExpression()))</div><div class=\"line\"></div><div class=\"line\">  val outputCols =</div><div class=\"line\">    (if (cols.isEmpty) aggregatableColumns.map(usePrettyExpression(_).sql) else cols).toList</div><div class=\"line\"></div><div class=\"line\">  val ret: Seq[Row] = if (outputCols.nonEmpty) &#123;</div><div class=\"line\">    val aggExprs = statistics.flatMap &#123; case (_, colToAgg) =&gt;</div><div class=\"line\">      outputCols.map(c =&gt; Column(Cast(colToAgg(Column(c).expr), StringType)).as(c))</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    val row = groupBy().agg(aggExprs.head, aggExprs.tail: _*).head().toSeq</div><div class=\"line\"></div><div class=\"line\">    // Pivot the data so each summary is one row</div><div class=\"line\">    row.grouped(outputCols.size).toSeq.zip(statistics).map &#123; case (aggregation, (statistic, _)) =&gt;</div><div class=\"line\">      Row(statistic :: aggregation.toList: _*)</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125; else &#123;</div><div class=\"line\">    // If there are no output columns, just output a single column that contains the stats.</div><div class=\"line\">    statistics.map &#123; case (name, _) =&gt; Row(name) &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  // All columns are string type</div><div class=\"line\">  val schema = StructType(</div><div class=\"line\">    StructField(&quot;summary&quot;, StringType) :: outputCols.map(StructField(_, StringType))).toAttributes</div><div class=\"line\">  // `toArray` forces materialization to make the seq serializable</div><div class=\"line\">  LocalRelation.fromExternalRows(schema, ret.toArray.toSeq)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns the first `n` rows.</div><div class=\"line\"> *</div><div class=\"line\"> * @note this method should only be used if the resulting array is expected to be small, as</div><div class=\"line\"> * all the data is loaded into the driver&apos;s memory.</div><div class=\"line\"> *</div><div class=\"line\"> * @group action</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">取得前n行数据</div><div class=\"line\">def head(n: Int): Array[T] = withAction(&quot;head&quot;, limit(n).queryExecution)(collectFromPlan)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns the first row.</div><div class=\"line\"> * @group action</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def head(): T = head(1).head</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns the first row. Alias for head().</div><div class=\"line\"> * @group action</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def first(): T = head()</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Concise syntax for chaining custom transformations.</div><div class=\"line\"> * &#123;&#123;&#123;</div><div class=\"line\"> *   def featurize(ds: Dataset[T]): Dataset[U] = ...</div><div class=\"line\"> *</div><div class=\"line\"> *   ds</div><div class=\"line\"> *     .transform(featurize)</div><div class=\"line\"> *     .transform(...)</div><div class=\"line\"> * &#125;&#125;&#125;</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">转换？？</div><div class=\"line\">def transform[U](t: Dataset[T] =&gt; Dataset[U]): Dataset[U] = t(this)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * (Scala-specific)</div><div class=\"line\"> * Returns a new Dataset that only contains elements where `func` returns `true`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">过滤</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def filter(func: T =&gt; Boolean): Dataset[T] = &#123;</div><div class=\"line\">  withTypedPlan(TypedFilter(func, logicalPlan))</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * (Java-specific)</div><div class=\"line\"> * Returns a new Dataset that only contains elements where `func` returns `true`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def filter(func: FilterFunction[T]): Dataset[T] = &#123;</div><div class=\"line\">  withTypedPlan(TypedFilter(func, logicalPlan))</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n</code></pre><h2 id=\"数据的转换\"><a href=\"#数据的转换\" class=\"headerlink\" title=\"数据的转换\"></a>数据的转换</h2>  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div><div class=\"line\">122</div><div class=\"line\">123</div><div class=\"line\">124</div><div class=\"line\">125</div><div class=\"line\">126</div><div class=\"line\">127</div><div class=\"line\">128</div><div class=\"line\">129</div><div class=\"line\">130</div><div class=\"line\">131</div><div class=\"line\">132</div><div class=\"line\">133</div><div class=\"line\">134</div><div class=\"line\">135</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * (Scala-specific)</div><div class=\"line\"> * Returns a new Dataset that contains the result of applying `func` to each element.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">映射操作</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def map[U : Encoder](func: T =&gt; U): Dataset[U] = withTypedPlan &#123;</div><div class=\"line\">  MapElements[T, U](func, logicalPlan)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * (Java-specific)</div><div class=\"line\"> * Returns a new Dataset that contains the result of applying `func` to each element.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U] = &#123;</div><div class=\"line\">  implicit val uEnc = encoder</div><div class=\"line\">  withTypedPlan(MapElements[T, U](func, logicalPlan))</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * (Scala-specific)</div><div class=\"line\"> * Returns a new Dataset that contains the result of applying `func` to each partition.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def mapPartitions[U : Encoder](func: Iterator[T] =&gt; Iterator[U]): Dataset[U] = &#123;</div><div class=\"line\">  new Dataset[U](</div><div class=\"line\">    sparkSession,</div><div class=\"line\">    MapPartitions[T, U](func, logicalPlan),</div><div class=\"line\">    implicitly[Encoder[U]])</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * (Java-specific)</div><div class=\"line\"> * Returns a new Dataset that contains the result of applying `f` to each partition.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U] = &#123;</div><div class=\"line\">  val func: (Iterator[T]) =&gt; Iterator[U] = x =&gt; f.call(x.asJava).asScala</div><div class=\"line\">  mapPartitions(func)(encoder)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a new `DataFrame` that contains the result of applying a serialized R function</div><div class=\"line\"> * `func` to each partition.</div><div class=\"line\"> */</div><div class=\"line\">private[sql] def mapPartitionsInR(</div><div class=\"line\">    func: Array[Byte],</div><div class=\"line\">    packageNames: Array[Byte],</div><div class=\"line\">    broadcastVars: Array[Broadcast[Object]],</div><div class=\"line\">    schema: StructType): DataFrame = &#123;</div><div class=\"line\">  val rowEncoder = encoder.asInstanceOf[ExpressionEncoder[Row]]</div><div class=\"line\">  Dataset.ofRows(</div><div class=\"line\">    sparkSession,</div><div class=\"line\">    MapPartitionsInR(func, packageNames, broadcastVars, schema, rowEncoder, logicalPlan))</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * (Scala-specific)</div><div class=\"line\"> * Returns a new Dataset by first applying a function to all elements of this Dataset,</div><div class=\"line\"> * and then flattening the results.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def flatMap[U : Encoder](func: T =&gt; TraversableOnce[U]): Dataset[U] =</div><div class=\"line\">  mapPartitions(_.flatMap(func))</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * (Java-specific)</div><div class=\"line\"> * Returns a new Dataset by first applying a function to all elements of this Dataset,</div><div class=\"line\"> * and then flattening the results.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U] = &#123;</div><div class=\"line\">  val func: (T) =&gt; Iterator[U] = x =&gt; f.call(x).asScala</div><div class=\"line\">  flatMap(func)(encoder)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Applies a function `f` to all rows.</div><div class=\"line\"> *</div><div class=\"line\"> * @group action</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def foreach(f: T =&gt; Unit): Unit = withNewExecutionId &#123;</div><div class=\"line\">  rdd.foreach(f)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * (Java-specific)</div><div class=\"line\"> * Runs `func` on each element of this Dataset.</div><div class=\"line\"> *</div><div class=\"line\"> * @group action</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def foreach(func: ForeachFunction[T]): Unit = foreach(func.call(_))</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Applies a function `f` to each partition of this Dataset.</div><div class=\"line\"> *</div><div class=\"line\"> * @group action</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def foreachPartition(f: Iterator[T] =&gt; Unit): Unit = withNewExecutionId &#123;</div><div class=\"line\">  rdd.foreachPartition(f)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"数据的提取与聚合\"><a href=\"#数据的提取与聚合\" class=\"headerlink\" title=\"数据的提取与聚合\"></a>数据的提取与聚合</h2>  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div></pre></td><td class=\"code\"><pre><div class=\"line\">  /**</div><div class=\"line\">   * (Java-specific)</div><div class=\"line\">   * Runs `func` on each partition of this Dataset.</div><div class=\"line\">   *</div><div class=\"line\">   * @group action</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">  def foreachPartition(func: ForeachPartitionFunction[T]): Unit =</div><div class=\"line\">    foreachPartition(it =&gt; func.call(it.asJava))</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns the first `n` rows in the Dataset.</div><div class=\"line\">   *</div><div class=\"line\">   * Running take requires moving data into the application&apos;s driver process, and doing so with</div><div class=\"line\">   * a very large `n` can crash the driver process with OutOfMemoryError.</div><div class=\"line\">   *</div><div class=\"line\">   * @group action</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">  def take(n: Int): Array[T] = head(n)</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns the first `n` rows in the Dataset as a list.</div><div class=\"line\">   *</div><div class=\"line\">   * Running take requires moving data into the application&apos;s driver process, and doing so with</div><div class=\"line\">   * a very large `n` can crash the driver process with OutOfMemoryError.</div><div class=\"line\">   *</div><div class=\"line\">   * @group action</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">    获取数据成一个列表</div><div class=\"line\">  def takeAsList(n: Int): java.util.List[T] = java.util.Arrays.asList(take(n) : _*)</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns an array that contains all rows in this Dataset.</div><div class=\"line\">   *</div><div class=\"line\">   * Running collect requires moving all the data into the application&apos;s driver process, and</div><div class=\"line\">   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.</div><div class=\"line\">   *</div><div class=\"line\">   * For Java API, use [[collectAsList]].</div><div class=\"line\">   *</div><div class=\"line\">   * @group action</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\"></div><div class=\"line\">统计数据</div><div class=\"line\">  def collect(): Array[T] = withAction(&quot;collect&quot;, queryExecution)(collectFromPlan)</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns a Java list that contains all rows in this Dataset.</div><div class=\"line\">   *</div><div class=\"line\">   * Running collect requires moving all the data into the application&apos;s driver process, and</div><div class=\"line\">   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.</div><div class=\"line\">   *</div><div class=\"line\">   * @group action</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">  def collectAsList(): java.util.List[T] = withAction(&quot;collectAsList&quot;, queryExecution) &#123; plan =&gt;</div><div class=\"line\">    val values = collectFromPlan(plan)</div><div class=\"line\">    java.util.Arrays.asList(values : _*)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Return an iterator that contains all rows in this Dataset.</div><div class=\"line\">   *</div><div class=\"line\">   * The iterator will consume as much memory as the largest partition in this Dataset.</div><div class=\"line\">   *</div><div class=\"line\">   * @note this results in multiple Spark jobs, and if the input Dataset is the result</div><div class=\"line\">   * of a wide transformation (e.g. join with different partitioners), to avoid</div><div class=\"line\">   * recomputing the input Dataset should be cached first.</div><div class=\"line\">   *</div><div class=\"line\">   * @group action</div><div class=\"line\">   * @since 2.0.0</div><div class=\"line\">   */</div><div class=\"line\">  def toLocalIterator(): java.util.Iterator[T] = &#123;</div><div class=\"line\">    withAction(&quot;toLocalIterator&quot;, queryExecution) &#123; plan =&gt;</div><div class=\"line\">      plan.executeToIterator().map(boundEnc.fromRow).asJava</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  /**</div><div class=\"line\">   * Returns the number of rows in the Dataset.</div><div class=\"line\">   * @group action</div><div class=\"line\">   * @since 1.6.0</div><div class=\"line\">   */</div><div class=\"line\">  统计行</div><div class=\"line\">  def count(): Long = withAction(&quot;count&quot;, groupBy().count().queryExecution) &#123; plan =&gt;</div><div class=\"line\">    plan.executeCollect().head.getLong(0)</div><div class=\"line\">  &#125;</div></pre></td></tr></table></figure>\n<h2 id=\"分区\"><a href=\"#分区\" class=\"headerlink\" title=\"分区\"></a>分区</h2><pre><code><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset that has exactly `numPartitions` partitions.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">分区</div><div class=\"line\">def repartition(numPartitions: Int): Dataset[T] = withTypedPlan &#123;</div><div class=\"line\">  Repartition(numPartitions, shuffle = true, logicalPlan)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset partitioned by the given partitioning expressions into</div><div class=\"line\"> * `numPartitions`. The resulting Dataset is hash partitioned.</div><div class=\"line\"> *</div><div class=\"line\"> * This is the same operation as &quot;DISTRIBUTE BY&quot; in SQL (Hive QL).</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@scala.annotation.varargs</div><div class=\"line\">def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T] = withTypedPlan &#123;</div><div class=\"line\">  RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, numPartitions)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset partitioned by the given partitioning expressions, using</div><div class=\"line\"> * `spark.sql.shuffle.partitions` as number of partitions.</div><div class=\"line\"> * The resulting Dataset is hash partitioned.</div><div class=\"line\"> *</div><div class=\"line\"> * This is the same operation as &quot;DISTRIBUTE BY&quot; in SQL (Hive QL).</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@scala.annotation.varargs</div><div class=\"line\">def repartition(partitionExprs: Column*): Dataset[T] = withTypedPlan &#123;</div><div class=\"line\">  RepartitionByExpression(</div><div class=\"line\">    partitionExprs.map(_.expr), logicalPlan, sparkSession.sessionState.conf.numShufflePartitions)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset that has exactly `numPartitions` partitions.</div><div class=\"line\"> * Similar to coalesce defined on an `RDD`, this operation results in a narrow dependency, e.g.</div><div class=\"line\"> * if you go from 1000 partitions to 100 partitions, there will not be a shuffle, instead each of</div><div class=\"line\"> * the 100 new partitions will claim 10 of the current partitions.  If a larger number of</div><div class=\"line\"> * partitions is requested, it will stay at the current number of partitions.</div><div class=\"line\"> *</div><div class=\"line\"> * However, if you&apos;re doing a drastic coalesce, e.g. to numPartitions = 1,</div><div class=\"line\"> * this may result in your computation taking place on fewer nodes than</div><div class=\"line\"> * you like (e.g. one node in the case of numPartitions = 1). To avoid this,</div><div class=\"line\"> * you can call repartition. This will add a shuffle step, but means the</div><div class=\"line\"> * current upstream partitions will be executed in parallel (per whatever</div><div class=\"line\"> * the current partitioning is).</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def coalesce(numPartitions: Int): Dataset[T] = withTypedPlan &#123;</div><div class=\"line\">  Repartition(numPartitions, shuffle = false, logicalPlan)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns a new Dataset that contains only the unique rows from this Dataset.</div><div class=\"line\"> * This is an alias for `dropDuplicates`.</div><div class=\"line\"> *</div><div class=\"line\"> * @note Equality checking is performed directly on the encoded representation of the data</div><div class=\"line\"> * and thus is not affected by a custom `equals` function defined on `T`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group typedrel</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def distinct(): Dataset[T] = dropDuplicates()</div></pre></td></tr></table></figure>\n</code></pre><h2 id=\"数据的持久化\"><a href=\"#数据的持久化\" class=\"headerlink\" title=\"数据的持久化\"></a>数据的持久化</h2><p>  数据的持久化和缓存策略，一般我们操作rdd都是延迟计算，但是当我们多次重复使用一个rdd的时候可以选择将其缓存而不是每次进行一个计算，可以提高效率。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">                持久化</div><div class=\"line\">def persist(): this.type = &#123;</div><div class=\"line\">  sparkSession.sharedState.cacheManager.cacheQuery(this)</div><div class=\"line\">  this</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def cache(): this.type = persist()</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Persist this Dataset with the given storage level.</div><div class=\"line\"> * @param newLevel One of: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`,</div><div class=\"line\"> *                 `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`,</div><div class=\"line\"> *                 `MEMORY_AND_DISK_2`, etc.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def persist(newLevel: StorageLevel): this.type = &#123;</div><div class=\"line\">  sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel)</div><div class=\"line\">  this</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Get the Dataset&apos;s current storage level, or StorageLevel.NONE if not persisted.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 2.1.0</div><div class=\"line\"> */</div><div class=\"line\">def storageLevel: StorageLevel = &#123;</div><div class=\"line\">  sparkSession.sharedState.cacheManager.lookupCachedData(this).map &#123; cachedData =&gt;</div><div class=\"line\">    cachedData.cachedRepresentation.storageLevel</div><div class=\"line\">  &#125;.getOrElse(StorageLevel.NONE)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.</div><div class=\"line\"> *</div><div class=\"line\"> * @param blocking Whether to block until all blocks are deleted.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def unpersist(blocking: Boolean): this.type = &#123;</div><div class=\"line\">  sparkSession.sharedState.cacheManager.uncacheQuery(this, blocking)</div><div class=\"line\">  this</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def unpersist(): this.type = unpersist(blocking = false)</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Represents the content of the Dataset as an `RDD` of `T`.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">lazy val rdd: RDD[T] = &#123;</div><div class=\"line\">  val objectType = exprEnc.deserializer.dataType</div><div class=\"line\">  val deserialized = CatalystSerde.deserialize[T](logicalPlan)</div><div class=\"line\">  sparkSession.sessionState.executePlan(deserialized).toRdd.mapPartitions &#123; rows =&gt;</div><div class=\"line\">    rows.map(_.get(0, objectType).asInstanceOf[T])</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns the content of the Dataset as a `JavaRDD` of `T`s.</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def toJavaRDD: JavaRDD[T] = rdd.toJavaRDD()</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Returns the content of the Dataset as a `JavaRDD` of `T`s.</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def javaRDD: JavaRDD[T] = toJavaRDD</div></pre></td></tr></table></figure>\n<h2 id=\"注册临时表\"><a href=\"#注册临时表\" class=\"headerlink\" title=\"注册临时表\"></a>注册临时表</h2><p>通过注册可以将一个dataset直接当作一个表来操作，这样就可以直接通过sql来执行了，不过返回的结果又是一个dataset<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Registers this Dataset as a temporary table using the given name. The lifetime of this</div><div class=\"line\"> * temporary table is tied to the [[SparkSession]] that was used to create this Dataset.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">注册</div><div class=\"line\">@deprecated(&quot;Use createOrReplaceTempView(viewName) instead.&quot;, &quot;2.0.0&quot;)</div><div class=\"line\">def registerTempTable(tableName: String): Unit = &#123;</div><div class=\"line\">  createOrReplaceTempView(tableName)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Creates a local temporary view using the given name. The lifetime of this</div><div class=\"line\"> * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.</div><div class=\"line\"> *</div><div class=\"line\"> * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that</div><div class=\"line\"> * created it, i.e. it will be automatically dropped when the session terminates. It&apos;s not</div><div class=\"line\"> * tied to any databases, i.e. we can&apos;t use `db1.view1` to reference a local temporary view.</div><div class=\"line\"> *</div><div class=\"line\"> * @throws AnalysisException if the view name is invalid or already exists</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">创建表</div><div class=\"line\">@throws[AnalysisException]</div><div class=\"line\">def createTempView(viewName: String): Unit = withPlan &#123;</div><div class=\"line\">  createTempViewCommand(viewName, replace = false, global = false)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Creates a local temporary view using the given name. The lifetime of this</div><div class=\"line\"> * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def createOrReplaceTempView(viewName: String): Unit = withPlan &#123;</div><div class=\"line\">  createTempViewCommand(viewName, replace = true, global = false)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Creates a global temporary view using the given name. The lifetime of this</div><div class=\"line\"> * temporary view is tied to this Spark application.</div><div class=\"line\"> *</div><div class=\"line\"> * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,</div><div class=\"line\"> * i.e. it will be automatically dropped when the application terminates. It&apos;s tied to a system</div><div class=\"line\"> * preserved database `global_temp`, and we must use the qualified name to refer a global temp</div><div class=\"line\"> * view, e.g. `SELECT * FROM global_temp.view1`.</div><div class=\"line\"> *</div><div class=\"line\"> * @throws AnalysisException if the view name is invalid or already exists</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 2.1.0</div><div class=\"line\"> */</div><div class=\"line\">@throws[AnalysisException]</div><div class=\"line\">def createGlobalTempView(viewName: String): Unit = withPlan &#123;</div><div class=\"line\">  createTempViewCommand(viewName, replace = false, global = true)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">private def createTempViewCommand(</div><div class=\"line\">    viewName: String,</div><div class=\"line\">    replace: Boolean,</div><div class=\"line\">    global: Boolean): CreateViewCommand = &#123;</div><div class=\"line\">  val viewType = if (global) GlobalTempView else LocalTempView</div><div class=\"line\"></div><div class=\"line\">  val tableIdentifier = try &#123;</div><div class=\"line\">    sparkSession.sessionState.sqlParser.parseTableIdentifier(viewName)</div><div class=\"line\">  &#125; catch &#123;</div><div class=\"line\">    case _: ParseException =&gt; throw new AnalysisException(s&quot;Invalid view name: $viewName&quot;)</div><div class=\"line\">  &#125;</div><div class=\"line\">  CreateViewCommand(</div><div class=\"line\">    name = tableIdentifier,</div><div class=\"line\">    userSpecifiedColumns = Nil,</div><div class=\"line\">    comment = None,</div><div class=\"line\">    properties = Map.empty,</div><div class=\"line\">    originalText = None,</div><div class=\"line\">    child = logicalPlan,</div><div class=\"line\">    allowExisting = false,</div><div class=\"line\">    replace = replace,</div><div class=\"line\">    viewType = viewType)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h2 id=\"数据保存\"><a href=\"#数据保存\" class=\"headerlink\" title=\"数据保存\"></a>数据保存</h2><pre><code>数据保存有一个专门的write类来处理，这里就是调用write方法返回一个write对象来实现的\n</code></pre><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Interface for saving the content of the non-streaming Dataset out into external storage.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 1.6.0</div><div class=\"line\"> */</div><div class=\"line\">def write: DataFrameWriter[T] = &#123;</div><div class=\"line\">  if (isStreaming) &#123;</div><div class=\"line\">    logicalPlan.failAnalysis(</div><div class=\"line\">      &quot;&apos;write&apos; can not be called on streaming Dataset/DataFrame&quot;)</div><div class=\"line\">  &#125;</div><div class=\"line\">  new DataFrameWriter[T](this)</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * :: Experimental ::</div><div class=\"line\"> * Interface for saving the content of the streaming Dataset out into external storage.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">@Experimental</div><div class=\"line\">@InterfaceStability.Evolving</div><div class=\"line\">def writeStream: DataStreamWriter[T] = &#123;</div><div class=\"line\">  if (!isStreaming) &#123;</div><div class=\"line\">    logicalPlan.failAnalysis(</div><div class=\"line\">      &quot;&apos;writeStream&apos; can be called only on streaming Dataset/DataFrame&quot;)</div><div class=\"line\">  &#125;</div><div class=\"line\">  new DataStreamWriter[T](this)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"转换成json格式\"><a href=\"#转换成json格式\" class=\"headerlink\" title=\"转换成json格式\"></a>转换成json格式</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Returns the content of the Dataset as a Dataset of JSON strings.</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def toJSON: Dataset[String] = &#123;</div><div class=\"line\">  val rowSchema = this.schema</div><div class=\"line\">  val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone</div><div class=\"line\">  val rdd: RDD[String] = queryExecution.toRdd.mapPartitions &#123; iter =&gt;</div><div class=\"line\">    val writer = new CharArrayWriter()</div><div class=\"line\">    // create the Generator without separator inserted between 2 records</div><div class=\"line\">    val gen = new JacksonGenerator(rowSchema, writer,</div><div class=\"line\">      new JSONOptions(Map.empty[String, String], sessionLocalTimeZone))</div><div class=\"line\"></div><div class=\"line\">    new Iterator[String] &#123;</div><div class=\"line\">      override def hasNext: Boolean = iter.hasNext</div><div class=\"line\">      override def next(): String = &#123;</div><div class=\"line\">        gen.write(iter.next())</div><div class=\"line\">        gen.flush()</div><div class=\"line\"></div><div class=\"line\">        val json = writer.toString</div><div class=\"line\">        if (hasNext) &#123;</div><div class=\"line\">          writer.reset()</div><div class=\"line\">        &#125; else &#123;</div><div class=\"line\">          gen.close()</div><div class=\"line\">        &#125;</div><div class=\"line\"></div><div class=\"line\">        json</div><div class=\"line\">      &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">  import sparkSession.implicits.newStringEncoder</div><div class=\"line\">  sparkSession.createDataset(rdd)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"获取文件列表\"><a href=\"#获取文件列表\" class=\"headerlink\" title=\"获取文件列表\"></a>获取文件列表</h2><p>  可以获取当前dataSet都加载了那些文件<br>  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">/**</div><div class=\"line\"> * Returns a best-effort snapshot of the files that compose this Dataset. This method simply</div><div class=\"line\"> * asks each constituent BaseRelation for its respective files and takes the union of all results.</div><div class=\"line\"> * Depending on the source relations, this may not find all input files. Duplicates are removed.</div><div class=\"line\"> *</div><div class=\"line\"> * @group basic</div><div class=\"line\"> * @since 2.0.0</div><div class=\"line\"> */</div><div class=\"line\">def inputFiles: Array[String] = &#123;</div><div class=\"line\">  val files: Seq[String] = queryExecution.optimizedPlan.collect &#123;</div><div class=\"line\">    case LogicalRelation(fsBasedRelation: FileRelation, _, _) =&gt;</div><div class=\"line\">      fsBasedRelation.inputFiles</div><div class=\"line\">    case fr: FileRelation =&gt;</div><div class=\"line\">      fr.inputFiles</div><div class=\"line\">  &#125;.flatten</div><div class=\"line\">  files.toSet.toArray</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>"}],"PostAsset":[],"PostCategory":[{"post_id":"cj70mt0o9000060tus60zt94y","category_id":"cj70mt0or000460tuxolgvinf","_id":"cj70mt0pm000d60tudrvtta23"},{"post_id":"cj70mt0pn000e60tu14g5drzk","category_id":"cj70mt0pw000i60tu3c2ps462","_id":"cj70mt0qh000r60tu6tuwv1y2"},{"post_id":"cj70mt0qd000p60tui7nopoch","category_id":"cj70mt0qk000u60tujrwu8jwn","_id":"cj70mt0qr001160tuhg8qb74b"},{"post_id":"cj70mt11o001g60tusrxx6308","category_id":"cj70mt0pw000i60tu3c2ps462","_id":"cj70mt11z001i60tus0fcgehs"}],"PostTag":[{"post_id":"cj70mt0o9000060tus60zt94y","tag_id":"cj70mt0oy000560tugsc85bs8","_id":"cj70mt0pk000b60tuspdahmx3"},{"post_id":"cj70mt0p0000660tucsr32ywl","tag_id":"cj70mt0pi000a60tukjsprfwe","_id":"cj70mt0pv000h60tu5h72ibyf"},{"post_id":"cj70mt0p8000860tugthcweur","tag_id":"cj70mt0pi000a60tukjsprfwe","_id":"cj70mt0q5000m60tutxy8ggfb"},{"post_id":"cj70mt0pg000960tu6xsgjttb","tag_id":"cj70mt0q0000k60tuwkqkh14l","_id":"cj70mt0qh000q60tuevx057xl"},{"post_id":"cj70mt0q6000n60tu24fljvsv","tag_id":"cj70mt0q0000k60tuwkqkh14l","_id":"cj70mt0qk000t60tuew6z9ehg"},{"post_id":"cj70mt0pk000c60tucebvddu5","tag_id":"cj70mt0qb000o60tuz6n1nt0f","_id":"cj70mt0qo000x60tucqc7d0hg"},{"post_id":"cj70mt0qo000y60tulzcbcag5","tag_id":"cj70mt0qb000o60tuz6n1nt0f","_id":"cj70mt0qr001060tuxjkpb8xe"},{"post_id":"cj70mt0pn000e60tu14g5drzk","tag_id":"cj70mt0ql000v60tumq96gys6","_id":"cj70mt0qs001360tuiwiugrz1"},{"post_id":"cj70mt0pn000e60tu14g5drzk","tag_id":"cj70mt0qr000z60tuwokgslem","_id":"cj70mt0qs001460tuzaigk6z7"},{"post_id":"cj70mt0ps000g60tupfaojebl","tag_id":"cj70mt0qs001260tuue9hfg3e","_id":"cj70mt0qv001660tucy8nbepj"},{"post_id":"cj70mt0py000j60tupzkv0omm","tag_id":"cj70mt0ql000v60tumq96gys6","_id":"cj70mt0qw001860tu8dmupsrz"},{"post_id":"cj70mt0q2000l60tu3a1pfoam","tag_id":"cj70mt0qv001760tu10h07d5e","_id":"cj70mt0qx001a60tu44ciehnh"},{"post_id":"cj70mt0qd000p60tui7nopoch","tag_id":"cj70mt0qw001960tu5rw28csy","_id":"cj70mt0qy001c60tuk7m8s2iu"},{"post_id":"cj70mt0qi000s60tuo9h0op03","tag_id":"cj70mt0qw001960tu5rw28csy","_id":"cj70mt0r0001e60tu4e5dlm62"},{"post_id":"cj70mt0qm000w60tusjmyn6zp","tag_id":"cj70mt0qw001960tu5rw28csy","_id":"cj70mt0r0001f60tuc3t38s44"},{"post_id":"cj70mt11o001g60tusrxx6308","tag_id":"cj70mt0ql000v60tumq96gys6","_id":"cj70mt120001j60tucuwbcc26"},{"post_id":"cj70mt11o001g60tusrxx6308","tag_id":"cj70mt11w001h60tuu6wxrleg","_id":"cj70mt120001k60tu2qd5aub3"}],"Tag":[{"name":"-生活","_id":"cj70mt0oy000560tugsc85bs8"},{"name":"阅读","_id":"cj70mt0pi000a60tukjsprfwe"},{"name":"生活","_id":"cj70mt0q0000k60tuwkqkh14l"},{"name":"linux","_id":"cj70mt0qb000o60tuz6n1nt0f"},{"name":"spark","_id":"cj70mt0ql000v60tumq96gys6"},{"name":"sql","_id":"cj70mt0qr000z60tuwokgslem"},{"name":"hexo","_id":"cj70mt0qs001260tuue9hfg3e"},{"name":"java","_id":"cj70mt0qv001760tu10h07d5e"},{"name":"scala","_id":"cj70mt0qw001960tu5rw28csy"},{"name":"大数据","_id":"cj70mt11w001h60tuu6wxrleg"}]}}